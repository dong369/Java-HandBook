# 1 搜索引擎

## 1.1 正排索引

通过文档id=>文档内容、单词的关联关系。

| 文档ID | 文档内容                        |
| ------ | ------------------------------- |
| 1      | elasticsearch是最流行的搜索引擎 |
| 2      | Java是最好的开发语言            |
| 3      | 最流行的搜索引擎是如何诞生的    |

## 1.2 倒排索引

通过单词=>文档id的关联关系，通过正排索引进行分词形成倒排索引。

| 单词          | 文档ID |
| ------------- | ------ |
| elasticsearch | 1      |
| 搜索引擎      | 1，3   |
| 最流行        | 1，3   |

倒排索引-查询流程，查询包含“搜索引擎”的文档，通过倒排索引获得“搜索引擎”对应的文档ld有1和3；通过正排索引查询1和3的完整内容，返回用户最终结果。倒排索引组成，倒排索引是搜索引擎的核心，主要包含两部分：单词词典（ Term Dictionary）；倒排列表（ Posting List）。

### 1.2.1 单词词典

倒排索引-单词词典，单词词典（Term Dictionary）是倒排索引的重要组成，记录所有文档的单词，一般都比较大，记录单词到倒排列表的关联信息。倒排索引-单词词典，单词字典的实现一般是用B+Tree，示例如下图：下图排序采用拼音实现，构造方法参见如下网址：https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html

### 1.2.2 倒排列表

倒排索引-倒排列表，倒排列表（Posting List）记录了单词对应的文档集合，由倒排索引项（Posting）组成，倒排索引项（ Posting）主要包含如下信息：

文档ld，用于获取原始信息。

单词频率（TF，Term Frequency），记录该单词在该文档中的岀现次数，用于后续相关性算分。

位置（Position），记录单词在文档中的分词位置（多个），用于做词语搜索Phrase Query。

偏移（Offset），记录单词在文档的开始和结束位置，用于做高亮显示。

注意：ES存储的是一个json格式的文档，其中包含多个字段，每个字段会有自己的倒排索引。

通过单词=>文档id的关联关系，通过正排索引进行分词形成倒排索引。

| DocId | TF   | Position | Offset |
| ----- | ---- | -------- | ------ |
|       |      |          |        |
|       |      |          |        |
|       |      |          |        |

![](D:\dev\2019dev\code\idea-workspace\Java-HandBook\插图\服务中间件\Elastic-Stack\搜引擎原理.png)

ES存储的是一个json格式的文档，其中包含多个字段，每个字段会有自己的倒排索引。

## 1.3 相关性算分

相关性算分是指文档与査询语句间的相关度，英文为relevance。

通过倒排索引可以获取与査询语句相匹配的文档列表，那么如何将最符合用户查询，需求的文档放到前列呢？

本质是一个排序问题，排序的依据是相关性算分。

| 单词   | 文档ID列表 |
| ------ | ---------- |
| alfred | 1,2        |
| way    | 1          |

相关性算分的几个重要概念如下：

term Frequency（TF）词频，即单词在该文档中出现的次数。词频越高，相关度越高。

Document Frequency（DF）文档频率，即单词出现的文档数。

Inverse Document Frequency（IDF）逆向文档频率，与文栏频率相反，简单理解为1/DF，即单词出现的文档数越少，相关度越高。

Field-length Norm文档越短，相关性越高。

ES目前主要有两个相关性算分模型，如：TF/DF模型；BM25模型5X之后的默认模型。

可以通过explain参数来查看具体的计算方法。但要注意ES的算分是按照shard进行的，即shard的分数计算是相互独立的，所以在使用explain的时候注意分片数，可以通过设置索引的分片数为1来避免这个问题。

TFDF模型是Lucene的经典模型；BM25模型中BM指 Best match，25指迭代了25次才计算方法，是针对TFDF的一个优化。

# 2 分词

分词是指将文本转换成一系列单词（term or token）的过程，也可以叫做文本分析，在ES里面称为Analysis。

## 2.1 分词器

分词器是ES中专门处理分词的组件，英文为 Analyzer，它的组成如下：

Character Filters：针对原始文本进行处理，比如去除html特殊标记符。

Tokenizer：将原始文本按照一定规则切分为单词，**只允许有一个的**。

Token Filters：针对tokenizer处理的单词进行再加工，比如转小写、删除（stop word中文中的、那）或新增等处理。

注意：是**按照顺序进行分词**的。

## 2.2 Analyze APl

ES提供了一个测试分词的API接口，方便验证分词效果， endpoint是_analyze，可以直接指定analyzer进行测试
可以直接指定索引中的字段进行测试，可以自定义分词器进行测试。

```properties
GET _analyze
{
  "text": [
    "hello word"
  ],
  "analyzer": "standard"
}

# 响应结构
{
  "tokens" : [
    {
      // 分词结果
      "token" : "hello",
      // 起始偏移
      "start_offset" : 0,
      // 结束偏移
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      // 分词位置
      "position" : 0
    },
    {
      "token" : "word",
      "start_offset" : 6,
      "end_offset" : 10,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}

```

### 2.2.1 直接指定_analyzer

进行测试，接口如下：

```json
POST _analyze
{
  "analyzer": "standard",
  "text": "中华人民共和国"
}
```

### 2.2.2 直接指定索引

字段进行测试，接口如下：

```json
POST test_ik/_analyze
{
  "field": "message",
  "text": "中华人民共和国"
}
```

### 2.2.3 自定义分词器

进行测试，接口如下：

```json
# 语法
POST _analyze
{
  "tokenizer": "standard",
  "text": "HELLO WORD!",
  "filter": [
    "lowercase"
  ]
}
```

## 2.3 预定义分词器

### 2.3.1 ES自带分词器

Standard：默认分词器，特性为按词切分，支持多语言，小写处理。

Simple：特性为是按照非字母切分，小写处理。

Whitespace：特性为按照空格就行切分，不做小写处理。

Stop：特性为语气助词等修饰性的词语，比如the、an、的、这等等。

Keyword：特性为不分词，直接将输入作为一个单词输出。

pattern：特性为通过正则表达式自定义分割符，默认是\W+，即非字词的符号作为分隔符。

Language：提供了30+常见语言的分词器。

### 2.3.2 其它分词



## 2.4 自定义分词

当自带的分词无法满足需求时，可以自定义分词。通过自定义 Character Filters、 Tokenizer和 Token Filter实现。

### 2.4.1 Character Filters

在 Tokenizer之前对原始文本进行处理，比如增加、删除或替换字符等。自带的如下：

HTML Strip去除htm标签和转换htm实体；

Mapping进行字符替换操作；

Pattern Replace进行正则匹配替换；

会影响后续 tokenizer解析的 postion和offet信息。

Character Filters测试时可以采用如下api：

```json
# character filters自定义分词 
POST _analyze
{
    "tokenizer": "keyword",
    "char_filter": [
        "html_strip"
    ],
    "text": "<p>I &apos;elasticsearch is goods</p>"
}
```

### 2.4.2 Tokenizer

将原始文本按照一定规则切分为单词（term or token）自带的如下：

standard按照单词进行分割

letter按照非字符类进行分割

whitespace按照空格进行分割

UAX URL Email按照 standard分割，但不会分割邮箱和url

NGram和 Edge NGram连词分割

path_hierarchy按照文件路径进行切割

```json
# tokenizer自定义分词 
POST _analyze
{
    "tokenizer": "path_hierarchy",
    "text": "/usr/local/redis"
}
```

### 2.4.3 Token Filters

token filters是对于tokenizer输出的单词（term）进行增加、删除、修改等操作，自带的如下：

lowercase将所有term转换为小写

stop删除stop words

NGram和Edge NGram连词分割

Synonym添加近义词的term

注意：自定义分词的API，自定义分词需要在**索引的配置中设定**。

```json
# token filters自定义分词 
POST _analyze
{
  "tokenizer": "standard",
  "text": "a Hello,world!",
  "filter": [
    "stop",
    "lowercase",
    {
      "type": "ngram",
      "min_gram": 4,
      "max_gram": 4
    }
  ]
}
```

### 2.4.4 自定义analysis

自定义分词的API，自定义分词需要在索引的配置中设定。

```json
# 自定义分词01
PUT test_index1
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "type": "custom",
                    "tokenizer": "standard",
                    "char_filter": [
                        "html_strip"
                    ],
                    "filter": [
                        "lowercase",
                        "asciifolding"
                    ]
                }
            }
        }
    }
}

POST test_index1/_analyze
{
  "analyzer": "my_analyzer",
  "text": ["Is this <b> a box </b>"]
}

# 自定义分词02
PUT test_index2
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "type": "custom",
                    "tokenizer": "punctuation",
                    "char_filter": [
                        "emoticons"
                    ],
                    "filter": [
                        "lowercase",
                        "english_stop"
                    ]
                }
            },
            "char_filter": {
                "emoticons": {
                    "type": "mapping",
                    "mappings": [
                        ":)=>_happy_",
                        ":(=>_sad_"
                    ]
                }
            },
            "tokenizer": {
                "punctuation": {
                    "type": "pattern",
                    "pattern": "[.,!?]"
                }
            },
            "filter": {
                "english_stop": {
                    "type": "stop",
                    "stopwords": "_english_"
                }
            }
        }
    }
}
```

## 2.5 分词使用时机

创建或更新文档时（Index time），会对相应的文档进行分词处理。

查询时（Search Time），会对查询语句进行分词。

### 2.5.1 索引时分词

索引时分词是通过配置Index Mapping中每个字段的analyzer属性实现的，如下不指定分词时，使用默认 standard。

```json

```

### 2.5.2 查询时分词

查询时分词的指定方式有如下几种：查询的时候通过analyzer指定分词器；通过index mapping设置search_analyzer实现；

一般不需要特别指定査询时分词器，直接使用索引时分词器即可，否则会出现无法匹配的情况。

```json

```

### 2.5.3 分词使用建议

明确字段是否需要分词，不需要分词的字段就将type设置为 keyword，可以**节省空间**和**提高写性能**。

善用_analyze API，查看文档的具体分词结果，动手测试。
