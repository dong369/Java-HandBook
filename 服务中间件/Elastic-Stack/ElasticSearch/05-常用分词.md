# 1 分词

分词是指将文本转换成一系列单词（ term or token）的过程，也可以叫做文本分析，在ES里面称为 Analysis。

## 1.1 分词器

分词器是ES中专门处理分词的组件，英文为 Analyzer，它的组成如下：

Character Filters：针对原始文本进行处理，比如去除htm特殊标记符。

Tokenizer：将原始文本按照一定规则切分为单词。只允许有一个。

Token Filters：针对 tokenizer处理的单词就行再加工，比如转小写、删除或新增等处理。

注意：是按照顺序进行分词的。

## 1.2 Analyze APl

ES提供了一个测试分词的api接口，方便验证分词效果， endpoint是_analyze，可以直接指定 analyzer进行测试
可以直接指定索引中的字段进行测试，可以自定义分词器进行测试。

直接指定 analyzer进行测试，接口如下：

语法：post http://211.144.5.80:30136/_analyze   {"analyzer":"standard","text":"hello world"}

直接指定索引中的字段进行测试，接口如下：

语法：post http://211.144.5.80:30136/school/_analyze   {"field":"standard","text":"hello world"}

自定义分词器进行测试，接口如下：

语法：post http://211.144.5.80:30136/_analyze   {"analyzer":"standard","filter":["lowercase"],"text":"hello world"}

## 1.3 预定义分词器

ES自带如下的分词器
Standard：默认分词器，特性为按词切分，支持多语言，小写处理。
Simple：特性为是按照非字母切分，小写处理。
Whitespace：特性为按照空格就行切分，不做小写处理。
Stop：特性为语气助词等修饰性的词语，比如the、an、的、这等等。
Keyword：特性为不分词，直接将输入作为一个单词输出。
Pattern：特性为通过正则表达式自定义分割符，默认是\W+，即非字词的符号作为分隔符。
Language：提供了30+常见语言的分词器。

## 1.4 自定义分词

当自带的分词无法满足需求时，可以自定义分词。通过自定义 Character Filters、 Tokenizer和 Token Filter实现。

### 1.4.1 Character Filters

在 Tokenizer之前对原始文本进行处理，比如增加、删除或替换字符等。自带的如下：

HTML Strip去除htm标签和转换htm实体；

Mapping进行字符替换操作；

Pattern Replace进行正则匹配替换

会影响后续 tokenizer解析的 postion和offet信息。

Character Filters测试时可以采用如下api：

### 1.4.2 Tokenizer

将原始文本按照一定规则切分为单词（ term or token）自带的如下：

standard按照单词进行分割

letter按照非字符类进行分割

whitespace按照空格进行分割

UAX URL Email按照 standard分割，但不会分割邮箱和url

NGram和 Edge NGram连词分割

path_hierarchy按照文件路径进行切割

### 1.4.3 Token Filters

token filters是对于tokenizer输出的单词（term）进行增加、删除、修改等操作，自带的如下：

lowercase将所有term转换为小写

stop删除 stop words

NGram和 Edge NGram连词分割

Synonym添加近义词的term

注意：自定义分词的API，自定义分词需要在**索引的配置中设定**。



## 1.5 分词使用时机

创建或更新文档时（ Index time），会对相应的文档进行分词处理。

查询时（ Search Time），会对查询语句进行分词。

### 2.5.1 索引时分词

索引时分词是通过配置Index Mapping中每个字段的analyzer属性实现的，如下不指定分词时，使用默认 standard。

### 2.5.2 查询时分词

查询时分词的指定方式有如下几种：查询的时候通过analyzer指定分词器；通过index mapping设置 search_analyzer实现；

一般不需要特别指定査询时分词器，直接使用索引时分词器即可，否则会出现无法匹配的情况。

### 2.5.3 分词使用建议

明确字段是否需要分词，不需要分词的字段就将type设置为 keyword，可以节省空间和提高写性能。

善用_ analyze API，查看文档的具体分词结果，动手测试。

# 2 中文分词

中文分词指的是将一个汉字序列切分成一个一个单独的词。在英文中，单词之间是以空格作为自然分界符，汉语中词没有一个飛式上的分界符。上下文不同，分词结果迥异，比如交叉歧义问题，比如下面两种分词都合理。
乒乓球拍/卖/完了
乒乓球/拍卖/院了
https://mp.weixin.qq.com/s/SiHSMrn8lxCmrtHbcwL-NQ

## 2.1 常用分词系统

IK实现中英文单词的切分，支持 ik_smart、 ik_maxword等模式；可自定义词库，支持热更新分词词典.
https://github.com/medcl/elasticsearch-analysis-ik
Jieba是python中最流行的分词系统，支持分词和词性标注；支持繁体分词、自定义词典、并行分词等
https://github.com/sing1ee/elasticsearch-jieba-plugin

基于自然语言处理的分词系统
Hanlp由一系列模型与算法组成的ava工具包，目标是普及自然语言处理在生产环境中
的应用
https://github.com/hankcs/hanlp
THULAC是THU Lexical Analyzer for Chinese，由清华大学自然语言处理与社会人文计算实验室硏制推出的一套中文词法分析工具包，具有中文分词和词性标注功能
https://github.com/microbun/elasticsearch-thulac-plugin

## 2.2 IK分词插件

去<https://github.com/medcl/elasticsearch-analysis-ik/releases>下载对应`Elasticsearch`版本的IK分词插件`elasticsearch-analysis-ik-7.3.0.zip`这个文件

1、在elaticsearch安装目录下有个plugins文件夹，在该文件夹下新建ik文件夹

2、将刚才下载的elasticsearch-analysis-ik7.3.0.zip包解压到该ik目录内

​       unzip -d elasticsearch-analysis-ik-7.3.0 elasticsearch-analysis-ik-7.3.0.zip

​       rm -rf elasticsearch-analysis-ik-7.3.0.zip

## 2.3 拼音插件

去https://github.com/medcl/elasticsearch-analysis-pinyin/releases下载对应Elasticsearch版本的IK分词插件elasticsearch-analysis-pinyin-7.3.0.zip这个文件。

1、在elaticsearch安装目录下有个plugins文件夹，在该文件夹下新建ik文件夹

2、将刚才下载的elasticsearch-analysis-ik7.3.0.zip包解压到该ik目录内

​       unzip -d elasticsearch-analysis-pinyin-7.3.0 elasticsearch-analysis-pinyin-7.3.0.zip

​       rm -rf elasticsearch-analysis-pinyin-7.3.0.zip

```json
POST /_analyze
{
  "text":"中华人民共和国国徽",
  "analyzer":"pinyin"
}

PUT /book
{
  "settings": {
    "analysis": {
      "analyzer": {
        "pinyin_analyzer": {
          "tokenizer": "my_pinyin"
        }
      },
      "tokenizer": {
        "my_pinyin": {
          "type": "pinyin",
          "keep_separate_first_letter": false,
          "keep_full_pinyin": true,
          "keep_original": true,
          "limit_first_letter_length": 16,
          "lowercase": true,
          "remove_duplicated_term": true
        }
      }
    }
  }
}


POST /book/_mapping
{
  "properties": {
    "name": {
      "type": "keyword",
      "fields": {
        "pinyin": {
          "type": "text",
          "store": false,
          "term_vector": "with_offsets",
          "analyzer": "pinyin_analyzer",
          "boost": 10
        }
      }
    },
    "content": {
      "type": "text",
      "analyzer": "ik_max_word",
      "search_analyzer": "ik_smart"
    },
    "describe": {
      "type": "text",
      "analyzer": "ik_max_word",
      "search_analyzer": "ik_smart",
      "fields": {
        "pinyin": {
          "type": "text",
          "store": false,
          "term_vector": "with_offsets",
          "analyzer": "pinyin_analyzer",
          "boost": 10
        }
      }
    },
    "id": {
      "type": "long"
    }
  }
}
```



# 3 源码附录

```json
# post http://211.144.5.80:30136/_analyze
{
  "analyzer": "standard",
  "text": "elasticsearch hello, world!"
}
# post http://211.144.5.80:30136/school/_analyze
{
  "field": "username",
  "text": "elasticsearch hello, world!"
}
# post http://211.144.5.80:30136/_analyze
{
  "tokenizer": "standard",
  "filter": ["lowercase"],
  "text": "elasticsearch hello, World!"
}
# character filters自定义分词 post http://211.144.5.80:30136/_analyze
{
    "tokenizer": "keyword",
    "char_filter": [
        "html_strip"
    ],
    "text": "<p>I &apos;elasticsearch is goods</p>"
}
# tokenizer自定义分词 post http://211.144.5.80:30136/_analyze
{
    "tokenizer": "path_hierarchy",
    "text": "/usr/local/redis"
}
# token filters自定义分词 post http://211.144.5.80:30136/_analyze
{
    "tokenizer": "standard",
    "text": "a Hello,world!",
    "filter": [
        "stop",
        "lowercase",
        {
            "type": "ngram",
            "min_gram": 3,
            "max_gram":4
        }
    ]
}
# 自定义分词 put http://211.144.5.80:30136/test_index1
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "type": "custom",
                    "tokenizer": "standard",
                    "char_filter": [
                        "html_strip"
                    ],
                    "filter": [
                        "lowercase",
                        "asciifolding"
                    ]
                }
            }
        }
    }
}
# 自定义分词 put http://211.144.5.80:30136/test_index2
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "type": "custom",
                    "tokenizer": "punctuation",
                    "char_filter": [
                        "emoticons"
                    ],
                    "filter": [
                        "lowercase",
                        "english_stop"
                    ]
                }
            },
            "char_filter": {
                "emoticons": {
                    "type": "mapping",
                    "mappings": [
                        ":)=>_happy_",
                        ":(=>_sad_"
                    ]
                }
            },
            "tokenizer": {
                "punctuation": {
                    "type": "pattern",
                    "pattern": "[.,!?]"
                }
            },
            "filter": {
                "english_stop": {
                    "type": "stop",
                    "stopwords": "_english_"
                }
            }
        }
    }
}
```