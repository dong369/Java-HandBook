# 1 中文分词

中文分词指的是将一个汉字序列切分成一个一个单独的词。在英文中，单词之间是以空格作为自然分界符，汉语中词没有一个飛式上的分界符。上下文不同，分词结果迥异，比如交叉歧义问题，比如下面两种分词都合理。
乒乓球拍/卖/完了
乒乓球/拍卖/院了
https://mp.weixin.qq.com/s/SiHSMrn8lxCmrtHbcwL-NQ

## 1.1 常用分词系统

IK实现中英文单词的切分，支持 ik_smart、 ik_maxword等模式；可自定义词库，支持热更新分词词典。
https://github.com/medcl/elasticsearch-analysis-ik
Jieba是python中最流行的分词系统，支持分词和词性标注；支持繁体分词、自定义词典、并行分词等
https://github.com/sing1ee/elasticsearch-jieba-plugin

基于自然语言处理的分词系统
Hanlp由一系列模型与算法组成的ava工具包，目标是普及自然语言处理在生产环境中的应用
https://github.com/hankcs/hanlp
THULAC是THU Lexical Analyzer for Chinese，由清华大学自然语言处理与社会人文计算实验室硏制推出的一套中文词法分析工具包，具有中文分词和词性标注功能
https://github.com/microbun/elasticsearch-thulac-plugin

## 1.2 IK分词插件

### 1.2.1 安装配置

去<https://github.com/medcl/elasticsearch-analysis-ik/releases>下载对应`Elasticsearch`版本的IK分词插件`elasticsearch-analysis-ik-7.3.0.zip`这个文件。

1、在elaticsearch安装目录下有个plugins文件夹，在该文件夹下新建ik文件夹

2、将刚才下载的elasticsearch-analysis-ik7.3.0.zip包解压到该ik目录内

​       unzip -d elasticsearch-analysis-ik-7.3.0 elasticsearch-analysis-ik-7.3.0.zip

​       rm -rf elasticsearch-analysis-ik-7.3.0.zip

### 1.2.2 分词粒度

- ik_max_word: 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合；
- ik_smart: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”。

### 1.2.3 分词字典



### 1.2.4 热更新



### 1.2.5 配置field分词

```json
POST /_analyze
{
  "text":"中华人民共和国国徽",
  "analyzer":"pinyin"
}

PUT /book
{
  "settings": {
    "analysis": {
      "analyzer": {
        "pinyin_analyzer": {
          "tokenizer": "my_pinyin"
        }
      },
      "tokenizer": {
        "my_pinyin": {
          "type": "pinyin",
          "keep_separate_first_letter": false,
          "keep_full_pinyin": true,
          "keep_original": true,
          "limit_first_letter_length": 16,
          "lowercase": true,
          "remove_duplicated_term": true
        }
      }
    }
  }
}
```





### 1.2.6 查询高亮显示

```json
GET test_ik/_search
{
  "query": {
    "match": {
      "message": "java消息"
    }
  },
  "highlight": {
    "pre_tags": [
      "<tag1>",
      "<tag2>"
    ],
    "post_tags": [
      "</tag1>",
      "</tag2>"
    ],
    "fields": {
      "message": {}
    }
  }
}
```

注意：fields中的属性要和要高亮显示的数据属性对应上！！！

## 1.3 拼音插件

### 1.3.1 安装Pinyin

去https://github.com/medcl/elasticsearch-analysis-pinyin/releases下载对应Elasticsearch版本的IK分词插件elasticsearch-analysis-pinyin-7.3.0.zip这个文件，打开可以看到如下文件。

unzip -d elasticsearch-analysis-pinyin-7.3.0 elasticsearch-analysis-pinyin-7.3.0.zip

 rm -rf elasticsearch-analysis-pinyin-7.3.0.zip

### 1.3.2 测试使用

```json
DELETE book

PUT /book
{
  "settings": {
    "analysis": {
      "analyzer": {
        "pinyin_analyzer": {
          "tokenizer": "my_pinyin"
        }
      },
      "tokenizer": {
        "my_pinyin": {
          "type": "pinyin",
          "keep_separate_first_letter": false,
          "keep_full_pinyin": true,
          "keep_original": true,
          "limit_first_letter_length": 16,
          "lowercase": true,
          "remove_duplicated_term": true
        }
      }
    }
  }
}


POST /book/_mapping
{
  "properties": {
    "name": {
      "type": "keyword",
      "fields": {
        "pinyin": {
          "type": "text",
          "store": false,
          "term_vector": "with_offsets",
          "analyzer": "pinyin_analyzer",
          "boost": 10
        }
      }
    },
    "content": {
      "type": "text",
      "analyzer": "ik_max_word",
      "search_analyzer": "ik_smart"
    },
    "describe": {
      "type": "text",
      "analyzer": "ik_max_word",
      "search_analyzer": "ik_smart",
      "fields": {
        "pinyin": {
          "type": "text",
          "store": false,
          "term_vector": "with_offsets",
          "analyzer": "pinyin_analyzer",
          "boost": 10
        }
      }
    }
  }
}
```

