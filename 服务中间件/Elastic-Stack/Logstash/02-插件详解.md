# 1 Input Plugin

Input插件指定数据输入源，一个pipeline可以有多个 Input插件，我们主要讲解下面的几个 Input插件：stdin、file、kafka。

## 1.1 stdin

最简单的输入，从标准输入读取数据，通用配置为：code类型为codec；type类型为string，自定义该事件的类型，可用于后续判断；tags类型为aray，自定义该事件的tag，可用于后续判断；add_field类型为hash，为该事件添加字段。

```json
input {
    stdin {
    	codec => "plain"
    	tags => ["test"]
    	type => "std"
		add_field => {"key" => "value"}
	}
}
output {
    stdout {
    	codec => "rubydebug"
	}
}

echo "test" | ./bin/logstash -f /home/cat.conf
```



```json
input {
  stdin {
  }
  jdbc {
	jdbc_connection_string => "jdbc:mysql://localhost:3306/test?characterEncoding=UTF-8&useConfigs=maxPerformance"
	jdbc_user => "root"
	jdbc_password => "passw0rd"
	jdbc_driver_library => "mysql/mysql-connector-java-8.0.17.jar"
	jdbc_driver_class => "com.mysql.jdbc.Driver"
	statement => "SELECT * FROM test_user"
	type => "test"
  }
}
output {
  elasticsearch {
    hosts => "localhost:9200"
    index => "world"
    document_id => "%{id}"
  }
  stdout {
    codec => json_lines
  }
}
```

## 1.2 Input Plugin-file

### 1.2.1 认识

从文件读取数据，如常见的日志文件。文件读取通常要解决几个问题：

文件内容如何只被读取一次？即重启LS时，从上次读取的位置继续（singed）；

如何即时读取到文件的新内容？（定时检查文件是否有更新）；

如何发现新文件并进行读取？（可以，定时检查新文件）；

如果文件发生了归档（rotation操作），是否影响当前的內容读取？（不影响，被归档的文件内容可以继续被读取）。

基于 Filewatch的ruby库实现的
https://github.com/jordansissel/ruby-filewatch



path类型为数组，指明读取的文件路径，基于gob匹配语法
path=>[/var/log/**/*.log"，"/var/log/message"

exclue类型为数组排除不想监听的文件规则基于gob匹配语法
exclude =>*az

sincedb_path类型为字符串，记录 sincedb文件路径

start_postion类型为字符串， beginning or end，是否从头读取文件

stat interval类型为数值，单位秒，定时检查文件是否有更新，默认1秒

discover interva类型为数值，单位秒，定时检查是否有新文件待读取，默认15

ignore_older类型为数值，单位秒，扫描文件列表时，如果该文件上次更改时间超过设定的时长，则不做处理，但依然会监控是否有新内容，默认关闭

close_older类型为数值，单位秒，如果监听的文件在超过该设定时间内没有新内容，会被关闭文件句柄，释放资源，但依然会监控是否有新内容，默认3600秒，即1个小时

### 1.2.2 Glob匹配语法

主要包含如下几种匹配符
*匹配任意字符，但不匹配以。开头的隐藏文件，匹配这类文件时要使用。来匹配
**递归匹配子目录
？匹配单一字符
[]匹配多个字符，比如[a-z]、[^a-z]
{}旮匹配多个单词，比如{foo， bar hello}
\转义符号

```properties
input {
    file {
    	# 指定读的文件
        path => ["/var/log/access_log","/var/log/err_log"],
        type => "web",
        start_position => "beginning"
    }
}
```



```properties
input {
    file {
        path => ["/var/log/*.log"],
        sincedb_paht => "/dev/null",
        start_position => "beginning",
        ignore_order => 0
        close_older => 5        
    }
}
```

## 1.3 Kafka

Kafka是最流行的消息队列，也是 Elastic Stack架构中常用的，使用相对简单。

```properties
input {
    kafka {
        zk_connect => "kafka:2181"
        group_id => "logstash"
        topic_id => "apache_logs"
        consumer_threads => 16
    }
}
```

# 2 Codec Plugin

Codec plugin作用于 Input和 output plugin，负责将数据在原始与 LogstashEvent之间转换，常见的 codec有

plain读取原始内容
dots将内容简化为点进行输出
rubydebug将 Logstash Events按照ruby格式翰出，方便调试
ine处理带有换行符的内容
json处理json格式的内容
multiline处理多行数据的内容

```properties
./bin/logstash -e 'input {stdin {codec=>line}} output {stdout {codec=>rubydebug}}'
./bin/logstash -e 'input {stdin {codec=>line}} output {stdout {codec=>dots}}'
./bin/logstash -e 'input {stdin {codec=>line}} output {stdout {codec=>json}}'
./bin/logstash -e 'input {stdin {codec=>json}} output {stdout {codec=>rubydebug}}'
```

## 2.1 multiline

当一个 Event的message由多行组成时，需要使用该 codec，常见的情况是堆栈日志信息的处理。

主要设置参数如下：pattern设置行匹配的正则表式，可以使用grok ；what previous|next，如果匹配成功，那么匹配行是归属上一个事件还是下一个事件；negate true or false是否对 pattern的结果取反。

```properties
input {
    stdin {
        codec => multiline {
            pattern => "^\s"
            what => "previous"
        }
    }
}
```



# 3 Filter Plugin

Filter是Logstash功能强大的主要原因，它可以对 Logstash Event进行丰富的
处理，比如解析数据、删除字段、类型转换等等，常见的有如下几个
date日期解析
grok正则匹配解析
dissect分割符解析
mutate对字段作处理，比如重命名、删除、替换等
json按照json解析字段内容到指定字段中
geoip增加地理位置数据
ruby利用ruby代码来动态修改 Logstash Event

## 3.1 Date

将日期字符串解析为日期类型，然后替换@timestamp字段或者指定的其他字段。

```properties
filter {
    date {
        match => ["logdate","MMM dd yyyy HH:mm:ss"]
    }
}
```

match
类型为数组，用于指定日期匹配的格式，可以一次指定多种日期格式
match =>[logdate，"MMM dd yyyy Hh： mm ss" "MMM d yyyy
HH： mm： ss""ISO8601
target
类型为字符串，用于指定赋值的字段名，默认是@ timestamp
timezone
类型为字符串，用于指定时区

## 3.2 Grok

https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns



正则表达式
https://www.debuggex.com/
http://www.regexr.com/
grok调试
http://grokdebug.herokuapp.com/
http://grok.elasticsearch.cn/
x-pack grok debugger



## 3.3 Dissect

dissect的应用有一定的局限性，主要适用于每行格式相似且分隔符明确简单的场景。

dissect语法比较简单，有一系列字段（fied和分隔符 delimiter）组成。

%{}字段

%{}之间是分隔符

%{+XX}进行追加

%{+XX/2}进行追加指定顺序

dissect分割后的字段值都是字符串，可以使用convert_datatype属性进行类型转专换

## 3.4 Mutate

使用最频繁的操作，可以对字段进行各种操作，比如重命名、删除、替换、更新
等，主要操作如下
convert类型转换
gsub字符串替换
split/join/ merge字符串切割、数组合并为字符串、数组合并为数组
rename字段重命名
update/ replace字段内容更新或替换
remove field删除字段

### 3.4.1 convert

实现字段类型的转换，类型为hash，仅支持转换为 Integer、foat、 string和boolean。

```properties
filter {
    mutate {
        convert => {
            "age" => "integer"
        }
    }
}
```

### 3.4.2 gsub

对字段内容进行替换，类型为数组，每3项为一个替换配置。

```properties
filter {
    mutate {
        gsub {
            "path","/","_",
            "urlparams","[\\?#-]","."
        }
    }
}
```

### 3.4.3 split

```properties
filter {
    mutate {
        split {
            "jobs"=>","
        }
    }
}
```

### 3.4.4 join

```properties
filter {
    mutate {
        join => {
            "jobs"=>","
        }
    }
}
```

### 3.4.5 merge

```properties
filter {
    mutate {
        merge => {
            "jobs_arr"=>"user_arr"
        }
    }
}
```

### 3.4.6 update

更新字段内容，区别在于 update只在字段存在时生效。

```properties
filter {
    mutate {
        update => {
            
        }
    }
}
```

### 3.4.7 replace

而 replace在字段不存，在时会执行新增字段的操作。

```properties
filter {
    mutate {
        remove => {
            ["message"]
        }
    }
}
```

### 3.4.8 remove

```properties
filter {
    mutate {
        remove_field => {
            ["message"]
        }
    }
}
```



## 3.5 json

将字段内容为json格式的数据进行解析。

```properties
filter {
    join {
        source => "message"
        target => "msg_json"
    }
}
```

## 3.6 geoip

常用的插件，根据地址提供对应的地域信息，比如经纬度、城市名等，方便进行地理数据分析

```properties
filter {
    geoip {
        source => "ip"
    }
}
```

## 3.7 ruby

最灵活的插件，可以以πuby语言来随心所欲的修改 Logstash Event对象。

```properties
filter {
    ruby {
        code => "ip"
    }
}
```

# 4 Output Plugin

负责将Logstash Event输出，常见的插件如下

## 4.1 stdout

输出到标准输出，多用于调试。

```properties
output {
    stdout {
        codec => rubydebug
    }
}
```

## 4.2 file

输出到文件，实现将分散在多地的文件统一到处的需求，比如将所有web机器的Web日志收集到1个文件中，从而方便查阅信息。

```properties
output {
    file {
        path => "/var/log/web.log"
        codec => line {format => "%{message}"}
    }
}
```



## 4.3 elasticsearch

输出到elasticsearch是最常用的插件，基于http协议实现。

```properties
output {
    elasticsearch {
        hosts => [""]
        index => "nginx-%{+YYYY.MM.dd}"
        template => "./nginx_template.json"
        template_name => "nginx_template"
        template_overwriter => true
    }
}
```

