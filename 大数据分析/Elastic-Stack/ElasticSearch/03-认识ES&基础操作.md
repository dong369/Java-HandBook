# 1 概念理解

## 1.1 是什么

完备的数据分析工具集合：搜索引擎、日志分析、指标分析。

对大数据处理感兴趣，想要进入这个领域或者找工作；对数据分析感兴趣，想要掌握一套崭新易用的分析工具；正在使用Elastic Stack，想要进一步提升相关技术。

Elasticsearch简称ES。大数据分析利器！！！快快快！！！**You know, for search (and analysis)**

Elasticsearch基于Apache Lucene构建的开源搜索引擎，提供一个**分布式多用户能力的全文搜索引擎**，用Java编写的，提供简单易用的RESTFul API，当前流行的企业级搜索引擎，轻松的**横向扩展**，可支持**PB级的结构化或非结构化数据处理**，可以准实时地快速存储、搜索、分析海量的数据（用于云计算中，能够达到实时搜索）。

大数据分析：Hadoop、spark、flink、storm、hive

## 1.2 应用场景

海量数据分析引擎(聚合搜索)、站内搜索引擎、数据仓库、日志收集。

## 1.3 接近实时NRT

Elasticsearch是一个接近实时（NRT）的搜索平台。这意味着，从索引一个文档直到这个文档能够被搜索到有一个轻微的延迟（通常是1秒）。

## 1.4 产品对比

| MySQL               | ES(V6.X前)            | ES(6.X后) | MongoDB    |
| ------------------- | --------------------- | --------- | ---------- |
| Database            | Index                 |           | Database   |
| Table               | Type                  |           | Collection |
| Row                 | Document              |           | Document   |
| Column              | Field                 |           | Field      |
| Schema              | Mapping               |           |            |
| Index               | Everything is indexed |           |            |
| SQL                 | Query DSL             |           |            |
| SELECT * FROM table | GET http://…          |           |            |
| UPDATE table SET    | PUT http://…          |           |            |

Elasticsearch集群可以包含多个**索引(indices)**（可以理解成是数据库表，之前6.x之前可以理解成是数据库）。

6.x之前每一个索引可以包含多个**类型(types)**（表）；但是6.x不允许一个索引包含多个类型，只能有一个；

到7.x将不能创建类型，默认是_doc类型；更高版本会完全去掉类型。

每一个类型包含多个**文档(documents)**（行），然后每个文档包含多个**字段(Fields)**（列）。

百货大楼里有各式各样的商品，例如书籍、笔、水果等。书籍可以根据内容划分成不同种类，如科技类、教育类、悬疑推理等。悬疑推理类的小说中比较有名气的有《福尔摩斯探案集》、《白夜行》等

百货大楼=> Elasticsearch数据库
书籍=> 索引
悬疑推理=> 类型
白夜行=> 文档

## 1.5 索引

索引是含有**相同属性的文档集合**，可以类比关系数据库中的**tables表**。

类型索引可以定义一个或多个类型，文档必须属于一个类型。

每个索引都有自己的**mapping定义**，用于定义字段名和类型，一个集群可以有多个索引。一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。

一个索引由一个名字来标识（**必须全部是小写字母的**），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。在一个集群中，如果你想，可以定义**任意多的索引**。

## 1.6 类型

索引可以定义一个或多个类型，文档必须属于一个类型。但是6.x以后**只能有一个**默认_doc，可以忽略！！！

在一个索引中，你可以定义一种或多种类型（type）。一个类型是你的索引的一个逻辑上的分类/分区，其语义完全由你来定。通常，会为具有一组共同字段的文档定义一个类型。

比如说，我们假设你运营一个博客平台并且将你所有的数据存储到一个索引中。在这个索引中，你可以为用户数据定义一个类型，为博客数据定义另一个类型，当然，也可以为评论数据定义另一个类型。

## 1.7 文档

用户在ES中**存储的数据文档**，每一个文档都有唯一的标识（_id），id可以**自行指定**或ES**自动生成**！！

一个文档是一个**可被索引的基础信息单元**。比如，你可以拥有某一个客户的文档，某一个产品的一个文档，当然，也可以拥有某个订单的一个文档。文档以JSON(Javascript Object Notation)格式来表示，而JSON是一个到处存在的互联网数据交互格式。

在一个index/type里面，只要你想，你可以存储任意多的文档。注意，尽管一个文档，物理上存在于一个索引之中，文档必须被索引/赋予一个索引的type。文档类似于关系型数据库中Record的概念。实际上一个文档除了用户定义的数据外，还包括index、type和id字段。

文档document是可以被索引的**基本数据单位**。Json object，由字段（ Field）组成。用户存储在ES中的数据文档。每个文档有唯的id标识（可以自行指定；也可以自动生成）。

## 1.8 元数据

元数据，用于标注文档的相关信息。

_index：文档所在的索引名

_type：文档所在的类型名

_id：文档唯一id

_uid：组合id，由ype和id组成（6.x_type不再起作用，同id—样）。

_source：文档的**原始json数据**，可以从这里获取每个字段的内容。

_all：整合所有字段内容到该字段，**默认禁用，不推荐使用**。（可以使用copy替代）

## 1.9 分片

每个索引都有多个分片，每个**分片是一个Lucene索引**；将索引划分成**多份**的能力，这些份就叫做分片！！！

一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据`1TB`的磁盘空间，而任一节点都没有这样大的磁盘空间；或者单个节点处理搜索请求，响应太慢。为了解决这个问题，Elasticsearch提供了将索引**划分成多份的能力**，这些份就叫做分片。

当你创建一个索引的时候，你可以指定你想要的分片的数量。

每个分片本身也是一个**功能完善并且独立的“索引”**，这个“索引”可以被放置到集群中的任何节点上。

**分片之所以重要，主要有两方面的原因**：

1、允许你**水平分割/扩展**你的内容容量；

2、允许你在分片（潜在地，位于多个节点上）之上进行分布式的、并行的操作，进而**提高性能/吞吐量**。

至于一个分片怎样分布，它的文档怎样聚合回搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是**透明的**。

## 1.10 副本

拷贝一份分片就完成了**分片的备份**。在一个网络/云的环境里，失败随时都可能发生，在某个分片/节点不知怎么的就处于离线状态，或者由于任何原因消失了。这种情况下，有一个故障转移机制是非常有用并且是强烈推荐的。为此目的，Elasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片，或者直接叫**复制**。

**复制之所以重要，主要有两方面的原因**：

1、在分片/节点失败的情况下，**提供了高可用性**。因为这个原因，注意到复制分片从不与原/主要（original/primary）分片置于同一节点上是非常重要的；

2、扩展你的**搜索量/吞吐量**，因为搜索可以在所有的复制上并行运行。

总之，每个索引可以被分成多个分片。一个索引也可以被复制0次（意思是没有复制）或多次。一旦复制了，每个索引就有了主分片（作为复制源的原来的分片）和复制分片（主分片的拷贝）之别。分片和复制的数量可以在索引创建的时候指定。在索引创建之后，你可以在任何时候**动态地改变复制数量**，但是**不能改变分片的数量**。

默认情况下，Elasticsearch中的每个索引被分片1个主分片和1个复制（7.x之前），这意味着，如果你的集群中至少有两个节点，你的索引将会有5个主分片和另外5个复制分片（1个完全拷贝），这样的话每个索引总共就有10个分片。

一个索引的多个分片可以存放在集群中的一台主机上，也可以存放在多台主机上，这取决于你的集群机器数量。

主分片和复制分片的具体位置是由**ES内在的策略所决定**的。

## 1.11 节点

节点Node一个Elasticsearch的**运行实例**，是集群的构成单元；

## 1.12 集群

集群Cluster由一个或**多个节点组成**，对外提供服务。一个集群就是由一个或多个节点组织在一起，它们共同持有你整个的数据，并一起提供索引和搜索功能。

一个集群由一个**唯一的名字标识**，这个名字默认就是Elasticsearch。这个名字是重要的，因为一个节点只能通过指定某个集群的名字，来加入这个集群。

在产品环境中显式地设定这个名字是一个好习惯，但是使用默认值来进行测试/开发也是不错的。

## 1.13 别名

Elasticsearch的别名，类似关系型数据库的**视图**。别名不仅仅可以关联一个索引，它能聚合多个索引。

# 2 Rest API基础

通过URI（统一资源定位符）的来指定资源，通过资源的**表现形式**来操作资源，对资源的操作包括获取、创建、修改和删除资源，对应HTTP协议提供的GET、POST、PUT 和DELETE方法；Elasticsearch集群对外提供RESTful API；REST-REpresentational State Transfer；URI指定资源，如Index、Document等。

| 方法    | 操作                     |
| ------- | ------------------------ |
| GET     | 获取资源                 |
| POST    | 修改资源或更新现有资源   |
| DELETE  | 删除资源                 |
| PUT     | 插入资源或更新现有资源   |
| OPTIONS | 列举允许对资源进行的操作 |
| HEAD    | 返回Response Header      |

Http Method指明资源操作类型，如PUT（创建一个对象）、 DELETE（销毁对象）、POST（改变对象的当前状态）、GET（获取请求对象的状态）、HEAD（请求获取对象的基础信息）等。

## 2.1 交互方式

### 2.1.1 Curl

Curl命令行：语法相对不好写。

### 2.1.2 DevTools

Kibana DevTools：推荐使用，带自动提示功能。

### 2.1.3 Postman

Postman：推荐使用。

## 2.2 索引结构

下面是一个索引的基本构成！！！

```properties
{
    "search_model": {
    	// 索引别名
        "aliases": {},
        // 索引设置
        "settings": {},
        // 数据类型
        "mappings": {}
    }
}
```

### 2.2.1 Aliases

ElasticSearch的别名，类似关系型数据库的**视图**。

```properties
# 准备数据阶段
PUT l1/doc/1
{
  "title":"我想要睡你"
}

PUT l2/doc/1
{
  "title":"你却拿我当兄弟"
}

PUT l3/doc/1
{
  "title":"不过，我不介意"
}

# 创建别名给l1索引创建别名a1
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "l1",
        "alias": "a1"
      }
    }
  ]
}

# 查看别名
GET l1/_alias

# 删除别名
POST _aliases
{
  "actions": [
    {
      "remove": {
        "index": "l1",
        "alias": "a1"
      }
    }
  ]
}

# 重命名别名
POST _aliases
{
  "actions": [
    {
      "remove": {
        "index": "l1",
        "alias": "a1"
      }
    },
    {
      "add": {
        "index": "l2",
        "alias": "a1"
      }
    }
  ]
}

# 为多个索引指向同样的别名
POST _aliases
{
  "actions": [
    {"add": {"index": "l1", "alias": "a1"}},
    {"add": {"index": "l2", "alias": "a1"}},
    {"add": {"index": "l3", "alias": "a1"}}
  ]
}

# 使用indeices数组语法在一个操作中为多个索引指向别名
POST _aliases
{
  "actions": [
    {"add": {"indices": ["l1", "l2", "l3"], "alias": "a2"}}
  ]
}

# 一个索引指向多个别名
POST _aliases
{
  "actions": [
    {"add": {"index": "l1", "aliases": ["a1", "a2", "a3"]}}
  ]
}

# 普通查询和根据别名查询
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "l4",
        "alias": "a4",
        "filter": {
          "term": {
            "year": 2019
          }
        }
      }
    }
  ]
}

# 与路由连用
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "l4",
        "alias": "a4",
        "routing": "2"
      }
    }
  ]
}

POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "l4",
        "alias": "a4",
        "search_routing": "1,2",
        "index_routing": "1"
      }
    }
  ]
}
```

### 2.2.2 滚动索引

最早项目中没有Index滚动功能，随着数据增多Index变得巨大后效率急剧降低。后来加了逻辑，每个月滚动一次Index，但是这样还是不能应对每个Index差异化的数据增加速度。

正好要升级到ES 6.x，看到有`Rollover Index API`，特别记录一笔，以备将来可查。

```properties
# 1、创建索引，需要设置别名（或者通过logs-*创建索引模板），注意必须是短横线-+数字结尾。
# is_write_index如果不设置为true，logs_index将查不到数据
PUT /logs-000001
{
  "aliases": {
    "logs_index": {
      "is_write_index": true 
    }
  }
}

# 2、通过别名，写入数据
POST /logs_index/_doc
{
  "name": "feng"
}

POST /logs_index/_doc
{
  "name": "feng1"
}

# 3、执行rollover，新index的mappings、settings等
POST /logs_index/_rollover
{
  "conditions": {
    "max_docs": 2
  }
}

POST /logs_index/_doc
{
  "name": "feng"
}

POST /logs_index/_doc
{
  "name": "feng1"
}

POST /logs_index/_rollover
{
  "conditions": {
    "max_docs": 2
  }
}

# 4、查询信息
GET logs_index/_search
GET logs-000001/_search
GET logs-000002/_search

# 5、删除
DELETE logs-*
```

### 2.2.3 ILM周期

ILM一方面**简化了滚动索引**的定时调用的情况；另一方面控制了index中数据的生命周期！！！

Index Lifecycle Management(索引生命周期管理，后文简称 ILM)在**V6.6**第一次被引入beta版本，在**V6.7里GA版**本，它内部实现了cron 的工作，我们只需要去配置它的policy即可。

ILM一共将索引生命周期分为四个阶段 (Phase)：Hot阶段、Warm阶段、Cold阶段、Delete阶段

ES集群设置，默认pollinterval是10分钟，check一次，相当于cronjob的执行间隔是10分钟。

```properties
# 1、重点，创建一个策略（Create a lifecycle policy that defines the appropriate phases and actions.）
PUT /_ilm/policy/my_policy
{
  "policy":{
    "phases":{
      "hot":{
        "actions":{
          "rollover":{
          	"max_age":"1d",
            "max_docs":  5
          }
        }
      }
    }
  }
}

# 2、创建索引模板（Create an index template to apply the policy to each new index.）
# index.lifecycle.name           指明该索引应用的ILM Policy策略
# index.lifecycle.rollover_alias 指明在Rollover的时候使用的alias
PUT _template/my_template
{
  "index_patterns": ["test-*"], 
  "order":"10",
  "settings": {
    "index.lifecycle.name": "my_policy", 
    "index.lifecycle.rollover_alias": "test-alias"
  }
}

# 3、创建一个初始的索引（Bootstrap an index as the initial write index.）
PUT /test-000001 
{
  "aliases": {
    "test-alias":{
      "is_write_index": true 
    }
  }
} 

# 4、ILM Service会在后台轮询执行Policy
# 默认间隔时间为10分钟，为了更快地看到效果，我们将其修改为5秒，注意设置太小会不生效。
PUT _cluster/settings
{
  "persistent": {
    "indices.lifecycle.poll_interval":"5s"
  }
}

# 5、批量写数据
POST test-alias/_bulk
{"create":{"_id":1}}
{"text":"Some log message","@timestamp":"2016-07-04T01:00:00Z"}
{"create":{"_id":2}}
{"text":"Some log message","@timestamp":"2016-07-05T01:00:00Z"}
{"create":{"_id":3}}
{"text":"Some log message","@timestamp":"2016-07-04T01:00:00Z"}
{"create":{"_id":4}}
{"text":"Some log message","@timestamp":"2016-07-05T01:00:00Z"}
{"create":{"_id":5}}
{"text":"Some log message","@timestamp":"2016-07-04T01:00:00Z"}
{"create":{"_id":6}}
{"text":"Some log message","@timestamp":"2016-07-05T01:00:00Z"}

# 6、查询操作
GET test-alias/_ilm/explain

GET test-alias/_search
GET /test-000001/_search
GET /test-000002/_search
GET /test-000003/_search

# 删除
DELETE test-*
```

需要指定定期处理索引的

```properties
GET _cat/nodeattrs?s=attr

# 索引名以nginx_logs为前缀，且以每10个文档做一次Rollover，Rollover后5秒转为Warm阶段，Rollover后20 秒转为Cold阶段，Rollover后40秒删除
PUT /_ilm/policy/nginx_ilm_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_docs": "10"
          }
        }
      },
      "warm": {
        "min_age": "5s",
        "actions": {
          "allocate": {
            "include": {
              "box_type": "warm"
            }
          }
        }
      },
      "cold": {
        "min_age": "20s",
        "actions": {
          "allocate": {
            "include": {
              "box_type": "cold"
            }
          }
        }
      },
      "delete": {
        "min_age": "40s",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}

# index.routing.allocation.include.box_type 指明新建的索引都分配在hot节点上
PUT /_template/nginx_ilm_template
{
  "index_patterns": ["nginx_logs-*"],                 
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    "index.lifecycle.name": "nginx_ilm_policy",      
    "index.lifecycle.rollover_alias": "nginx_logs",
    "index.routing.allocation.include.box_type": "hot"
  }
}
```

### 2.2.4 Settings

通常主要来设置分片、副本、分词！！！

```properties
{
  "prod_search_info" : {
    "settings" : {
      "index" : {
        "number_of_shards" : "3", 
        "number_of_replicas" : "1",
        "analysis" : {
          "analyzer" : {
            "pinyin_analyzer" : {
              "tokenizer" : "my_pinyin"
            }
          },
          "tokenizer" : {
            "my_pinyin" : {
              "type" : "pinyin",
              "lowercase" : "true",
              "keep_original" : "true",
              "remove_duplicated_term" : "true",
              "keep_separate_first_letter" : "false",              
              "limit_first_letter_length" : "16",
              "keep_full_pinyin" : "true"
            }
          }
        }
      }
    }
  }
}
```

### 2.2.5 Mappings

通常主要来设置属性、模板！！！

```properties
{
  "prod_search_info" : {
    "mappings" : {
      "dynamic" : "true",
      "dynamic_date_formats" : [
        "yyyy-MM-dd HH:mm:ss",
        "yyyy-MM-dd"
      ],
      "dynamic_templates" : [
        {
          "strings-pinyin-text" : {
            "match" : "pinyin*",
            "match_mapping_type" : "string",
            "mapping" : {
              "fields" : {
                "pinyin" : {
                  "analyzer" : "pinyin_analyzer",
                  "term_vector" : "with_offsets",
                  "boost" : 10,
                  "store" : false,
                  "type" : "text"
                }
              },
              "type" : "keyword"
            }
          }
        },
        {
          "strings-ik-text" : {
            "match" : "message*",
            "match_mapping_type" : "string",
            "mapping" : {
              "analyzer" : "ik_max_word",
              "fields" : {
                "keyword" : {
                  "ignore_above" : 256,
                  "type" : "keyword"
                }
              },
              "search_analyzer" : "ik_smart",
              "type" : "text"
            }
          }
        },
        {
          "strings-ik-pinyin-text" : {
            "match" : "describe*",
            "match_mapping_type" : "string",
            "mapping" : {
              "analyzer" : "ik_max_word",
              "fields" : {
                "pinyin" : {
                  "analyzer" : "pinyin_analyzer",
                  "term_vector" : "with_offsets",
                  "boost" : 10,
                  "store" : false,
                  "type" : "text"
                }
              },
              "search_analyzer" : "ik_smart",
              "type" : "text"
            }
          }
        },
        {
          "strings-nested" : {
            "match" : "nested*",
            "match_mapping_type" : "object",
            "mapping" : {
              "type" : "nested"
            }
          }
        },
        {
          "strings-suggest" : {
            "match" : "suggest*",
            "match_mapping_type" : "string",
            "mapping" : {
              "analyzer" : "ik_max_word",
              "search_analyzer" : "ik_smart",
              "type" : "completion"
            }
          }
        },
        {
          "strings-keyword" : {
            "match_mapping_type" : "string",
            "mapping" : {
              "type" : "keyword"
            }
          }
        },
        {
          "strings-date" : {
            "match" : "dateTime*",
            "match_mapping_type" : "string",
            "mapping" : {
              "format" : "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis",
              "type" : "date"
            }
          }
        }
      ],
      "numeric_detection" : true,
      "properties" : {
        "dateTimeCreate" : {
          "type" : "date",
          "format" : "yyyy-MM-dd HH:mm:ss"
        },
        "dateTimeUpdate" : {
          "type" : "date",
          "format" : "yyyy-MM-dd HH:mm:ss"
        },
        "describeInfo" : {
          "type" : "text",
          "fields" : {
            "pinyin" : {
              "type" : "text",
              "boost" : 10.0,
              "term_vector" : "with_offsets",
              "analyzer" : "pinyin_analyzer"
            }
          },
          "analyzer" : "ik_max_word",
          "search_analyzer" : "ik_smart"
        },
        "id" : {
          "type" : "long"
        },
        "messageInfo" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          },
          "analyzer" : "ik_max_word",
          "search_analyzer" : "ik_smart"
        },
        "nestedStudentInfos" : {
          "type" : "nested",
          "properties" : {
            "comment" : {
              "type" : "keyword"
            },
            "dateTimeBirthday" : {
              "type" : "date",
              "format" : "yyyy-MM-dd HH:mm:ss"
            },
            "dateTimeCreate" : {
              "type" : "date",
              "format" : "yyyy-MM-dd HH:mm:ss"
            },
            "dateTimeUpdate" : {
              "type" : "date",
              "format" : "yyyy-MM-dd HH:mm:ss"
            },
            "id" : {
              "type" : "long"
            },
            "score" : {
              "type" : "long"
            },
            "studentAge" : {
              "type" : "long"
            },
            "studentName" : {
              "type" : "keyword"
            }
          }
        },
        "pinyinInfo" : {
          "type" : "keyword",
          "fields" : {
            "pinyin" : {
              "type" : "text",
              "boost" : 10.0,
              "term_vector" : "with_offsets",
              "analyzer" : "pinyin_analyzer"
            }
          }
        },
        "subject" : {
          "type" : "keyword"
        },
        "suggestInfo" : {
          "type" : "completion",
          "analyzer" : "ik_max_word",
          "search_analyzer" : "ik_smart",
          "preserve_separators" : true,
          "preserve_position_increments" : true,
          "max_input_length" : 50
        },
        "teacherAge" : {
          "type" : "long"
        },
        "teacherName" : {
          "type" : "keyword"
        }
      }
    }
  }
}
```

## 2.3 索引操作

ES有专门的Index API，用于创建、更新、删除索引配置等。

### 2.3.1 非结构化创建

非结构化创建语法：PUT prod_base

查看所有的indices：GET _cat/indices

查看创建后的结构：GET prod_base，可以发现mappings中是空的！

```json
# create index
PUT /prod_base

# 查看所有索引
GET _cat/indices

# delete index
DELETE prod_base

# 查看结构
GET /prod_base
```

### 2.3.2 结构化创建

结构化创建语法：PUT prod_base{ // 对应信息 }

```json
DELETE /prod_base

# 创建索引结构
PUT /prod_base
{
  "aliases": {
    "base-alias": {
      "is_write_index": true
    }
  },
  "settings": {
    // 分片数6.8之前默认是3个，7.3默认1个，创建后不能再修改
    "number_of_shards": 3,
    // 备份数，创建后是可以修改的
    "number_of_replicas": 1,
    "analysis": {}
  },
  "mappings": {
    // 注意外层不能指定types，7.x之前是可以指定的
    "properties": {
      "name": {
        "type": "text"
      },
      "age": {
        "type": "integer"
      },
      "date": {
        "type": "date",
        "format": [
          "yyyy-MM-dd HH:mm:ss",
          "yyyy-MM-dd"
        ]
      }
    }
  }
}
```

### 2.3.3 模板创建

后面会具体了解模板创建！！！

```properties
PUT prod_search_model/_doc/1
{
  // 对应JSON属性
}
```

### 2.3.4 删除索引

```properties
删除索引语法：DELETE prod_base
```

### 2.3.5 修改索引

```properties
修改语法：PUT /prod_base/_doc/mapping
```

### 2.3.6 查看索引

```properties
查看有哪些索引语法：GET /_cat/indices
查看指定索引的结构：GET /prod_base/_mapping
```

## 2.3 文档操作

创建文档的时候如果索引是不存在的，ES会自动创建对应的index和type。

### 2.3.1 指定ID创建文档

语法：PUT /prod_base/_doc/1   {"name":"guo","age":22}

```json
# create document can auto create index，default shards is 1 or replicas is 1
PUT /prod_base/_doc/1
{
  "name":"guo",
  "age":22
}
```

### 2.3.2 不指定ID创建文档

语法：POST /prod_base/_doc   {"name":"guo","age":22}

```json
# create document without id
POST /prod_base/_doc
{
  "name":"dong",
  "age":12
}
```

### 2.3.3 查询文档

语法：GET /prod_base/_doc/1

```json
# 指定id查询
GET /prod_base/_doc/1

# 查询整个索引
GET /prod_base/_search

# 条件查询
GET /prod_base/_search
{
  "query": {
    "term": {
      "_id": "1"
    }
  }
}

# 响应格式
{
  # 耗费了几毫秒
  "took" : 6,
  # 是否超时
  "timed_out" : false,
  # 分片信息
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  # 命中查询的所有结果
  "hits" : {
    # 查询结果的数量
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    # 相关度的匹配分数
    "max_score" : 1.0,
    # 包含了匹配搜索的document的详细数据，一般都有两个hits嵌套
    "hits" : [
      {}
      }
    ]
  }
}
```

### 2.3.4 批量操作文档

ES允许一次创建多个文档，从而减少网络传输开销，提升写入速率，endpoint为_bulk，action_type是指定要操作什么。

```json
# 批量操作 bulk api
POST _bulk
{"index":{"_index":"prod_base","_id":"1"}}
{"name":"alfred","age":20}
{"delete":{"_index":"prod_base","_id":"2"}}
{"update":{"_id":"3","_index":"prod_base"}}
{"doc":{"age":"20"}}
{"create":{"_index":"prod_base","_id":"4"}}
{"name":"guo","age":24}
```

注意：index（存在会覆盖）、delete、update、create（存在会报错）。

### 2.3.5 批量查询文档

ES允许一次查询多个文档，endpoint为_mget。

```json
# 批量查询 multi_get
GET /prod_base/_mget
{
  "docs": [
    {
      "_id": "1"
    },
    {
      "_id": "2"
    }
  ]
}


GET /prod_base/_mget
{
 "ids":[ "1","20" ]
}
```

### 2.3.6 更新文档

```properties
# 指定文档id更新
POST /prod_base/_update/1
{
  "script": {
    "source": "ctx._source.teacherAge = 30",
    "lang": "painless"
  }
}

# 全量更新
POST /prod_base/_update_by_query
{
  "script": {
    "source": "ctx._source.teacherName = '东东'",
    "lang": "painless"
  }
}

# 指定条件更新
POST /prod_base/_update_by_query
{
  "query": {
    "term": {
      "id": "1"
    }
  },
  "script": {
    "source": "ctx._source.teacherName='东东'"
  }
}

# 修改时间日期（当前时间加一天）
POST /prod_base/_update/1
{
  "script": {
    "lang": "painless",
    "source": """
      def date = ctx._source['dateTimeUpdate'].replace(' ', 'T');
      date += '.000Z';
      def new_date = Instant.parse(date);
      def value = 60 * 60 * 24;
      
      def date2 = new_date.minusSeconds(value).toString()
        .replace('T', ' ').replace('Z', '');
      ctx._source['dateTimeUpdate'] = date2;
"""
  }
}


# 修改nested中的数据
POST /prod_base/_update/1
{
  "script": {
    "lang": "painless",
    "source": """
      for(int i = ctx._source.nestedStudentInfos.length-1; i >= 0; i--) {
        def date = ctx._source.nestedStudentInfos[i].dateTimeBirthday.replace(' ', 'T');
        date += '.000Z';
                def new_date = Instant.parse(date);
                def date2 = new_date.toString()
        .replace('T', ' ').replace('Z', '').substring(0, 10).concat(" 00:00:01");
              ctx._source.nestedStudentInfos[i].dateTimeBirthday = date2;   
      }
"""
  }
}

POST /prod_base/_update/1
{
  "script": {
    "lang": "painless",
    "source": """
      for(int i = ctx._source.nestedStudentInfos.length-1; i >= 0; i--) {
        def date = ctx._source.nestedStudentInfos[i].dateTimeBirthday.replace(' ', 'T');
        date += '.000Z';
                def new_date = Instant.parse(date);
                def value = 60 * 60 * 24;
                def date2 = new_date.minusSeconds(value).toString()
        .replace('T', ' ').replace('Z', '').substring(0, 10).concat(" 00:00:00");
              ctx._source.nestedStudentInfos[i].dateTimeBirthday = date2;   
      }
"""
  }
}

POST /prod_search_info/_update_by_query
{
  "query": {
    "term": {
      "_id": "13"
    }
  },
  "script": {
    "lang": "painless",
    "source": """
      for(int i = ctx._source.nestedStudentInfos.length-1; i >= 0; i--) {
        def date = ctx._source.nestedStudentInfos[i].dateTimeCreate.replace(' ', 'T');
       def add = ctx._source.nestedStudentInfos[i].studentAge;
       def iadd = Integer.parseInt(add);
        date += '.000Z';
                def new_date = Instant.parse(date);
                def value = 60 * 60 * 24 * iadd;
                def date2 = new_date.plusSeconds(value).toString()
        .replace('T', ' ').replace('Z', '').substring(0, 10).concat(" 00:00:00");
              ctx._source.nestedStudentInfos[i].dateTimeCreate1 = date2;   
      }
"""
  }
}


POST /prod_medication_info/_update_by_query
{
  "query": {
    "bool": {
      "must": [
        {
          "exists": {
            "field": "duration"
          }
        }
      ],
      "must_not": [
        {
          "exists": {
            "field": "dateTimeEnd"
          }
        }
      ]
    }
  },
  "script": {
    "lang": "painless",
    "source": """
        def date = ctx._source.dateTimeStart.replace(' ', 'T');
       def add = ctx._source.duration;
       String str = String.valueOf(add);
        int idx = str.lastIndexOf(".");
        String strNum = str.substring(0, idx);
        int iadd = Integer.parseInt(strNum);
        date += '.000Z';
                def new_date = Instant.parse(date);
                def value = 60 * 60 * 24 * iadd;
                def date2 = new_date.plusSeconds(value).toString()
        .replace('T', ' ').replace('Z', '').substring(0, 10).concat(" 00:00:00");
              ctx._source.dateTimeEnd = date2;   
"""
  }
}
```

### 2.3.7 删除文档

```properties
# 根据id删除
DELETE /prod_base/_doc/1

# 指定条件删除
POST /prod_base/_delete_by_query
{
  "query": {
    "match": {
      "username": "郭东升"
    }
  }
}
```
