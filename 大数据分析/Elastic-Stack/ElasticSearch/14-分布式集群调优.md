# 1 生成环境部署建议

## 1.1 配置文件

系统设置要到位，遵照官方建议设置所有的系统参数，参见文档" Setup Elasticsearch-> Important System Configuration。

ES设置尽量简洁，elasticsearch.yml中尽量只写必备的参数，其他可以通过api动态设置的参数都通过api
来设定，参见文档" Setup Elasticsearch-> Important Elasticsearch Configuration。

随着ES的版本升级，很多网络流传的配置参数已经不再支持，因此不要随便复制别人的
集群配置参数

系统设置要到位，遵照官方建议设置所有的系统参数，参见文档" Setup Elasticsearch-> Important System Configuration，elasticsearch.yml中建议设定的基本参数

```properties
cluster.name
node.name
node.master/node.data/node.ingest
network.host建议显示指定为内网ip，不要偷懒直接设为0.0.0.0
discovery.zen.ping.unicast.hosts设定集群其他节点地址
discovery. zen.minimum_master_nodes一般设定为2
path.data/path.log
```

除上述参数外再根据需要增加其他的静态配置参数

## 1.2 动态设置

动态设定的参数有transient和persistent两种设置，前者在集群重启后会丢失，后者不会，但两种设定都会覆盖 elasticsearch.ym中的配置

```properties
PUT /_cluster/settings
{
  "persistent": {
    "discovery.zen.minimum_master_nodes": 2
  },
  "transient": {
    "indices.store.throttle.max_bytes_per_sec": "50mb"
  }
}
```

## 1.3 JVM内存设定

不要超过31GB，预留一半內存给操作系统，用来做文件缓存。

具体大小根据该node要存储的数据量来估算，为了保证性能，在**內存**和**数据量**间有一个建议的比例：搜索类项目的比例建议在1:16以内；日志类项目的比例建议在1:48~1:96。

假设总数据量大小为1TB，3个node，1个副本，那么每个node要存储的数据量为2TB/3=666GB，即700GB左右，做20%的预留空间，每个node要存储大约850GB的数据。

如果是搜索类项目，每个node内存大小为850GB/16=53GB，大于31GB；31*16=496，即每个node最多存储496GB数据，所以需要至少5个node。

如果是日志类型项目，每个node内存大小为850GB/48=18GB因此3个节点足够。

## 1.4 默认参数

# 2 设计阶段调优

## 2.1 基础

（1）根据业务增量需求，采取基于日期模板创建索引，通过 roll over API 滚动索引；

（2）使用别名进行索引管理；

（3）每天凌晨定时对索引做 force_merge 操作，以释放空间；

（4）采取冷热分离机制，热数据存储到 SSD，提高检索效率；冷数据定期进行 shrink操作，以缩减存储；

（5）采取 curator 进行索引的生命周期管理；

（6）仅针对需要分词的字段，合理的设置分词器；

（7）Mapping 阶段充分结合各个字段的属性，是否需要检索、是否需要存储等

# 3 写性能优化

（1）写入前副本数设置为 0；

（2）写入前关闭 refresh_interval 设置为-1，禁用刷新机制；

（3）写入过程中：采取 bulk 批量写入；

（4）写入后恢复副本数和刷新间隔；

（5）尽量使用自动生成的 id。

## 3.1 refresh

segment写入磁盘的过程依然很耗时，可以借助文件系统缓存的特性，先将 segment
在缓存中创健建并开放査询来进一步提升实时性，该过程在es中被称为 refresh.
在 refresh之前文档会先存储在—个 buffer中， refresh时将 buffer中的所有文档淸空
并生成 segment
es默认每1秒执行次 refres忭，因此文档的实时性被提高到1秒，这也是es被称为
近实时 Near real Time）的原因

## 3.2 translog

如果在内存中的 segment还没有写入磁盘前发生了宕机，那么其中的文栏无法恢复了，
如何解决这个问题？
es引入 translog机制。写入文档到 buffer时，同时将该操作写入 translog.
translog文件会即时写入磁盘（ fsync），6X默认每个请求都会落盘，可以修改为每5
秒写一次，这样风险便是丟失5秒内的数据，相关配置为 index translog*
es启动时会检查 translog文件，并从中恢复数据

```properties
PUT /prod_search_info/_settings
{
  "index.translog.durability": "async",
  "index.translog.sync_interval": "5s"
}
```



## 3.3 flush

flush负责将内存中的 segment写入磁盘，主要做如下的工作将translog写入磁盘，将index buffer清空，其中的文档生成一个新的segment，相当于个refresh操作，更新commit point并写入磁盘执行 fsync操作，将内存中的 segment写入磁盘，删除旧的 translog文件。

## 3.4 写性能优化

目标是增大写吞吐量-EPS（ Events per secorηd）越高越好，优化方案
客户端：多线程写，批量写
ES：在高质量数据建模的前提下，主要是在 refresh、 translog和秈sh之间做文章

一、目标为降低refresh的频率

增大 refresh_interval，降低实时性，以增大一次refresh处理的文栏数，默认是1s，设置为-1直接禁止自动refresh

增大index buffer size，参数为indices.memory.index_buffer_size（静态参数，需要设定在 elasticsearch.yml中），默认为10%。

二、目标是降低translog写磁盘的频率，从而提高写效率，但会降低容灾能力。

index.translog.durability设置为async，index.translog.sync_interval设置需要的大小，比如120，那么translog会改为每120s写一次磁盘

index.translog.flush_threshold_size默认为512mb，即translog超过该大小时会触发一次fush，那么调大该大小可以避免fush的发生。

三、目标为降低flush的次数，在6X可优化的点不多，多为es自动完成。

四、副本设置为0，写入完毕再增加。

五、合理地设计shard数，并保证shard均匀地分配在所有node上，充分利用所有node的资源。

index.routing.allocation.total_shards_per_node限定每个索引在每个node上可分配的总主副分片数。

5个node，某索引有10个主分片，1个副本，上述值应该设置为多少？

（10+10）/5=4，实际要设置为5个，防止在某个node下线时，分片迁移失败的问题

```properties
PUT /test/
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 0,
    "refresh_interval": "30s",
    "routing": {
      "allocation": {
        "total_shards_per_node": "3"
      }
    },
    "translog": {
      "sync_interval": "30s",
      "durability": "async"
    }
  }
}
```

# 4 读性能优化

读性能主要受以下几方面影响：数据模型是否符合业务模型？、数据规模是否过大？、索引配置是否优化？、查询语句是否优化？

（1）禁用 wildcard；

（2）禁用批量 terms（成百上千的场景）；

（3）充分利用倒排索引机制，能 keyword 类型尽量 keyword；

（4）数据量大时候，可以先基于时间敲定索引再检索；

（5）设置合理的路由机制。

## 4.1 数据建模

高质量的数据建模是优化的基础，将需要通过 script脚本动态计算的值提前算好作为字段存到文档中；尽量使得数据**模型贴近业务**模型。

根据不同的数据规模设定不同的SLA，上万条数据与上干万条数据性能肯定存在差异

索引配置优化主要包括如下，根据数据规模设置合理的主分片数，可以通过测试得到最适合的分片数；设置合理的副本数目，不是越多越好。

查询语句调优主要有以下几种常见手段：尽量使用Filter上下文，减少算分的场景，由于Filter有缓存机制，可以极大提升查询性能；尽量不使用script进行字段计算或者算分排序等；结合profile、explain API分析慢查询语句的症结所在，然后再去优化数据模型。

# 5 Shard数

ES的性能基本是线性扩展的，因此我们只要测出1个 Shard的性能指标，然后根据实际性能需求就能算出需要的 Shard数。比如单 Shard写入eps是1000，而线上eps需求是50000，那么你需要5个 shard.（实际还要考虑副本的情况）

## 5.1 测试流程

测试1个 Shard的流程如下
搭建与生产环境相同配置的**单节点**集群

设定一个单分片**零副本**的索引

写入实际生产数据进行测试，获取**写性能**指标

针对数据进行査询请求，获取**读性能**揹标

## 5.2 压测

压测工具可以采用esrally

压测的流程还是比较复杂，可以根据经验来设定。

如果是搜索引擎场景，单Shard大小不要超过15GB，如果是日志场景，单Shard大小不要超过50GB（ Shard越大，查询性能越低）

此时只要估算出你索引的总数据大小，然后再除以上面的单Shad大小也可以得到分片数

# 6 xpack监控

## 6.1 配置

官方推出的免费集群监控功能

