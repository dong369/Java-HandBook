# 1 生成环境部署建议

## 1.1 配置文件

系统设置要到位
遵照官方建议设置所有的系统参数
参见文档" Setup Elasticsearch-> Important System Configuration

ES设置尽量简洁
elasticsearch yn中尽量只写必备的参数，其他可以通过api动态设置的参数都通过api
来设定
参见文档" Setup Elasticsearch-> Important Elasticsearch Configuration
随着ES的版本升级，很多网络流传的配置参数已经不再支持，因此不要随便复制别人的
集群配置参数

系统设置要到位
遵照官方建议设置所有的系统参数
参见文档" Setup Elasticsearch-> Important System Configuration

elasticsearch.yml中建议设定的基本参数
cluster.name
node.name
node.master/node.data/node.ingest
network.host建议显示指定为内网ip，不要偷懒直接设为0.0.0.0
discovery.zen.ping.unicast.hosts设定集群其他节点地址
discovery. zen.minimum_master_nodes一般设定为2
path.data/path.log
除上述参数外再根据需要增加其他的静态配置参数

## 1.2 动态设置

动态设定的参数有 transient和 persistent两种设置，前者在集群重启后会丢失，后者不
会，但两种设定都会覆盖 elasticsearch ym中的配置



## 1.3 JVM内存设定

不要超过31GB，预留一半內存给操作系统，用来做文件缓存。

具体大小根据该node要存储的数据量来估算，为了保证性能，在內存和数据量间有一个建议的比例：搜索类项目的比例建议在1:16以内；日志类项目的比例建议在1:48~1:96。

假设总数据量大小为1TB，3个node，1个副本，那么每个node要存储的数据量为2TB/3=666GB，即700GB左右，做20%的预留空间，每个node要存储大约850GB的数据。

如果是搜索类项目，每个node内存大小为850GB/16=53GB，大于31GB；31*16=496，即每个node最多存储496GB数据，所以需要至少5个node。

如果是日志类型项目，每个node内存大小为850GB/48=18GB因此3个节点足够。

## 1.4 默认参数



# 2 写性能优化

## 2.1 refresh

segment写入磁盘的过程依然很耗时，可以借助文件系统缓存的特性，先将 segment
在缓存中创健建并开放査询来进一步提升实时性，该过程在es中被称为 refresh.
在 refresh之前文档会先存储在—个 buffer中， refresh时将 buffer中的所有文档淸空
并生成 segment
es默认每1秒执行次 refres忭，因此文档的实时性被提高到1秒，这也是es被称为
近实时 Near real Time）的原因

## 2.2 translog

如果在内存中的 segment还没有写入磁盘前发生了宕机，那么其中的文栏无法恢复了，
如何解决这个问题？
es引入 translog机制。写入文档到 buffer时，同时将该操作写入 translog.
translog文件会即时写入磁盘（ fsync），6X默认每个请求都会落盘，可以修改为每5
秒写一次，这样风险便是丟失5秒内的数据，相关配置为 index translog*
es启动时会检查 translog文件，并从中恢复数据

## 2.3 flush

fush负责将内存中的 segment写入磁盘，主要做如下的工作
将 translog写入磁盘
将 index buffer清空，其中的文档生成-个新的 segment，相当于个 refresh操作
更新 commit point并写入磁盘
执行 fsync操作，将内存中的 segment写入磁盘
删除旧的 translog文件

## 2.4 写性能优化

目标是增大写吞吐量-EPS（ Events per secorηd）越高越好
优化方案
客户端：多线程写，批量写
ES：在高质量数据建模的前提下，主要是在 refresh、 translog和秈sh之间做文章



目标为降低 refresh的频率，增大 refresh_interval，降低实时性，以增大一次 refresh处理的文栏数，默认是1s，设置为-1直接禁止自动 refresh增大 index buffer size，参数为 indices. memory index buffer size（静态参数，需要设定在 elasticsearch ym中），默认为10%。



目标是降低τ translog写磁盘的频率，从而提高写效率，但会降低容灾能力。index translog durability设置为 async， index. translog. sync interval设置需
要的大小，比如120，那么 translog会改为每120s写一次磁盘
index translog flush threshold size默认为512mb，即 translog超过该大小时会触发一次fush，那么调大该大小可以避免fush的发生。



目标为降低fush的次数，在6X可优化的点不多，多为es自动完成。



副本设置为0，写入完毕再增加。
合理地设计 shard数，并保证 shard均匀地分配在所有node上，充分利用所有node的资源
index. routing. allocation. total shards per node限定每个索引在每个node上可分配的总主副分片数
5个node，某索引有10个主分片，1个副本，上述值应该设置为多少？（10+10）/5=4
实际要设置为5个，防止在某个node下线时，分片迁移失败的问题

# 3 读性能优化

读性能优化
读性能主要受以下几方面影响：
数据模型是否符合业务模型？
数据规模是否过大？
索引配置是否优化？
查询语句是否优化？



读性能优化-数据建模
高质量的数据建模是优化的基础
将需要通过 script脚本动态计算的值提前算好作为字段存到文档中
尽量使得数据模型贴近业务模型





根据不同的数据规模设定不同的SLA
上万条数据与上干万条数据性能肯定存在差异



索引配置优化主要包括如下
根据数据规模设置合理的主分片数，可以通过测试得到最适合的分片数
设置合理的副本数目，不是越多越好



查询语句调优主要有以下几种常见手段：
尽量使用 Filter上下文，减少算分的场景，由于 Filter有缓存机制，可以极大提升查询性能
尽量不使用Scit进行字段计算或者算分排序等
结合 profile、 explain API分析慢查询语句的症结所在，然后再去优化数据模型



# 4 shard数

ES的性能基本是线性扩展的，因此我们只要测出1个 Shard的性能指标，然后根据实际
性能需求就能算出需要的 Shard数。比如单 Shard写入eps是1000，而线上eps需求
是50000，那么你需要5个 shard.（实际还要考虑副本的情况）
测试1个 Shard的流程如下
搭建与生产环境相同配置的单节点集群
设定一个单分片零副本的索引
写入实际生产数据进行测试，获取写性能指标
针对数据进行査询请求，获取读性能揹标
压测工具可以采用 esrally



压测的流程还是比较复杂，可以根据经验来设定。如果是搜索引擎场景，单 Shard大小不要超过15GB，如果是日志场景，单 Shard大小不要超过50GB（ Shard越大，查询性能越低）
此时只要估算出你索引的总数据大小，然后再除以上面的单Shad大小也可以得到分片数

# 5 xpack监控



