# 1 分布式介绍

```json
# cluster
GET _cluster/health

PUT test_index
{
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 1
  }
}
```

## 1.1 好处

ES支持集群模式，是一个分布式系统，其好处主要有两个：增大系统容量，如內存、磁盘，使得ES集群可以支持PB级的数据；提高系统可用性，即使部分节点停止服务，整个集群依然可以正常服务。

## 1.2 组成

ES集群由多个ES实例组成：不同集群通过集群名字来区分，可通过cluster.name进行修改，默认为lasticsearch；每个ES实例本质上是一个JVM进程，且有自己的名字，通过 node.name进行修改。

## 1.3 Cerebro插件

### 1.3.1 安装配置

用于监控集群的节点分布情况：https://github.com/lmenezes/cerebro

<img src="../../../插图\服务中间件\Elastic-Stack\cerebro集群管理.png"/>

### 1.3.2 其它配置

<img src="../../../插图/服务中间件/Elastic-Stack/关闭磁盘不够不分配shard.png"/>

### 1.3.3 基础命令

```
bin/cerebro -Dhttp.port=1234 -Dhttp.address=127.0.0.1
docker run -p 9000:9000 --env-file env-ldap  lmenezes/cerebro
```

# 2 集群构建理论

## 2.1 Cluster state

ES集群相关的数据称为cluster state集群状态，主要记录如下信息：节点信息，比如节点名称、连接地址等；索引信息，比如索引名称、配置等。

## 2.2 Master Node

可以修改cluster state的节点称为master节点，一个集群**只能有一个**。cluster state存储在每个节点上，**master维护最新版本并同步给其他节点**。master节点是通过集群中所有节点**选举产生**的，可以被选举的节点称为master-eligible节点，相关配置如下：node.master:true。

## 2.3 Coordinating Node

<img src="../../../插图\服务中间件\Elastic-Stack\ES三个节点的集群.png"/>

在下面的例子中，将所有的请求发送到 `Node 1` ，我们将其称为协调节点(coordinating node) 。处理请求的节点即为coordinating节点，该节点为所有节点的默认角色，不能取消。路由请求到正确的节点处理，比如创建索引的请求到master节点。

## 2.4 Data node

存储数据的节点即为data节点，默认节点都是data类型，相关配置如下：node.data:true。

## 2.5 新增一个节点

运行如下命令可以启动一个es节点实例

```json
bin/elasticsearch -E node.name=node01 -E cluster.name=bysocket_es_cluster -E path.data=node01_data -E http.port=5200 -d

bin/elasticsearch -E node.name=node02 -E cluster.name=bysocket_es_cluster -E path.data=node02_data -d

bin/elasticsearch -E node.name=node03 -E cluster.name=bysocket_es_cluster -E path.data=node03_data -d

bin/elasticsearch -E node.name=node04 -E cluster.name=bysocket_es_cluster -E path.data=node04_data -d

bin/elasticsearch -Ecluster.name=my_cluster -Epath.data=my_cluster_node1 -Enode.name =node1 -Ehttp.port=5200 -d
```

- node.name ： ES 节点名称，即实例名

- cluster.name ： ES 集群名称
- path.data ： 指定了存储文档数据目录

## 2.6 提高系统可用性

如何提高系统 的高可用？引入副本（ Replication）解决问题。

**服务可用性**：2个节点的情况下，允许其中1个节点停止服务。

**数据可用性**：引入副本（ Replication）解决；每个节点上都有完备的数据；自己备份自己的副本状态时yellow。

## 2.7 增大系统容量

**如何将数据分布于所有节点上**？引入分片（ Shard）解决问题。分片是ES支持PB级数据的基石：分片存储了部分数据，可以分布于任意节点上；分片数在索引创建时指定且后续不允许再更改，默认为1个（7.x之前是5个）；

分片有主分片和副本分片之分，以实现数据的**高可用**；

副本分片的数据由主分片同步，可以有多个，从而提高读取的**吞吐量**，可以动态指定副本分片数。

## 2.8 Cluster Health

通过如下API可以查看集群健康状况，包括以下三种：

green健康状态，指所有**主副分片**都正常分配；

yellow指所有**主分片都正常分配**，但是有**副本分片未正常分配**；

red有主分片**未分配**。

## 2.9 分片

每个索引都有多个分片，每个分片都是`Lucene`索引。

下图演示的是3个节点的集群中user-info的分片分布情况，创建时我们指定了3个分片和1个副本。

<img src="../../../插图\服务中间件\Elastic-Stack\分片.png"/>

**增加节点是否能提高user-info的数据容量？**不能。因为只有3个分片，已经分布在3台节点上，新增的节点无法利用。

**增加副本数是否能提高user-info的读取吞吐量？**不能。因为新增的副本也是分布在这3个节点上，还是利用了同样的资源。

如果要增加吞吐量，还需要新增节点。**分片数的设定很重要，需要提前规划好！！！**过小会导致后续无法通过增加节点实现水平扩容；过大会导致一个节点上分布过多分片，造成资源浪费，同时会影响查询性能。

## 2.10 故障转移

集群由3个节点组成，如下所示，此时集群状态是 green

步骤一：node1所在机器宕机导致服务终止，此时集群会如何处理？node2和node3发现node1无法响应一段时间后会发起master选举，比如这里选择node2为master节点，此时由于主分片P0下线，集群状态处于Red。

步骤二：node2发现主分片P0未分配，将R0提升为主分片。此时由于所有主分片都正常分配，集群状态变为 Yellow。

步骤三：node2为P0和P1生成新的副本，集群状态变成Green。

# 3 文档分布式存储

文栏最终会存储在分片上，如下图所示：Document1最终存储在分片P1上。

Document1是如何存储到分片P1的？选择P1的依据是什么？需要文档到分片的映射算法。

目的：使得文档均匀分布在所有分片上，以充分利用资源。

## 3.1 文档分布算法

随机选择或者round-robin(轮询)算法？不可取，因为需要维护文档到分片的映射关系，成本巨大。

根据文档值**实时计算**对应的分片，ES通过如下的公式计算文档对应的分片：shard = hash（routing）% number_of_primary_shards。

hash算法保证可以将数据均匀地分散在分片中。

routing是个关键参数，**默认是文档id**，也可以**自行指定**；将document路由到shard上面。

number_of_primary_shards是**主分片数**。

**该算法与主分片数相关，这也是分片数一旦确定后便不能更改的原因。**

## 3.2 文档创建流程

步骤一：Client向node3发起创建文档的请求。

步骤二：node3通过routing计算该文档应该存储在Shard1上，查询cluster state后确认主分片P1在node2上，然后转发创建文档的请求到node2上。

步骤三：node2的P1接收到请求后执行创建文档请求，将同样的请求发送到副本分片R1上。

步骤四：R1接收到请求后执行创建文档，通知P1成功的结果。

步骤五：P1接收到副本创建成功的结果后，通知node3创建成功。

步骤六：node3返回结果到client。

## 3.3 文档读取流程

步骤一：Client向node3发起获取文档id是1的请求。

步骤二：node3通过routing计算该文栏在Shard1上，查询cluster state后获取 Shard1的**主副分片**列表，然后以轮询的机制获取—个shard，比如这里是R1，然后转发读取文档的请求到node1。

步骤三：R1接收并执行读取文档请求后，将结果返回node3。

步骤四：node3返回结果到client。

## 3.4 文档批量创建流程

步骤一：Client向node3发起批量创建文档的请求（bulk）。

步骤二：node3通过routing计算**所有文档对应的shard**，然后按照**主shard分配(分组)**对应执行的操作，同时发送请求到涉及的主shard，比如这里3个主 shard都需要参与。

步骤三：主 shard接收并执行请求后，将同样的请求同步到对应的副本shard。

步骤四：副本 shard执行结果后返回结果到主 shard，主 shard再返回nde3。

步骤五：node3返回结果到client。

## 3.4 文档批量读取流程

步骤一：Client向node3发起批量获取文档的请求（mget）。

步骤二：node3通过 routing计算所有文栏对应的 shard，然后以轮询的机制获取要参与 shard，按照 shard构建mget请求，同时发送请求到涉及的 shard，比如这里有2个 shard需要参与。

步骤三：R1、R2返回文档结果。

步骤四：node3返回结果到client。

# 4 脑裂问题

## 4.1 产生原因

脑裂问题，英文为 split-brain，是分布式系统中的**经典网络问题（网络隔离造成的）**。

3个节点组成的集群，突然node1的网络和其他两个节点中断。

node2与node3会重新选举 master，比如node2成为了新 master，此时会更新cluster state；

node1自己组成集群后，也会更新 cluster state

同—个集群有两个 master，而且维护不同的 cluster state，网络恢复后无法选择正确的master

## 4.2 解决方案

为仅在可选举 master-eligible节点数大于等于quorun时才可以进行master选举。

quorum = master-eligible节点数 / 2+1，例如3个master-eligible节点时，quorum为2。

设定discovery.zen.minimum_master_nodes为quorum即可避免脑裂。

```yaml
discovery.zen.minimum_master_nodes: 2         # 为了避免脑裂，集群节点数最少为 半数+1
```

# 5 Shard详解

## 5.1 倒排索引

倒排索引一旦生成，不能更改。

### 5.1.1 好处

不用考虑并发写文件的问题，杜绝了锁机制带来的性能问题。

由于文件不再更改，可以充分利用文件系统缓存，只需载入一次，只要內存足够，对该文件的读取都会从内存读取，性能高。

利于生成缓存数据，利于对文件进行压缩存储，节省磁盘和内存存储空间。

### 5.1.2 坏处

需要写入新文档时，**必须重新构建倒排索引文件**，然后**替换老文件**后，新文档才能被检索，导致文栏实时性差。

## 5.2 文档搜索实时性

**解决方案**是新文档**直接生成新的倒排索引文件**，查询的时候**冋时査询所有的倒排文件**，然后做结果的汇总计算。开销小，提高了文档的实时性。

Lucene便是采用了这种方案，它构建的单个**倒排索引称为segment**，合在一起称为Index，是segment的集合。

ES中的Index概念不同，ES中是指document的一个集合。

ES中的一个Shard对应一个Lucene Index。

Lucene会有一个专门的文件来记录所有的segment信息，称为commit point。

### 5.2.1 Refresh

segment写入磁盘的过程依然很耗时，可以借助文件系统缓存的特性，先将segment在缓存(**内存**)中创建并开放査询来进步提升实时性，该过程在ES中被称为refresh。

在refresh之前文档会先存储在一个buffer（缓冲队列）中，refresh时将buffer中的所有文档淸空并生成segment。

ES默认每1秒执行次refresh，因此文档的实时性被提高到1秒，这也是ES被称为近实时（Near Real time）的原因。

refresh发生的时机主要有如下几种情况：

1. 间隔时间达到时；通过indexsettings.refresh_interval来设定，默认是1秒；

2. index. buffer占满时，其大小通过 indices.memory.index_buffer_size设置，默认为 jvm heap的10%，所有 shard共享；

3. fush发生时也会发生 refresh。

### 5.2.2 Translog

如果在内存中的segment还没有写入磁盘前发生了宕机，那么其中的文档就无法恢复了，如何解决这个问题？

ES引入translog机制。写入文栏到index buffer时，同时将该操作写入translog。translog文件会即时写入磁盘（fsync），6.×默认每个请求都会落盘，可以修改为每5秒写一次，这样风险便是丢失5秒内的数据，相关配置为 index.translog.*。

ES启动时会检查 translog文件，并从中恢复数据。

<img src="../../../插图\服务中间件\Elastic-Stack\近实时操作.png"/>

### 5.2.3 Flush

flush负责将内存中的segment写入磁盘。主要做如下的工作：将translog写入磁盘；将index buffer清空，其中的文档生成-个新的segment，相当于个refresh操作；更新 commit point并写入磁盘；执行fync操作，将内存中的 segment写入磁盘；删除旧的translog文件。

flush发生的时机主要有如下几种情况：

1. 间隔时间达到时，默认是30分钟，5X之前可以通过index.translog.flush_threshold_period修改，之后**无法修改**；
2. translog占满时，其大小可以通过index.translog.flush_threshold_size控制，默认是512mb，每个 index有自己的 translog。

<img src="../../../插图\服务中间件\Elastic-Stack\时时刷新.png"/>

### 5.2.4 删除与更新文档

segment一旦生成就不能更改，那么如果你要删除文档该如何操作？Lucene专门维护一个.del的文件，记录所有已经删除的文档，注意.del上记录的是文栏在 Lucene内部的id，在查询结果返回前会过滤掉.del中的所有文档。

更新文档如何进行呢？首先删除文档，然后再创建新文档。

## 5.3 整体视角

<img src="../../../插图\服务中间件\Elastic-Stack\整观ES&Lucene.png"/>

## 5.4 Segment Merging

随着segment的增多，由于一次査询的segment数增多，查询速度会变慢；

ES会定时在后台进行segment merge的操作，减少segment的数量；

通过force_merge API可以手动强制做segment merge的操作。

# 6 Master选举

## 6.1 步骤

（1）只有候选主节点（master：true）的节点才能成为主节点。

（2）最小主节点数（min_master_nodes）的目的是防止脑裂。

核对了一下代码，核心入口为 findMaster，选择主节点成功返回对应 Master，否则返回 null。选举流程大致描述如下：

第一步：确认候选主节点数达标，elasticsearch.yml 设置的值

discovery.zen.minimum_master_nodes；

第二步：比较：先判定是否具备 master 资格，具备候选主节点资格的优先返回；

若两节点都为候选主节点，则 id 小的值会主节点。注意这里的 id 为 string 类型。

题外话：获取节点 id 的方法。

GET /_cat/nodes?v&h=ip,port,heapPercent,heapMax,id,name

2ip port heapPercent heapMax id name



（1）Elasticsearch 的选主是 ZenDiscovery 模块负责的，主要包含 Ping（节点之间通过这个 RPC 来发现彼此）和 Unicast（单播模块包含一个主机列表以控制哪些节点需要 ping 通）这两部分；

（2）对所有可以成为 master 的节点（node.master: true）根据 nodeId 字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第 0 位）节点，暂且认为它是 master 节点。

（3）如果对某个节点的投票数达到一定的值（可以成为 master 节点数 n/2+1）并且该节点自己也选举自己，那这个节点就是 master。否则重新选举一直到满足上述条件。

（4）补充：master 节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data 节点可以关闭 http 功能

# 7 集群构建规划

ES 集群架构 13 个节点，索引根据通道不同共 20+索引，根据日期，每日递增 20+，索引：10 分片，每日递增 1 亿+数据，每个通道每天索引大小控制：150GB 之内。

## 7.1 机器配置

假设每天抓取50万条数据 每条数据8K 抓取的数据同步到ES中进行检索统计 数据保存三年

一月 119G、一年 1.4T、二年 2.8T、三年 4.3T

机器规格：8C 32G 2TSSD云盘，集群配置：1个副本，
最小磁盘总大小 = 源数据 * 3.4；node数 = 最小磁盘总大小 / 2T
每个shard 50G 每个node支持 1.4/0.05 = 28个shard (每个node的disk控制在70%以下～2TB可以用1.4TB的样子)

一年的集群配置：3个node 3 * 28/2 = 42shard

二年的集群配置：5个node 5 * 28/2 = 70shard

三年的集群配置：7个node 7 * 28/2 = 98shard

出于经济成本的考虑 先购买3个node 但分片数=98 这样的话 当后面增加节点的时候 不用reshard了 这样可行吗？

系统：Centos 7.4 x64，**关闭防火墙**，并且创建用户组和用户。

软件包：elasticsearch-7.3.0-linux-x86_64.tar.gz

| 机器          | 开放端口         | 安装应用 |
| ------------- | ---------------- | -------- |
| 192.168.7.217 | 5601；9000；9200 | JDK 1.8  |
| 192.168.7.218 | 9200             | JDK 1.8  |
| 192.168.7.219 | 9200             | JDK 1.8  |

打算使用ES对抓取下来的文章进行检索, 文章数据量预计是3T左右， 假设机器规格是 `8C 32G` 每个shard**两个**副本 那么这么一个集群需要多少台机器呢？

根据[阿里云](https://help.aliyun.com/document_detail/72660.html?spm=a2c4g.11186623.6.549.45914de0AD4KoZ)给的公式：最小磁盘总大小 = 源数据 * (1 + 副本数量) * 1.7 = 3 * 3 * 1.7 = 15.3

SSD 云盘最大存储空间支持2T那么=> 需要 15.3 / 2 ~= 8 个node

建议单节点上同一索引的shard个数不要超5个。每个node的disk控制在70%以下～2TB可以用1.4TB的样子

## 7.2 存储空间规划

| 节点类型   | CPU  | 内存 | 硬盘      | JVM内存 |
| ---------- | ---- | ---- | --------- | ------- |
| 热数据节点 | 16核 | 64G  | 2TB SSD   | 31G     |
| 冷数据节点 | 56核 | 128G | 20TB SATA | 64G     |

打算使用ES对抓取下来的文章进行检索, 文章数据量预计是3T左右， 假设机器规格是 `8C 32G` 每个shard**两个**副本 那么这么一个集群需要多少台机器呢？

## 7.3 冷热分离

类似关系型数据库中的读写分离！！！

- Hot Phase
  - Rollover 滚动索引操作，可用在索引大小或者文档数达到某设定值时，创建新的索引用于数据读写，从而控制单个索引的大小。这里要注意的一点是，如果启用了 Rollover，那么所有阶段的时间不再以索引创建时间为准，而是以该索引 Rollover 的时间为准。
- Warm Phase
  - Allocate 设定副本数、修改分片分配规则 (如将分片迁移到中等配置的节点上) 等
  - Read-Onlly 设定当前索引为只读状态
  - Force Merge 合并 segment 操作
  - Shrink 缩小 shard 分片数
- Cold Phase
  - Allocate 同上
- Delete Phase
  - Delete 删除

```properties
node.attr.box_type: hot（warm、cold、delete）
```

## 7.4 滚动索引

类似关系型数据库中的分库分表操作！！！

## 7.5 节点配置

```properties
vim /etc/security/limits.conf
* soft nofile 65537
* hard nofile 65537
* soft nproc 65537
* hard nproc 65537
```



```properties
vim /etc/sysctl.conf
vm.max_map_count = 262144
net.core.somaxconn = 65535
net.ipv4.ip_forward = 1
```



```properties
cd /usr/local/src
tar -zxv -f elasticsearch-7.3.0-linux-x86_64.tar.gz -C /usr/local/
cd /usr/local/
cp elasticsearch-7.3.0 elasticsearch-7.3.0_node1
cp elasticsearch-7.3.0 elasticsearch-7.3.0_node2
cp elasticsearch-7.3.0 elasticsearch-7.3.0_node3

groupadd esgroup
useradd esuser -g esgroup -p 123456

chown -R esuser:esgroup elasticsearch-7.3.0_node1
chown -R esuser:esgroup elasticsearch-7.3.0_node2
chown -R esuser:esgroup elasticsearch-7.3.0_node3
```



```properties
# 节点1，主节点
## 集群名称
cluster.name: es-prod-tj
## 节点的名字，一般为Master或者Slave
node.name: node-master
## 节点是否为Master，设置为true的话，说明此节点为Master节点
node.master: true
## 是数据节点
node.data: true
## 设置网络，如果是本机的话就是127.0.0.1，其他服务器配置对应的IP地址即可(0.0.0.0支持外网访问)
network.host: 0.0.0.0
## 设置对外服务的Http端口，默认为 9200，可以修改默认设置
http.port: 9200
## 设置节点间交互的TCP端口，默认是9300
transport.port: 9300
## 集群发现
discovery.seed_hosts: ["192.168.7.217:9300","192.168.7.218:9300","192.168.7.219:9300"]
## 手动指定可以成为Master的所有节点的Name或者IP，这些配置将会在第一次选举中进行计算
cluster.initial_master_nodes: ["node-master"]
## 设置支持Elasticsearch-Head
http.cors.enabled: true
http.cors.allow-origin: "*"
## 为了避免脑裂，集群节点数最少为 半数+1
discovery.zen.minimum_master_nodes: 2
## 数据文件和日志文件最好配置自己的路径，便于后期的迁移
path.data: /path/to/data
path.logs: /path/to/logs

# 节点2
cluster.name: es-prod-tj
node.name: node-slave01
node.master: false
node.data: true 
network.host: 0.0.0.0
http.port: 9200
transport.port: 9300
discovery.seed_hosts: ["192.168.7.217:9300","192.168.7.218:9300","192.168.7.219:9300"]
cluster.initial_master_nodes: ["node-master", "node-slave01","node-slave02"]
http.cors.enabled: true
http.cors.allow-origin: "*"
path.data: /path/to/data
path.logs: /path/to/logs

# 节点3
cluster.name: es-prod-tj
node.name: node-slave02
node.master: false
node.data: true 
network.host: 0.0.0.0
http.port: 9200
transport.port: 9300
discovery.seed_hosts: ["192.168.7.217:9300","192.168.7.218:9300","192.168.7.219:9300"]
cluster.initial_master_nodes: ["node-master", "node-slave01","node-slave02"]
http.cors.enabled: true
http.cors.allow-origin: "*"
path.data: /path/to/data
path.logs: /path/to/logs
```

若报错说找不到主节点，可以先启动主节点，等主节点集群建立后，再启动从节点，观察从节点日志输出，确保从节点加入集群。分别启动的话需要先切换到 elastic 普通用户，然后运行`/usr/local/elasticsearch-7.3.0-cluster-node1/bin/elasticsearch`，可以先分别启动查看状态，待配置无误后再用脚本启动

启动脚本

```
#!/bin/bash

/usr/bin/su - elastic -c '/usr/local/elasticsearch-7.3.0-cluster-node1/bin/elasticsearch -p /tmp/elasticsearch_9200_pid -d'
/usr/bin/su - elastic -c '/usr/local/elasticsearch-7.3.0-cluster-node2/bin/elasticsearch -p /tmp/elasticsearch_9201_pid -d'
/usr/bin/su - elastic -c '/usr/local/elasticsearch-7.3.0-cluster-node3/bin/elasticsearch -p /tmp/elasticsearch_9202_pid -d'
```

关闭脚本

```
#!/bin/bash

kill -9 `ps -u elastic|awk '{print $1}'`
```

## 7.6 集群节点测试

查看集群主从分配：`http://192.168.0.160:9200/_cat/nodes?v`

```
ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
192.168.0.160           35          97   0    0.04    0.23     0.22 di        -      node-3
192.168.0.160           15          97   0    0.04    0.23     0.22 dim       *      node-1
192.168.0.160           31          97   0    0.04    0.23     0.22 di        -      node-2
```

查看集群状态：`http://192.168.0.160:9200/_cluster/health?pretty`

```json
{
  "cluster_name": "my-application",
  "status": "green",
  "timed_out": false,
  "number_of_nodes": 3,
  "number_of_data_nodes": 3,
  "active_primary_shards": 0,
  "active_shards": 0,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 0,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 100.0
}
```

## 7.7 特别注意

1. 一定要关闭防火墙！！！
2. 如果是直接拷贝的elasticsearch安装文件，一定要清除原来data目录下面的nodes！！！

# 8 安全认证功能

在6.8之前免费版本并不包含安全认证功能，之后版本有开放一些基础认证功能，对于普通用户来说是够用的。

免费版本

1. TLS 功能，可对通信进行加密
2. 文件和原生Realm，可用于创建和管理用户
3. 基于角色的访问控制，可用于控制用户对集群 API 和索引的访问权限；
4. 通过针对Kibana Spaces 的安全功能，还可允许在 Kibana 中实现多租户。

收费版本包含更丰富的安全功能，比如：

1. 日志审计
2. IP过滤
3. LDAP、PKI和活动目录身份验证
4. 单点登录身份验证（SAML、Kerberos）
5. 基于属性的权限控制
6. 字段和文档级别安全性
7. 静态数据加密支持

需要同时在ES和kibana端开启安全认证功能，下面介绍如何操作。

## 8.1 ES端配置

### 8.1.1 启动安全模块

1. 打开ES配置文件 `$ES_PATH_CONF/elasticsearch.yml` 添加设置：`xpack.security.enabled：true`，在免费版本中此项设置是禁用的，所以需要显式打开它。
2. 非集群模式下启用单节点发现 `discovery.type: single-node`。如果你是在ES集群中配置则忽略本项，因为集群模式下要配置TLS。

启用Elasticsearch安全功能时默认情况下会启用基本身份验证，要与集群通信你必须指定用户名和密码，除非启用匿名访问，否则所有不包含用户名和密码的请求都将被拒绝。

### 8.1.2 创建密码

Elastic Stack安全功能提供内置的用户凭据可帮助你启动运行ES，这些用户具有一组固定的权限，在设置密码之前无法进行身份验证，其中elastic用户可以用来设置所有内置的用户密码。

上面的内置用户存储在一个特殊 `.security` 索引中，该索引由Elasticsearch管理。如果禁用内置用户或其密码更改，则更改将自动反映在集群中的每个节点上。但是，如果从快照中删除或恢复索引，则已应用的任何更改都将丢失。

1. 先创建keystore文件 `bin/elasticsearch-keystore create`。
2. 在Elasticsearch目录中运行命令：`./bin/elasticsearch-setup-passwords interactive`，回车之后为每一个用户设置独立的密码。记住ES实例必须启动。
3. 你需要在后续步骤中使用这些内置用户，因此务必牢记前面设置的密码！

### 8.1.3 主节点配置TLS

> 如果你在操作单节点ES则可以跳过本内容。

Elastic Stack安全功能使你可以加密来自Elasticsearch集群的流量。使用传输层安全性（TLS）来保护连接，传统层安全性通常称为“SSL”。为每个Elasticsearch节点生成私钥和X.509证书

1. 生成CA证书 `bin/elasticsearch-certutil ca`，将产生新文件 `elastic-stack-ca.p12`。该 `elasticsearch-certutil` 命令还会提示你输入密码以保护文件和密钥，请保留该文件的副本并记住其密码。
2. 为集群中的每个节点生成证书和私钥 `bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12`，将产生新文件 `elastic-certificates.p12`。系统还会提示你输入密码，你可以输入证书和密钥的密码，也可以按Enter键将密码留空。默认情况下 `elasticsearch-certutil` 生成没有主机名信息的证书，这意味着你可以将证书用于集群中的每个节点，另外要关闭主机名验证。
3. 将 `elastic-certificates.p12` 文件复制到每个节点上Elasticsearch配置目录中。例如，`/home/es/config/certs`。无需将 `elastic-stack-ca.p12` 文件复制到此目录。

```properties
# 生成机器证书
bin/elasticsearch-certutil ca
bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12

# 移动到配置文件夹下
mkdir config/certs
mv elastic-certificates.p12 config/certs/
```

### 8.1.4 分发子节点

配置集群中的每个节点以使用其签名证书标识自身并在传输层上启用TLS

1、从主节点获取生产的证书信息

```properties
scp -r esuser@s20:/config/certs /config/
scp -r esuser@s20:/config/elasticsearch.keystore /config/
```

2、启用TLS并指定访问节点证书所需的信息，将以下信息添加到每个节点的 `elasticsearch.yml` 文件中：

```properties
xpack.security.enabled: true

xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.verification_mode: certificate 
xpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12 
xpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12
```

3、如果你在创建证书时输入了密码，那可以通过下面的方法设置。

```properties
bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password

bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password
```

4、重启Elasticsearch

配置为使用TLS的节点无法与使用未加密网络的节点通信（反之亦然）。启用TLS后，必须重新启动所有节点才能保持群集之间的通信。

## 8.2 客户端配置

### 8.2.1 Kibana

```properties
elasticsearch.username: "elastic"
elasticsearch.password: "passw0rd"
```

# 9 扩容迁移

## 9.1 扩容

查询网上资料说是当Elasticsearch所在磁盘占用大于等于**95%**时，Elasticsearch会把所有相关索引**自动置为只读**。往ES中写数据会报错：[FORBIDDEN/12/index read-only / allow delete (api)] - read only elasticsearch indices 

```properties
PUT /prod_test/_settings
{
	"index.blocks.read_only_allow_delete": null
}

PUT /_all/_settings
{
	"index.blocks.read_only_allow_delete": false
}
```

## 9.2 迁移

升级或者修改配置的时候，一定不要上去就改。

1.cp一份新的

2.和目前系统并行试试，或者关了测试环境（正式）。