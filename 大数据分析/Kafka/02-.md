Kafka
----------------------
	分布式流计算平台。
	类似于消息系统发布订阅数据流。
	以分布式、副本集群方式存储数据流。
	实时处理数据流。
	
	构建实时数据流管道，水平可伸缩，容错，速度快
	
	特点
	----------------
		1.巨量数据，TB级
		2.高吞吐量
			支持每秒中百万消息。
		3.分布式。
			支持在多个server之间进行消息分区。
		4.多客户端支持
			和多语言进行协同。

组件
--------
	1.broker			//kafka服务器	zk
		log + index
		test3-1			//主题 + 分区号
		zk znode : /brokers/topics/test3/partitions/0|1|2/state(leader isr(101,102,broker id))
	
	2.Topic				//主题			zk
	3.producer			//生产者		no zk
	4.consumer			//消费者		zk


编写简单的生产者
-------------
	package com.it18zhang.mykafka01001;
	
	import java.util.Properties;
	
	import kafka.javaapi.producer.Producer;
	import kafka.producer.KeyedMessage;
	import kafka.producer.ProducerConfig;
	
	/**
	 * 简单生产者
	 */
	@SuppressWarnings("deprecation")
	public class SimpleProducer {
		
		private static Producer<Integer, String> producer;
		
		private static final Properties props = new Properties();
	
		public static void main(String[] args) {
			//创建配置信息
			props.put("metadata.broker.list", "s101:9092");
			props.put("serializer.class", "kafka.serializer.StringEncoder");
			props.put("request.required.acks", "1");
			producer = new Producer<Integer, String>(new ProducerConfig(props));
			
			KeyedMessage<Integer, String> msg = new KeyedMessage<Integer, String>("test3", "hello world from win7 client!");
			producer.send(msg);
			System.out.println("over");
		}
	}
	
	/**
	 * 简单生产者
	 */
	public class SimpleProducer1 {
		public static void main(String[] args) {
			Properties props = new Properties();
			props.put("bootstrap.servers", "s101:9092");	//broker list
			props.put("acks", "all");
			props.put("retries", 0);
			props.put("batch.size", 16384);
			props.put("linger.ms", 1);
			props.put("buffer.memory", 33554432);
			props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
			props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
	
			Producer<String, String> producer = new KafkaProducer<String, String>(props);
			for (int i = 0; i < 100; i++)
				producer.send(new ProducerRecord<String, String>("test3", Integer.toString(i), "hello world - " + i));
			producer.close();
		}
	}

KafkaProducer发送消息分析
---------------------------
	生成者是线程安全的，维护了本地buffer pool，发送消息，消息进入pool，
	send方式异步，立刻返回。
	ack=all导致记录完全提交时阻塞，
												对消息进行批处理
	p.send(rec) -->p.doSend(rec,callback) --> interceptors.onSend() --> 对key+value进行串行化 -> 计算分区 ->
	
	->计算总size -> 创建TopicPartition对象 ->创建回调拦截器 -> **** Sender *****

使用带有分区函数的生产者发送消息
---------------------------------
	//new API
	public class SimplePartitioner2 implements Partitioner {
	
		public void configure(Map<String, ?> configs) {
		}
	
		public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
			return 1;
		}
	
		public void close() {
		}
	}
	
	public static void main(String[] args) {
		Properties props = new Properties();
		props.put("bootstrap.servers", "s101:9092");	//broker list
		props.put("acks", "all");
		props.put("retries", 0);
		props.put("batch.size", 16384);
		props.put("linger.ms", 1);
		props.put("buffer.memory", 33554432);
		props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
		props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
		props.put("partitioner", "com.it18zhang.mykafka01001.SimplePartitioner2");
		
		Producer<String, String> producer = new KafkaProducer<String, String>(props);
		for (int i = 0; i < 5; i++){
			ProducerRecord<String, String> rec = new ProducerRecord<String, String>("test3", Integer.toString(i), "WWW" + i);
			producer.send(rec, new Callback() {
				public void onCompletion(RecordMetadata metadata, Exception exception) {
					System.out.println("ack !!");
				}
			});
		}
		producer.close();
		System.out.println("over");
	}

Consumer
-----------------
	消费者.
	package com.it18zhang.mykafka01001;
	
	import java.util.HashMap;
	import java.util.Iterator;
	import java.util.List;
	import java.util.Map;
	import java.util.Properties;
	
	import kafka.consumer.Consumer;
	import kafka.consumer.ConsumerConfig;
	import kafka.consumer.ConsumerIterator;
	import kafka.consumer.KafkaStream;
	import kafka.javaapi.consumer.ConsumerConnector;
	
	public class SimpleConsumer1 {
		public static void main(String[] args) {
			Properties props = new Properties();
			props.put("zookeeper.connect", "s101:2181");
			props.put("group.id", "g1");
			props.put("zookeeper.session.timeout.ms", "500");
			props.put("zookeeper.sync.time.ms", "250");
			props.put("auto.commit.interval.ms", "1000");
			
			//创建消费者配置对象
			ConsumerConfig conf = new ConsumerConfig(props);
			//创建连接器
			ConsumerConnector conn = Consumer.createJavaConsumerConnector(conf);
			
			//
			Map<String, Integer> topicCount = new HashMap<String, Integer>();
			topicCount.put("test3", 1);
			//Key == topic
			Map<String, List<KafkaStream<byte[], byte[]>>> map = conn.createMessageStreams(topicCount);
			List<KafkaStream<byte[], byte[]>> list = map.get("test3");
			for(KafkaStream<byte[], byte[]> s : list){
				ConsumerIterator<byte[], byte[]> it = s.iterator();
				while(it.hasNext()){
					String msg = new String(it.next().message());
					System.out.println(msg);
				}
			}
			conn.shutdown();
		}
	}

/consumers/g1
/consumers/g1/ids
/consumers/g1/ids/g1_Thinkpad-PC-1475218254850-a0080320
/consumers/g1/ids/g1_Thinkpad-PC-1475218293130-89728e2a
/consumers/g1/owners
/consumers/g1/owners/test3
/consumers/g1/owners/test3/0
/consumers/g1/owners/test3/1
/consumers/g1/owners/test3/2
/consumers/g1/offsets
/consumers/g1/offsets/test3
/consumers/g1/offsets/test3/0
/consumers/g1/offsets/test3/1
/consumers/g1/offsets/test3/2

消费者消费的偏移量记载的是消费者组消费的该分区的消息个数。


flume整合kafka
--------------------
	1.kafka Source
		tier1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource
		tier1.sources.source1.channels = c1
		tier1.sources.source1.zookeeperConnect = s101:2181
		tier1.sources.source1.topic = test3
		tier1.sources.source1.groupId = g1
		tier1.sources.source1.kafka.consumer.timeout.ms = 100
	
	2.Kafka Sink
		a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
		a1.sinks.k1.topic = test3
		a1.sinks.k1.brokerList = s101:9092
		a1.sinks.k1.requiredAcks = 1
		a1.sinks.k1.batchSize = 20
		a1.sinks.k1.channel = c1
	
	3.kafka Channel
		a1.sources = r1
		a1.sinks = k1
		a1.channels = c1
	
		# Describe/configure the source
		a1.sources.r1.type = netcat
		a1.sources.r1.bind = 0.0.0.0
		a1.sources.r1.port = 8888
	
		# Describe the sink
		a1.sinks.k1.type = logger
	
		# Use a channel which buffers events in memory
		a1.channels.c1.type   = org.apache.flume.channel.kafka.KafkaChannel
		a1.channels.c1.capacity = 10000
		a1.channels.c1.transactionCapacity = 1000
		a1.channels.c1.brokerList=s101:9092,s102:9092
		a1.channels.c1.topic=test3
		a1.channels.c1.zookeeperConnect=s101:2181
		a1.channels.c1.parseAsFlumeEvent=false		//*****?????
	
		# Bind the source and sink to the channel
		a1.sources.r1.channels = c1
		a1.sinks.k1.channel = c1


​	