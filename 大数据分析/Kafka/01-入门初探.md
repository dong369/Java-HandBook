# 1 基础认识

## 1.1 Hive

```properties

```

## 1.2 Flume

flume--channel Selector:

	replicating
	multiplexing


1.3 Sink processor
--------------------
	load balancing，提供负载均衡的功能。


1.4 Load_balance
-------------------
	a1.sources
	a1.sinks
	a1.channels
	
	a1.sinkgroups = g1
	a1.sinkgroups.g1.sinks = k1 k2
	a1.sinkgroups.g1.processor.type = load_balance
	a1.sinkgroups.g1.processor.backoff = true
	a1.sinkgroups.g1.processor.selector = random

1.5 Failover
-------------------
	a1.sinkgroups = g1
	a1.sinkgroups.g1.sinks = k1 k2
	a1.sinkgroups.g1.processor.type = failover
	a1.sinkgroups.g1.processor.priority.k1 = 5
	a1.sinkgroups.g1.processor.priority.k2 = 10
	a1.sinkgroups.g1.processor.maxpenalty = 10000

## 1.6 Timestamp

使用拦截器:-timestamp

	a1.sources = r1
	a1.channels = c1
	a1.sources.r1.channels =  c1
	a1.sources.r1.type = seq
	a1.sources.r1.interceptors = i1
	a1.sources.r1.interceptors.i1.type = timestamp

1.7 JMS
------------
	Java message service
	middle ware，中间件技术

1.8 Queue
----------------------
	队列模式。
	p2p:点对点。

1.9 Publish Subscribe
----------------------
	主题模式，发布订阅模式。

2 Kafka
======================

整合了queue和publish subscribe进行了统一，就是通过消费者组的模式。

	分布式流计算平台。
	类似于消息系统发布订阅数据流。
	以分布式、副本集群方式存储数据流。
	实时处理数据流。
	
	构建实时数据流管道，水平可伸缩，容错，速度快
	
	特点
	----------------
		1.巨量数据，TB级
		2.高吞吐量
			支持每秒中百万消息。
		3.分布式。
			支持在多个server之间进行消息分区。
		4.多客户端支持
			和多语言进行协同。


2.1 安装
-------------------
	1、jdk1.8+
	2、下载
	3、tar开kafka_2.12-2.5.0.tgz


2.2 体验
-------------
	1、启动服务器
		a)启动zk，kafka自带的zk
			$>zookeeper-server-start.sh config/zookeeper.properties &
		b)启动kafka服务器
			$>kafka-server-start.sh config/server.properties &
			
	2、创建topic
		a)创建主题
			#创建一个分区，一个副本的主题
			$>kafka-topics.sh --zookeeper s10:2181 --create --topic test --partitions 1 --replication-factor 1
			$>zkCli.sh -server localhost:2181			//通过zk客户端命令观察zk的数据结构
		b)列出主题
			$>kafka-topics.sh --list --zookeeper s10:2181
			
	3、发送消息
		$>kafka-console-producer.sh --broker-list s10:9092 --topic test
			>helloworld
			>how are you
			
	4.消费消息
		$>kafka-console-consumer.sh --bootstrap-server s10:9092 --topic test --from-beginning

## 2.3 伪集群

搭建多broker的kafka集群

	1、创建多个server配置文件.
		$>cp server.properties server-1.properties
			[server-1.properties]
			broker.id=1
			listeners=PLAINTEXT://:9092		
			log.dir=/tmp/kafka-logs-1
			
		$>cp server.properties server-2.properties
			[server-1.properties]
			broker.id=2
			listeners=PLAINTEXT://:9093
			log.dir=/tmp/kafka-logs-2	
			
		$>cp server.properties server-2.properties
			[server-1.properties]
			broker.id=2
			listeners=PLAINTEXT://:9094
			log.dir=/tmp/kafka-logs-3
			...
		
	2、启动server
		$>kafka-server-start.sh /opt/soft/bigdata/kafka/config/server-1.properties &
		$>kafka-server-start.sh /opt/soft/bigdata/kafka/config/server-2.properties &
		$>kafka-server-start.sh /opt/soft/bigdata/kafka/config/server-3.properties &
	
	3、创建主题(3个副本)
		a)创建主题
		$>kafka-topics.sh --create --zookeeper s10:2181 --replication-factor 3 --partitions 1       --topic test
		
		b)查看主题主题的内容
		$>kafka-topics.sh --describe --zookeeper s10:2181 --topic test		//查看原有主题
			
	5.发布新消息给主题
		$>kafka-console-producer.sh --bootstrap-server s10:9092,s10:9093,s10:9094 --topic test
	
	6.消费消息
		$>kafka-console-consumer.sh --bootstrap-server s10:9092 --topic test --from-beginning
	
	7.容错测试
		a)找到server-1进程
		$>ps -Af | grep server-1
		b)杀死进程server-1
		kill pid
		c)查看主题描述
		$>kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
		d)启动消费者消费主题消息
		$>kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic test

2.4 消息系统
------------------
	zk				// 协同系统
	broker			// 代理，kafka server，并不维护哪个消费者消费了消息。
	producer		// 生产者
	consumer group	// 消费者组，每个组中只有一个消费者可以消费消息
	consumer		// 消费者，维护了消费的消息状态。
	topic			// 主题

2.5 核心部分
------------------
	1、消息缓存与FileSystem的存储，数据被即刻写入OS内核页，并缓存以及清理磁盘(可以配置)
	2、消息被消费后，kafka长时间驻留消息,如有必要，可以实现重复消费。
	3、对[分组消息]使用消息set，放置网络过载.
	4、使用消费者保持着消息[元数据]。
	5、消费者状态默认存在zk中，也可以存在于其他OLTP中。
	6、kafka中的生产和消费是pull-push（推拉）模式.
	7、kafa没有主从模式，所有broker地位相同。broker元数据均在zk中维护，并在producer和consumer之间共享。
	8、kafka的LB(load balance)策略运行producer动态发现broker.
	9、producer维护了broker的连接池，并能够通过zk的warcher call机制实时进行更新。
	10、producer可以选择同步或异步的方式向broker发送消息。


2.6 消息压缩
-------------------
	1、proceduer压缩消息(GZIP + snappy)，consumer解压缩。
	2、压缩的消息的没有深度限制。
	3、在message的header中有一个compress type。
	    x x --> 压缩类型(0:未压缩)
		-----------------
		| | | | | | | | |
		-----------------

2.7 镜像
------------------
	将源集群的数据副本化到target kafka集群。

## 2.8 完全分布式

构建完全分布式kafka集群

	1.使用外部zk集群存放kafka的数据
		a)启动s101-s103的zk
			$>ssh s101 zkServer.sh start
			$>ssh s102 zkServer.sh start
			$>ssh s103 zkServer.sh start
	
	2.复制s100的kafka + environment等到s101 ~ s103.
		[kafka文件]
		$>rsync -lrv /soft/kafka /soft/kafka-xxx-xxx ubuntu@s101:/soft/
		$>rsync -lrv /soft/kafka /soft/kafka-xxx-xxx ubuntu@s102:/soft/
		$>rsync -lrv /soft/kafka /soft/kafka-xxx-xxx ubuntu@s103:/soft/
		
		[环境变量]
		$>rsync /etc/environment root@s101:/etc/
		$>rsync /etc/environment root@s102:/etc/
		$>rsync /etc/environment root@s103:/etc/
	
	3.配置文件
		[kafka/conf/server.properties]
		broker.id=103
		listeners=PLAINTEXT://:9092
		num.network.threads=3
		num.io.threads=8
		socket.send.buffer.bytes=102400
		socket.receive.buffer.bytes=102400
		socket.request.max.bytes=104857600
		log.dirs=/home/ubuntu/kafka/logs
		num.partitions=1
		num.recovery.threads.per.data.dir=1
		log.retention.hours=168
		log.segment.bytes=1073741824
		log.retention.check.interval.ms=300000
		zookeeper.connect=s101:2181 s102:2181 s103:2181
		zookeeper.connection.timeout.ms=6000
	4.创建/home/ubuntu/kafka/logs目录
		$>ssh s101 mkdir -p /home/ubuntu/kafka/logs
	
	5.启动zk
		$>ssh s101 zkServer.sh start
		$>ssh s102 zkServer.sh start
		$>ssh s103 zkServer.sh start
	
	6.启动kafka server
		$>ssh s101 kafka-server-start.sh /soft/kafka/config/server.properties
	
	7.创建topic
		$>kafka-topics.sh --create --zookeeper s101:2181,s102:2181,s103:2181 --replication-factor 3 --partitions 2 --topic test
	
	8.使用消息生产发送消息
		$>kafka-console-producer.sh --broker-list s101:9092,s102:9092 --topic test
	
	9.producer不在zk中注册。
	
	10.consumer在zk中注册
		/consumers
		/consumers/console-consumer-9891
		/consumers/console-consumer-9891/ids
		/consumers/console-consumer-9891/ids/console-consumer-9891_s101-1475138088384-27cf8e73
		/consumers/console-consumer-9891/owners
		/consumers/console-consumer-9891/owners/test
		/consumers/console-consumer-9891/owners/test/0
		/consumers/console-consumer-9891/owners/test/1
	
	11、副本数无法修改,只能在创建topic时进行指定。
		$>kafka-topics.sh --crate --zookeeper s101:2181 --replication-factor 2 --topic test
	    
	12、可以给副本指定到不同的broker上去
		$>kafka-topics.sh --crate --zookeeper s101:2181 --replication-factor 2 --topic test  

2.9 zk的znode
-----------------
	1./controller//data={"version":1,"brokerid":101,"timestamp":"1475134340990"} ==> leader
	2./controller_epoch		//data=1
	3./brokers/ids		
	  /brokers/ids/101		//实时维护的active的broker.
	  /brokers/ids/102
	  /brokers/ids/103
	
	  /brokers/topics/
	  /brokers/topics/test/partitions/0/state	//data={"controller_epoch":2,"leader":103,"version":1,"leader_epoch":0,"isr":[103,101,102]}
	  /brokers/topics/test/partitions/1/state	//data={"controller_epoch":2,"leader":101,"version":1,"leader_epoch":0,"isr":[103,101,102]}
		
	  /brokers/seqid
	
	 4./admin/delete_topics
	 5./isr_change_notification
	 6./consumers/
	 7./config
	   /config/changes
	   /config/clients
	   /config/topics

2.10 本地目录结构
------------------
	./.lock
	./recovery-point-offset-checkpoint
	./replication-offset-checkpoint
	./meta.properties
	./test-1
	
	./test-1/00000000000000000000.log			//存放kafka消息的地方
	./test-1/00000000000000000000.index
	./test-0
	./test-0/00000000000000000000.log
	./test-0/00000000000000000000.index
	./cleaner-offset-checkpoint

2.11 副本机制
-----------------

配置文件

	delete.topic.enable


log4j:WARN No appenders could be found for logger (org.apache.zookeeper.ZooKeeper).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
/
/controller_epoch
/controller
/brokers
/brokers/ids
/brokers/ids/101
/brokers/ids/102
/brokers/ids/103
/brokers/topics
/brokers/topics/test2
/brokers/topics/test2/partitions
/brokers/topics/test2/partitions/0
/brokers/topics/test2/partitions/0/state
/brokers/topics/test2/partitions/1
/brokers/topics/test2/partitions/1/state
/brokers/topics/test2/partitions/2
/brokers/topics/test2/partitions/2/state
/brokers/topics/test2/partitions/3
/brokers/topics/test2/partitions/3/state
/brokers/topics/test2/partitions/4
/brokers/topics/test2/partitions/4/state
/brokers/topics/test
/brokers/topics/test/partitions
/brokers/topics/test/partitions/0
/brokers/topics/test/partitions/0/state
/brokers/topics/test/partitions/1
/brokers/topics/test/partitions/1/state
/brokers/seqid

/admin
/admin/delete_topics
/admin/delete_topics/test
/isr_change_notification
/consumers
/config
/config/changes
/config/clients
/config/topics
/config/topics/test2
/config/topics/test


log4j:WARN No appenders could be found for logger (org.apache.zookeeper.ZooKeeper).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
/
/controller_epoch
/controller
/brokers
/brokers/ids
/brokers/ids/101
/brokers/ids/102
/brokers/ids/103
/brokers/topics
/brokers/topics/test2
/brokers/topics/test2/partitions
/brokers/topics/test2/partitions/0
/brokers/topics/test2/partitions/0/state
/brokers/topics/test2/partitions/1
/brokers/topics/test2/partitions/1/state
/brokers/topics/test2/partitions/2
/brokers/topics/test2/partitions/2/state
/brokers/topics/test2/partitions/3
/brokers/topics/test2/partitions/3/state
/brokers/topics/test2/partitions/4
/brokers/topics/test2/partitions/4/state

/brokers/topics/test3
/brokers/topics/test3/partitions			  副本位置			本地目录
/brokers/topics/test3/partitions/0			//102 103			test3-0
/brokers/topics/test3/partitions/0/state
/brokers/topics/test3/partitions/1			//101 103			
/brokers/topics/test3/partitions/1/state
/brokers/topics/test3/partitions/2			//101 102			//
/brokers/topics/test3/partitions/2/state

s101	:test3-1 test3-2
s102	:test3-0 test3-2
s103	:test3-0 test3-1

--replica-assignment s101
brokers: s101:s101,    s102:s102,    s103:s103,
		 test3-0 test3-0  test3-1 test3-1  test3-2 tests-2
part : 0 1 2
repl : 0 1


replication factor x partitions  / brokers

kafka副本机制

	每个分区有n个副本，可以承受n-1节点故障。每个副本都有自己的leader，其余的就是follow.
	zk中存放分区的leader和all replica的信息。(get /brokers/topics/test3/partitions/0/state)
	
	每个副本存储消息的部分数据在本地的log和offset中,周期性同步到disk.确保消息写入全部副本或者其中一个。
	
	leader故障时，消息或者写入本地log，或者在producer在收到ack消息前，resent partitoin给new leader.
	
	kafka支持的副本模型
	----------------------
		1.同步复制
			procudure从zk中找leader，并发送message,消息立即写入本地log，而且follow开pull 消息。
			每个follow将消息写各自的本地log后，向leader发送确认回执，leader在收到所有的follow确认
			回执和本地副本的写入工作均完成后，再向producer发送确认回执。
	
			消费者的数据pull从leader中完成。
	
		2.异步复制
			leader的本地log写入完成即向producer发送确认回执。

## 2.12 提取zk节点树

```java
package cn.hacz.zookeeper;

import org.apache.zookeeper.*;
import org.junit.Before;
import org.junit.Test;
import org.springframework.util.StringUtils;

import java.io.IOException;
import java.util.List;

/**
 * The class/interface
 *
 * @author guodd
 * @version 1.0 use jdk 1.8
 */
public class ZkTest {
    String zkServer = "s10:2181,s10:2182,s10:2183";
    private ZooKeeper zk;

    @Before
    public void before() throws IOException {
        zk = new ZooKeeper(zkServer, 5000, null);
    }

    @Test
    public void create() throws Exception {
        if (StringUtils.isEmpty(zk.exists("/test", null))) {
            String s = zk.create("/test", "java".getBytes(), 
                                 ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            System.out.println(s);
        }
    }

    @Test
    public void deleteZk() throws Exception {
        zk.delete("/test/aa", 0);
    }

    @Test
    public void zkTree() throws Exception {
        printZNode("/");
    }

    private void printZNode(String path) throws Exception {
        System.out.println(path);
        List<String> list = zk.getChildren(path, false);
        if (path.equals("/")) {
            path = "";
        }
        for (String p : list) {
            printZNode(path + "/" + p);
        }
    }
}
```

