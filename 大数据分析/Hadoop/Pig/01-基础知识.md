1 pig
==================

	类似hive.
	大数据集的数据分析平台。
	pig有编译器，最终生成一系列mr.
	pig latin
	----------
		易于编写
		优化处理。
		可扩展。
	
	pig				mr

---------------------------

	数据流语言		数据处理过程
	高级语言		低级语言
	连接操作很简单	  很难
	SQL人员易于使用	java编程
	多次查询		20代码量
	不需要编译，	较长编译过程
	转成MR
	
	pig					sql

---------------------------

	过程是语言			声明时语言
	
	schema可选，可以	必须的
	不用shema存数据
	
	数据模型是嵌套关系	扁平的
	
	查询优化有限		sql优化机会更多

ETL (Extract, Transform, and Load) functions.
--------------------------------------------

	pig(数据结构丰富)	hive

--------------------------------------

	pig latin(yahoo)	hive QL	(facebook)
	数据流				查询处理
	过程式语言，适用	声明式
	管线编程
	
	结构化、半结构、	主要是结构化数据
	非结构


体验pig
------------------

	1.准备
		jdk1.6+
	2.下载pig
		pig-0.15.0.tar.gz
	3.tar
	4.mv
	5.配置path+ pig_home
		[/etc/environment]
		...
		PIG_HOME=/soft/pig
		PATH=...:/soft/pig/bin
	6.生效pig
		$>source /etc/environment
	7.验证安装是否成功
		$>pig -version


pig执行模式 
-------------

	1.local
		所有文件都在本地，不需要hadoop支持。for test
		$>pig -x local
		$grunt>
	
	2.map reduce
		处理的数据在hdfs上，基于hdfs的操作。
		$>pig -x mapreduce
		$grunt>



pig执行机制
-----------------

	1.交互模式(grunt shell)
		输入 - 执行 - 输出
	2.batch mode(批处理模式)
		编写pig为扩展名的pig脚本。
	3.embed mode(嵌入式)
		编写udf，在脚本中使用。

pig shell常用命令
-------------------

	$grunt>quit			//退出 ctrl + d
	$grunt>help			//帮助
	$grunt>sh xxx		//运行shell命令
	$grunt>fs -lsr		//fs === hdfs dfs -lsr
	$grunt>kill -lsr	//hadoop job
	$grunt>set -lsr		//
	$grunt>customers = load '/home/ubuntu/customer.txt' using PigStorage(',') ;		//PigStorage区分大小写
	$grunt>dump customers ;	//转储数据(输出)
	
	#以脚本方式运行pig程序
	0.准备数据
		[customers.txt]
		1,tom,12
		2,tomas,13
		3,tomasLee,11
		4,tomson,19
		5,tommon,18
	
	1.编写test.pig
		[/home/ubuntu/pig]
		customers = load '/home/ubuntu/pig/customers.txt' using PigStorage(',');					#local
		#customers = load 'hdfs://mycluster/user/ubuntu/data/customers.txt' using PigStorage(',');	#mr
		dump customers ;			#
	2.运行脚本
		$pig -x local /home/ubuntu/pig/test.pig
		$pig -x mapreduce /home/ubuntu/pig/test.pig



pig架构
---------------

	pig将脚本转换成一些列mr，
	Parser：初始处理脚本的组件，语法检查、类型检查、相关检查，该阶段的输出是DAG(direct acycle graph,有向无环图)
	        使用pig latin语句和操作符进行表示。在DAG中，逻辑操作符表示节点，数据流表示边。
	
	Optimizer：优化器，具有逻辑优化的算法（投影|pushdown）
	
	Compiler:编译器
			 将plan编译成一些列mr.
	
	Execution engine:执行引擎
					 mr有序提交给hadoop，在yarn上执行，并输出结果。


pig数据model
----------------

	嵌套数据类型，可以实现更加复杂类型（map / tuple元组）
	
	tuple(id,name,xxx,..)	//是dimension,维度 field, 有序的字段集合，类似于record(row)
							//使用(Raja, 30)
	Bag						//是tuple的集合,是无序的tuple集合。使用{}表示，类似于table.
							//tuple的字段个数可以不用。
							//{Raja, 30, {9848022338, raja@gmail.com,}}嵌套
	
	relation				//是元组的包,是包的特列。
							//{(..),(..),(..)}
	
	map						//key-value对，key是chararray并唯一。value可以是任何类型，使用[]表示
							//[name#Raja, age#30]

pig shell常用命令
-------------------

	$grunt>quit			//退出 ctrl + d
	$grunt>help			//帮助
	$grunt>sh xxx		//运行shell命令
	$grunt>fs -lsr		//fs === hdfs dfs -lsr
	$grunt>kill -lsr	//hadoop job
	$grunt>set -lsr		//
	$grunt>customers = load '/home/ubuntu/customer.txt' using PigStorage(',') ;		//PigStorage区分大小写
	$grunt>dump customers ;	//转储数据(输出)
	
	$grunt>clear								//
	$grunt>history								//历史命令
	$grunt>set job.name pig						//设置key的value
	$grunt>exec	/home/ubuntu/pig/test.pig		//执行pig脚本
	$grunt>run /xx								//同exec，run执行脚本命令在history中可见，exec不可见。
	$grunt>


pig latin
--------------------------

	操纵的数据都是relation(元组的包).包括表达式和schema(表结构)
	使用pig latin操作符()
	除了load和store之外，其余所有的操作都已relation为input，output新的relation
	dump时才会执行mr job。


pig类型构造操作符
--------------------

	1.()			//构造tuple,(Raju, 30)
	2.{}			//构造bag,{(Raju, 30), (Mohammad, 45)}
	3.[]			//map,[name#Raja, age#30]

pig关系操作符
---------------------

	[加载/存储]
	LOAD			//加载数据到关系中
	STORE			//保存关系到hdfs/local
	
	[filter]
	filter						//删除不需要的tuple
	distinct					//去重
	foreach ... generate		//基于数据的列，生成新数据。
	stream						//使用外部程序变换数据
	
	[group | join]
	join			//连接多个关系
	cogrup			//使用多个关系分组数据
	group			//使用一个关系分组
	cross			//多个关系的cross结果
	
	[sorting]
	order			//基于多个字段排序
	limit			//分页
	
	[Combining and Splitting]
	union			//合成多个关系到一个关系
	split			//将一个关系分割成多个关系
	
	[Diagnostic Operators,诊断操作]
	dump			//输出关系内容
	describe		//打印关系的schema描述
	explain			//针对ralation，输出执行计划
	ILLUSTRATE		//查看每个执行语句的逐步操作。


​	

----------------------------------------

准备数据
	[students.txt]
	001,Rajiv,Reddy,9848022337,Hyderabad
	002,siddarth,Battacharya,9848022338,Kolkata
	003,Rajesh,Khanna,9848022339,Delhi
	004,Preethi,Agarwal,9848022330,Pune
	005,Trupthi,Mohanthy,9848022336,Bhuwaneshwar
	006,Archana,Mishra,9848022335,Chennai

读取数据
-----------

	students = load ...

存储数据
-----------

	store Relation_name INTO ' required_directory_path ' [USING function];
	store Relation_name INTO ' required_directory_path ' using PigStorage(',');
	store Relation_name INTO ' required_directory_path ' using PigStorage('\t');

诊断操作
---------------

	1.dump
		触发mr，打印关系数据
	2.describe
		查看schema，
	3.Explanation
		显示执行计划
	
	4.Illustration
		阐述关系运算的逐步过程


001,Rajiv,Reddy,21,9848022337,Hyderabad
002,siddarth,Battacharya,22,9848022338,Kolkata
003,Rajesh,Khanna,22,9848022339,Delhi
004,Preethi,Agarwal,21,9848022330,Pune
005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar
006,Archana,Mishra,23,9848022335,Chennai
007,Komal,Nayak,24,9848022334,trivendram 
008,Bharathi,Nambiayar,24,9848022333,Chennai

group
-------------

	group r by f ;		//按照f进行分组
	1.按照单列分组
		g = group s by $3 ;	//
		[结果]
		(21,{(004,Preethi,Agarwal,21,9848022330,Pune),
			 (001,Rajiv,Reddy,21,9848022337,Hyderabad)
			}
		)
	
	2.按照多个列分组
		group s by (age,city);
		结果:
		((21,Pune),{(4,Preethi,Agarwal,21,9848022330,Pune)})
		((21,Hyderabad),{(1,Rajiv,Reddy,21,9848022337,Hyderabad)})
		((22,Delhi),{(3,Rajesh,Khanna,22,9848022339,Delhi)})
		((22,Kolkata),{(2,siddarth,Battacharya,22,9848022338,Kolkata)})
		((23,Chennai),{(6,Archana,Mishra,23,9848022335,Chennai)})
		((23,Bhuwaneshwar),{(5,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar)})
		((24,Chennai),{(8,Bharathi,Nambiayar,24,9848022333,Chennai)})
		((24,trivendram),{(7,Komal,Nayak,24,9848022334,trivendram)})
	
	3.全分组,将整个关系分成一组。
		g = group s all ;
		[结果]
		(all,{(8,Bharathi,Nambiayar,24,9848022333,Chennai),
			  (7,Komal,Nayak,24,9848022334,trivendram),
			  (6,Archana,Mishra,23,9848022335,Chennai),
			  (5,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar),
			  (4,Preethi,Agarwal,21,9848022330,Pune),
			  (3,Rajesh,Khanna,22,9848022339,Delhi),
			  (2,siddarth,Battacharya,22,9848022338,Kolkata),
			  (1,Rajiv,Reddy,21,9848022337,Hyderabad)})
	
	4.cogroup对多个关系进行分组操作，合并之后的分组。
		准备
		[s.data]
		...
	
		[e.data]
		001,Robin,22,newyork
		002,BOB,23,Kolkata
		003,Maya,23,Tokyo
		004,Sara,25,London
		005,David,23,Bhuwaneshwar
		006,Maggy,22,Chennai
	
		g = cogroup s by age , e by age ;
		[结果]
		(group,{s bag},{e bag})
		-----------------------
		(21,{(4,Preethi,Agarwal,21,9848022330,Pune),(1,Rajiv,Reddy,21,9848022337,Hyderabad)},{})
		(22,{(3,Rajesh,Khanna,22,9848022339,Delhi),(2,siddarth,Battacharya,22,9848022338,Kolkata)},
			{(6,Maggy,22,Chennai),(1,Robin,22,newyork)})

join
------------------

	0.准备数据
		[c.data]
		1,Ramesh,32,Ahmedabad,2000.00
		2,Khilan,25,Delhi,1500.00
		3,kaushik,23,Kota,2000.00
		4,Chaitali,25,Mumbai,6500.00
		5,Hardik,27,Bhopal,8500.00
		6,Komal,22,MP,4500.00
		7,Muffy,24,Indore,10000.00
	
		[o.data]
		102,2009-10-08 00:00:00,3,3000
		100,2009-10-08 00:00:00,3,1500
		101,2009-11-20 00:00:00,2,1560
		103,2008-05-20 00:00:00,4,2060
	
		[加载关系]
		c = LOAD '/home/ubuntu/pig/c.data' USING PigStorage(',')as (id:int, name:chararray, age:int, address:chararray, salary:int);
		o = LOAD '/home/ubuntu/pig/o.data' USING PigStorage(',')as (oid:int, date:chararray, customer_id:int, amount:int);
	
	1.inner join(等值连接)
		r = join r1 by key1 , r2 by key2 ;					//语法
		r = JOIN c BY id, o BY customer_id;					//例子
		(2,Khilan,25,Delhi,1500,101,2009-11-20 00:00:00,2,1560)
		(3,kaushik,23,Kota,2000,100,2009-10-08 00:00:00,3,1500)
		(3,kaushik,23,Kota,2000,102,2009-10-08 00:00:00,3,3000)
		(4,Chaitali,25,Mumbai,6500,103,2008-05-20 00:00:00,4,2060)
	
	2.self join
		r = join c by id , c by id ;
		(1,Ramesh,32,Ahmedabad,2000,1,Ramesh,32,Ahmedabad,2000)
		(2,Khilan,25,Delhi,1500,2,Khilan,25,Delhi,1500)
		(3,kaushik,23,Kota,2000,3,kaushik,23,Kota,2000)
		(4,Chaitali,25,Mumbai,6500,4,Chaitali,25,Mumbai,6500)
		(5,Hardik,27,Bhopal,8500,5,Hardik,27,Bhopal,8500)
		(6,Komal,22,MP,4500,6,Komal,22,MP,4500)
		(7,Muffy,24,Indore,10000,7,Muffy,24,Indore,10000)
	
	3.left outer join
		r = join c by id left outer , o by customer_id ;
		(1,Ramesh,32,Ahmedabad,2000,,,,)
		(2,Khilan,25,Delhi,1500,101,2009-11-20 00:00:00,2,1560)
		(3,kaushik,23,Kota,2000,100,2009-10-08 00:00:00,3,1500)
		(3,kaushik,23,Kota,2000,102,2009-10-08 00:00:00,3,3000)
		(4,Chaitali,25,Mumbai,6500,103,2008-05-20 00:00:00,4,2060)
		(5,Hardik,27,Bhopal,8500,,,,)
		(6,Komal,22,MP,4500,,,,)
		(7,Muffy,24,Indore,10000,,,,)
	
	4.right outer
		r = join c by id right outer , o by customer_id ;
		(2,Khilan,25,Delhi,1500,101,2009-11-20 00:00:00,2,1560)
		(3,kaushik,23,Kota,2000,100,2009-10-08 00:00:00,3,1500)
		(3,kaushik,23,Kota,2000,102,2009-10-08 00:00:00,3,3000)
		(4,Chaitali,25,Mumbai,6500,103,2008-05-20 00:00:00,4,2060)
	
	5.full outer
		r = join c by id full outer , o by customer_id ;
		同left outer。
	
	6.使用多个字段连接
		r = join c by (id,age) right outer , o by (customer_id,age) ;
		略
	7.cross，交叉连接(笛卡尔积)
		r = cross c , o ;
		(7,Muffy,24,Indore,10000,103,2008-05-20 00:00:00,4,2060)
		(7,Muffy,24,Indore,10000,101,2009-11-20 00:00:00,2,1560)
		(7,Muffy,24,Indore,10000,100,2009-10-08 00:00:00,3,1500)
		(7,Muffy,24,Indore,10000,102,2009-10-08 00:00:00,3,3000)
		(6,Komal,22,MP,4500,103,2008-05-20 00:00:00,4,2060)
		(6,Komal,22,MP,4500,101,2009-11-20 00:00:00,2,1560)
		...

Combining and Splitting
-------------------------

	1.union
		r = union c , o ;		//联合,字段个数可不同
		...
	2.split						//分割一个关系成多个关系
		SPLIT s into s0 if age<22, s1 if (22<=age and age<25);
		dump s0 ;
		dump s1 ;

filter:按照条件筛选tuple
---------------------------

	1.filter by
		r = filter s by age == 22 ;
		(2,siddarth,Battacharya,22,9848022338,Kolkata)
		(3,Rajesh,Khanna,22,9848022339,Delhi)
	
	2.distinct		//去重
		r = distinct s ;


foreach,生成特定数据
-------------------------

	0.语法
		r = foreach s generate (id,name,age);		//类似于select id,name from ...
	1.select...
		r = foreach s generate id,name,age;			//(...)(...)(...)
		r = foreach s generate (id,name,age);		//((...))((...))((...))

order
------------------------

	1. age desc == select ... order by age desc ;
		r = order s by age desc ;
		(8,Bharathi,Nambiayar,24,9848022333,Chennai)
		(7,Komal,Nayak,24,9848022334,trivendram)
		(6,Archana,Mishra,23,9848022335,Chennai)
		(5,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar)
		(3,Rajesh,Khanna,22,9848022339,Delhi)
		(2,siddarth,Battacharya,22,9848022338,Kolkata)
		(4,Preethi,Agarwal,21,9848022330,Pune)
		(1,Rajiv,Reddy,21,9848022337,Hyderabad)

limit 
---------------------

	r = limit s 4 ;


内置函数
------------------------

	1.AVG MIN ..需要大写。
		需要对bag进行计算。
		a.对s进行全分组
			g = group s all ;
			r = foreach g generate s.id,s.fname,AVG(s.age);		//对元组集合进行循环
			dump r ;
			({(8),(7),(6),(5),(4),(3),(2),(1)},
			 {(Bharathi),(Komal),(Archana),(Trupthi),(Preethi),(Rajesh),(siddarth),(Rajiv)},22.5)
		b.diff
			比较两个关系的不同
			r = cogroup s by age , e by age ;		//
			(21,{(4,Preethi,Agarwal,21,9848022330,Pune),(1,Rajiv,Reddy,21,9848022337,Hyderabad)},{})
	
			(22,{(3,Rajesh,Khanna,22,9848022339,Delhi),(2,siddarth,Battacharya,22,9848022338,Kolkata)},{(6,Maggy,22,Chennai),(1,Robin,22,newyork)})
			
			(23,{(6,Archana,Mishra,23,9848022335,Chennai),(5,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar)},{(5,David,23,Bhuwaneshwar),(3,Maya,23,Tokyo),(2,BOB,23,Kolkata)})
			
			(24,{(8,Bharathi,Nambiayar,24,9848022333,Chennai),(7,Komal,Nayak,24,9848022334,trivendram)},{})
			
			(25,{},{(4,Sara,25,London)})
	
			r0 = foreach r generate diff(s,e);		//
			({(4,Preethi,Agarwal,21,9848022330,Pune),(1,Rajiv,Reddy,21,9848022337,Hyderabad)})
			({(3,Rajesh,Khanna,22,9848022339,Delhi),(2,siddarth,Battacharya,22,9848022338,Kolkata),(6,Maggy,22,Chennai),(1,Robin,22,newyork)})
			({(6,Archana,Mishra,23,9848022335,Chennai),(5,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar),(3,Maya,23,Tokyo),(2,BOB,23,Kolkata),(5,David,23,Bhuwaneshwar)})
			({(8,Bharathi,Nambiayar,24,9848022333,Chennai),(7,Komal,Nayak,24,9848022334,trivendram)})
			({(4,Sara,25,London)})
	
		c.subtract
			包的减法.
			1,Robin,22,25000,sales
			2,BOB,23,30000,sales
			3,Maya,23,25000,sales
			4,Sara,25,40000,sales
			5,David,23,45000,sales
			6,Maggy,22,35000,sales
	
			1,Robin,22,25000,sales
			2,Jaya,23,20000,admin
			3,Maya,23,25000,sales
			4,Alia,25,50000,admin
			5,David,23,45000,sales
			6,Omar,30,30000,admin


load和store函数
------------------

	1.PigStorage
		加载存储结构化数据。
	2.TextLoad
		加载非结构化数据。
	3.BinStorage
		使用机器可读格式进行加载和保存
	4.处理压缩
		压缩数据。


bag | tuple函数
------------------

	1.TOBAG
		e = LOAD '/home/ubuntu/pig/e.data' USING PigStorage(',')as (id:int, name:chararray, age:int, city:chararray);
		r = foreach e generate TOBAG(id,name,age);			//将每个字段转换成tuple，并放置到一个bag中。
	2.TOP
		r0 = FOREACH r {top = TOP(2, 0, e);GENERATE top;}	//
	3.ToDate(date,'yyyy/MM/dd HH:mm:ss')
		
	4.ToString(date_time,Text);
		r = foreach e generate ToString(CurrentTime(),'yyyy/MM/dd HH:mm:ss');


InputFormat/OutputFormat
-------------------------

	MySQL
