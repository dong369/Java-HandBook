# 1 基础知识

1.1 Hadoop
--------------------

	存储 + 计算.
	
	分布式计算框架。
	common
	hdfs			// 分布式存储，block linux
	mapreduce		// mapreduce，编程模型 map + reduce
	yarn			// 资源调度框架

1.2 MySQL
--------------------
	SQL : structure query language.
	select 
	insert
	update
	delete
	ddl : create drop alter

1.3 RDBMS
--------------------
	Relation Database Management System,关系型数据库管理系统。
	OLTP:	online transaction process.
	a.原子性
	c.一致性
	i.隔离性
	d.永久性
	
	低延迟。
	
	事务并发现象
	--------------
		1.脏读			//读未提交
		2.不可重复读	//读不回去
		3.幻读			//读多了
	
	事务的隔离级别
	--------------
		1.读未提交
		2.读已提交
		4.可以重复读
		8.串行化

2 Hive
====================
	数据仓库软件。
	存储，重在分析，延迟高。
	使用SQL来读、写、管理驻留在分布式存储系统大型数据集。可以使用命令行和Driver连接到hive。
	运行在hadoop之上，用来汇集查询数据。
	OLAP:online analyze process.
	不是：
		关系数据库
		OLTP
		不适合实时查询和低层更新操作。
	特点
		在数据库中存放schema，处理数据到HDFS
		OLAP
		提供类SQL语言，HQL(HiveQL)
		可扩展、可伸缩、速度快。


2.1 架构信息
-----------------
	UI(web I / CLI/)
	
	MetaStore +  HQL PrcessEngine
				 Execution Engine
					MapReduce
	
	HDFS/Hbase...


2.2 组件构成
----------------
	UI					//
	MetaStore			//schema、table、clumn信息在RDBMS
	HQL Prcess Engein	//编写SQL代替MR程序
	Excution Engine		//处理查询，生成结果.
	HDFS				//

2.3 安装配置
-----------------
	1.下载并apache-hive-2.1.10.bin.tar.gzn开
	2.安装jdk
		参考之前
	3.安装hadoop
		参考之前
	4.安装hive
		$>cd ~/Desktop
		$>tar -xzvf apache-xxx.x.gz 
		$>mv apache-hive-xx /soft
		$>ln -s /soft/apache-hive-xxx /soft/hive
		$>sudo nano /etc/environment
			JAVA_HOME=/soft/jdk
			HADOOP_HOME=/soft/hadoop
			M2_HOME=/soft/maven
			HIVE_HOME=/soft/hive
			PATH="...:/soft/hive/bin"
		$>source /etc/environment
	5.查看hive版本
		$>hive --version

2.4 配置文件
----------------

	cp hive-default.xml.template hive-site.xml
	
	[${hive_home}/conf/hive-env.sh]
	...
	HADOOP_HOME=/soft/hadoop

2.5 配置元数据库
-------------------

内置Derby数据库

	1.hive使用rdbms存储元数据，内置了derby数据库。
		hive/conf/hive-default.xml.template	//hive的默认配置，不要修改
	2.创建hive-site.xml
		$>cp hive-default.xml.template hive-site.xml
	3.修改配置hive-site.xml
		替换${system:java.io.tmp.dir}=/home/ubuntu/hive
		替换${system:user.nane}=ubunt0
	4.替换jar包guava-27.0-jre.jar
	om.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
	5.删除特使字符
	java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansi
	&#

外部MySQL数据库

cd /opt/soft/hive/apache-hive-3.2.1-bin/lib/
rz
mysql-connector-java-8.0.20.jar

```properties
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<!-- MySQL数据库基础连接配置 -->
	<property>
		<name>javax.jdo.option.ConnectionURL</name>
		<value>jdbc:mysql://211.144.5.80:30112/hive?serverTimezone=GMT</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionDriverName</name>
		<value>com.mysql.jdbc.Driver</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionUserName</name>
		<value>root</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionPassword</name>
		<value>passw0rd</value>
	</property>
	<!-- 这是metastore -->
	<property>
		<name>hive.metastore.schema.verification</name>
		<value>false</value>
	</property>
	<property>
		<name>hive.cli.print.current.db</name>
		<value>true</value>
	</property>
	<property>
		<name>hive.cli.print.header</name>
		<value>true</value>
	</property>
	<!-- 这是hiveserver2 -->
	<property>
		<name>hive.server2.thrift.port</name>
		<value>10000</value>
	</property>
	<property>
		<name>hive.server2.thrift.bind.host</name>
		<value>211.144.5.80</value>
	</property>
</configuration>
```



2.6 初始化使用
------------------------

	0.启动hadoop
		$>start-all.sh
	1.初始化schema库
		$>schematool -initSchema -dbType derby  // Derby数据库
		$>schematool -dbType mysql -initSchema  // MySQL数据库
	2.完程后，在当前目录下创建一个文件夹metastore_db(元数据库)，对应的MySQL中会创建74张表
	3.进入hive shell
		$>hive

2.7 常见命令
--------------------------
	$hive>show databases ;			//显示库
	$hive>show tables ;				//显示表
	
	//创建表
	$hive>CREATE TABLE IF NOT EXISTS myhive.employee ( eid int, name String,salary String, destination String) COMMENT 'Employee details' ROW			FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n' STORED AS TEXTFILE;
	
	//加载数据到hive(hdfs)
	$hive>LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
	
	//插入数据 ==== 上传
	$hive>LOAD DATA LOCAL INPATH '/home/ubuntu/Desktop/employees.txt' INTO TABLE employee
	
	准备employee数据
	1201	Gopal	45000	Technical manager
	1202	Manisha	45000	Proof reader
	1203	Masthanvali	40000	Technical writer
	1204	Krian	40000	Hr Admin
	1205	Kranthi	30000	Op Admin
	
	//支持插入, 不支持删除和更新。
## 2.8 外部连接

```properties
nohup hive --service metastore &
nohup hive --service hiveserver2 &
netstat -nltpu |grep 10000
beeline
beeline> !connect jdbc:hive2://192.168.134.154:10000
Connecting to jdbc:hive2://192.168.134.154:10000
Enter username for jdbc:hive2://192.168.134.154:10000: root # 用户名root
Enter password for jdbc:hive2://192.168.134.154:10000: **** # 密码root
```

## 2.9 日志管理

```properties
cd /opt/soft/hive/apache-hive-3.1.1-bin/lib/
rm -rf slf
```

## 2.10 常见错误



# 3 入坑

## 3.1 MySQL连接阻塞

is blocked because of many connection errors ;unblock with 'mysqladmin flush-hosts' 

由于许多连接错误而被阻塞；使用“mysqladmin刷新主机”解除阻塞。

我的解决办法是进去mysql

mysql -uroot -pxxxx

刷新hosts

flush hosts;

```properties
show variables like '%max_connection_errors%';
set global max_connect_errors = 1000;
flush hosts;
```

# 4 趋势

大数据上半场战斗集中在底层框架，目前已经接近尾声，未来的底层大数据生态圈中将不再有那么多的新的技术和框架，每个细分领域都将优胜劣汰，走向成熟，更加集中化。下半场战斗的重点讲从底层走向上层，走向生态。之前的大数据创新更偏向于 IAAS 和 PAAS ，未来你将看到更多 SAAS 类型的大数据产品和创新。

IaaS：基础设施服务，Infrastructure-as-a-service

PaaS：平台服务中间件，Platform-as-a-service

SaaS：软件服务，Software-as-a-service