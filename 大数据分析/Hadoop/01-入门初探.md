# 1 基础环境

## 1.1 下载版本

[Hadoop下载地址](http://hadoop.apache.org/)

| 软件      | 版本      | 描述                                                         |
| --------- | --------- | ------------------------------------------------------------ |
| JDK       | 1.8.0_201 | Java开发工具包。                                             |
| MySQL     | 8.0.20    | 关系型数据存储。                                             |
| Hadoop    | 3.2.1     | 开发用于可靠、可伸缩的分布式计算的开源软件。                 |
| Zookeeper | 3.6.1     | 分布式应用程序的高性能协调服务。                             |
| Hive      | 3.1.2     | 提供数据汇总和特别查询的数据仓库基础结构。                   |
| Hbase     | 2.2.4     | 一个可伸缩的分布式数据库，支持大表的结构化数据存储，数据库技术NoSQL。 |
| Sqoop     | 1.4.7     | 一种用于在Apache Hadoop和结构化数据存储(如关系数据库)之间高效传输大容量数据的工具。 |
| Kafka     | 2.5.0     | 构建实时数据管道和流媒体应用程序。它是水平可伸缩的，容错能力强，速度快，可以在数千家公司的产品中运行。 |
| Spark     | 2.4.5     | 计算框架                                                     |
| Fink      | 1.10.1    | 计算框架（主流的框架）                                       |

1.2 模式切换
----------------------------

客户机桌面模式和文本模式切换

	1.ctrl + alt + f6		// 文本模式
	2.ctrl + alt + f7		// 桌面模式


1.3 开机进入文本模式
---------------------
	1.修改/etc/default/grub文件
		[/etc/default/grub]
		...
		#图形模式
		#GRUB_CMDLINE_LINUX_DEFAULT="quiet"
		#文本模式
		GRUB_CMDLINE_LINUX_DEFAULT="text"
		...
		#修改文本模式的分辨率
		#GRUB_GFXMODE=640x480
		...
	2.运行update-grub更新启动项
		$>sudo update-grub
	
	3.手动启动图形模式
		$>startx


1.4 客户机环境处理
----------------
	ip:10 - 14
	主机名:s10 - s14

1.5 修改主机名
--------------
	[/etc/hostname]
	s10

1.6 修改dns解析
-------------------

DNS是域名解析服务（hosts文件也是一个域名解析）。

	[/etc/hosts]
	127.0.0.1 localhost
	192.168.231.100 s100
	192.168.231.101 s101
	192.168.231.102 s102
	192.168.231.103 s103
	192.168.231.104 s104

1.7 修改ip地址
-----------------------
	[/etc/network/interfaces]
	# This file describes the network interfaces available on your system
	# and how to activate them. For more information, see interfaces(5).
	
	# The loopback network interface
	auto lo
	iface lo inet loopback
	
	#iface eth0 inet static
	iface eth0 inet static
	address 192.168.231.100
	netmask 255.255.255.0
	gateway 192.168.231.2
	dns-nameservers 192.168.231.2
	auto eth0

1.8 修改winhosts文件
--------------------
	[C:\Windows\System32\drivers\etc\hosts]
	127.0.0.1 localhost
	192.168.238.128 s100
	192.168.238.129 s200
	192.168.238.130 s300
	192.168.238.131 s400
	192.168.238.132 s500
	192.168.238.133 s600
	192.168.238.134 s700
	192.168.238.135 s800

# 2 认识大数据

2.1 BigData
-----------------

	1、分布式
		由分布在不同主机上的[进程]协同在一起，才能构成整个应用。
	
	2、海量数据
		1byte = 8bit
		1024B = 1K
		1024K = 1MB   2^10
		1024M = 1GB   2^10
		1024G = 1TB   2^10
		1024T = 1PB   2^10
		1024P = 1EB   2^10
		1024E = 1ZB   2^10
		1024Z = 1YB   2^10
		1024Y = 1NB   2^10
		
	3、存储
		分布式存储.
	
	4、计算
		分布式计算
	
	5、hadoop(一头大象)
		创始人：doug cutting（Lucene）
	
	6、Apache开源社区

2.2 Google公司
-----------------
	1、通过谷歌的两篇论文开发了Hadoop
	2、很牛逼的公司

2.3 Hadoop
-----------------
	1、介绍
		可靠、可伸缩、分布式计算的开源软件。
		Hadoop是分布式计算大规模数据集框架，使用简单编程模型，可从单个服务器扩展到几千台主机，每台
		机器都提供了本地计算和存储，不需要使用[硬件]来获取高可用性，类库在应用层处理检测并处理故障，
		因此在集群之上获得HA服务。
	
	2、HDFS
		hadoop distributed file system. 借鉴谷歌的GFS论文
	
	3、去IOE
		IBM（小型机） + Oracle + EMC（网络存储设备）
	
	4、MapReduce
		MR			// 映射和化简.编程模型.借鉴谷歌的MapReduce论文
	
	5、推荐系统（应用场景）

2.4 4V特征
------------------
	1、volumn	: 体量大
	2、velocity	: 速度快
	3、variaty	: 样式多
	4、Valueless		: 价值密度低，数据总量会越大（但是没有直接的联系）


2.5 hadoop的安装
------------------
	1、创建/soft目录，并更改用户和组
		$>sudo mkdir /soft
		$>sudo chown ubuntu:ubuntu /soft
	
	2、安装jdk
		a.复制jdk-8u65-linux-x64.tar.gz到 ~/Downloads
			$>cp /mnt/hgfs/downloads/bigdata/jdk-8u65-linux-x64.tar.gz ~/Downloads
		b.tar jdk-8u65-linux-x64.tar.gz
			$>cd ~/Downloads
			$>ta tar -xzvf jdk-8u65-linux-x64.tar.gz
	
		c.移动到jdk1.8.0_65到/soft下
			$>mv ~/Downloads/jdk1.8.0_65 /soft
			$>ln -s /soft/jdk-xxx jdk			//创建符号连接
	
		d.配置环境变量
			[/etc/environment]
			JAVA_HOME=/soft/jdk
			PATH="...:/soft/jdk/bin"
	
		e.让环境变量生效
			$>source /etc/environment
	
		f.检验安装是否成功
			$>java -version
	
	3、安装hadoop	
		a.复制并tar开hadoop.tar.gz
			$>cp /mnt/hgfs/downloads/bigdata/hadoop-2.7.2.tar.gz ~/Downloads/
			$>cd ~/Downloads
			$>tar -xzvf hadoop-2.7.2.tar.gz
			$>mv ~/Downloads/hadoop-2.7.2 /soft			//移动到/soft下
			$>cd /soft
			$>ln -s hadoop-2.7.2 hadoop					//创建hadoop符号连接
	
		b.配置环境变量.
			$>sudo nano /etc/environment
			[/etc/environment]
			JAVA_HOME=/soft/jdk
			HADOOP_HOME=/soft/hadoop
			PATH="...:/soft/jdk/bin:/soft/hadoop/bin:/soft/hadoop/sbin"
		
		c.重启系统
			$>sudo reboot
		
		d.验证hadoop安装是否成功
			$>hadoop version

2.6 分布式
--------------
	由分布在不同主机上的程序(进程)协同在一起才能构成整个应用。
	browser/web server：瘦客户端程序。

2.7 四个模块
------------------------	
	1.Hadoop Common:
		支持其他模块的工具模块
	2.Hadoop Distributed File System (HDFS™): 
		分布式文件系统，提供了对应用程序数据的高吞吐量访问。
		[进程]
		NameNode				// 名称节点		--NN
		DataNode				// 数据节点		--DN
		SecondaryNamenode		// 辅助名称节点	--2ndNN
	
	3.Hadoop YARN: 
		yet another resource negotiate,
		作业调度与集群资源管理的框架。
		[进程]
		ResourceManager			// 资源管理器--RM
		NodeManager				// 节点管理器--NM
	
	4.Hadoop MapReduce:
		基于yarn系统的对大数据集进行并行处理技术。

## 2.8 HA/HR

可用性(availability)是系统能够正常运行的时间比例。经常用两次故障之间的时间长度或在出现故障时系统能够恢复正常的速度来表示。

可靠性(reliability)是软件系统在应用或系统错误面前，在意外或错误使用的情况下维持软件系统的功能特性的基本能力。

2.9 可伸缩/可扩展
----------------

​	

## 2.10 性能/扩展性



## 2.11 延迟/吞吐量



## 2.12 CAP/BASE



3 伪分布式
================

## 3.1 配置

	1.Standalone/local
		独立/本地模式，使用的本地文件系统，不做任何配置默认情况下。
		nothing!!!
		查看文件系统的方式:
		$>hadoop fs -ls
	
		没有启动任何java进程。
		用于测试和开发环境.
	
	2.Pseudodistributed mode
		伪分布模式
		[配置过程]
		a.core-site.xml
			<?xml version="1.0" ?>
			<configuration>
				<property>
					<name>fs.defaultFS</name>
					<value>hdfs://localhost/</value>
				</property>
			</configuration>
		b.hdfs-site.xml
			<?xml version="1.0"?>
			<configuration>
				<property>
					<name>dfs.replication</name>
					<value>1</value>
				</property>
			</configuration>
		c.mapred-site.xml
			<?xml version="1.0"?>
			<configuration>
				<property>
					<name>mapreduce.framework.name</name>
					<value>yarn</value>
				</property>
			</configuration>
		d.yarn-site.xml
			<?xml version="1.0"?>
			<configuration>
				<property>
					<name>yarn.resourcemanager.hostname</name>
					<value>localhost</value>
				</property>
				<property>
					<name>yarn.nodemanager.aux-services</name>
					<value>mapreduce_shuffle</value>
				</property>
			</configuration>
	
		e.配置SSH（A：私钥加密公钥解密；B：公钥加密，私钥解密）
			安全登录.
			1)安装ssh
				$>sudo apt-get install ssh
			2)生成秘钥对
				$>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
				$>cd ~/.ssh								//查看生成的公私秘钥
			3)导入公钥数据到授权库中
				$>cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
			4)登录到localhost
				$>ssh localhost
				$>....yes
				$>exit
				$>ssh localhost				//不需要密码
	
			5)格式化hdfs文件系统
				$>hadoop namenode -format
	
			6)启动所有进程
				$>start-all.sh
			
			7)查看进程
				$>jps				//5 RM NM NN DN 2NN
			
			8)查看文件系统
				$>hadoop fs -ls /
			
			9)创建文件系统
				$>hadoop fs -mkdir -p /user/ubuntu/data
				$>hadoop fs -ls -R /					//-lsr


	3.Fully distributed mode
		完全分布式

## 3.2 NC传递文件

使用nc在两个client之间传递文件

	0.描述 
		100向101传递文件.
	1.在101机器
		$>nc -l 8888 > ~/.ssh/id_rsa.pub.100
	2.在100机器
		$>nc 192.168.231.101 8888 < ~/.ssh/id_rsa.pub
	3.在101上添加公钥文件
		$>cat ~/.ssh/id_rsa.pub.100 >> ~/.ssh/authorized_keys


3.3 进程处理
--------------------
	1.查看hadoop进程个数(5)
		$>jps
			NN
			DN
			2NN
			RM
			NM
	2.如果进程数不对，杀死所有进程
		$>stop-all.sh
	3.重新格式化系统
		$>hadoop namenode -format
	4.启动所有进程
		$>start-all.sh
	5.jps
按理说ssh localhost是非常简单的一个操作。

# 4 SSH配置

4.1 安装SSH
---------------

	1.禁用wifi
	2.关闭防火墙
	3.client能够访问外网
		$>ping www.baidu.com
	4.修改ubuntu的软件源 
		[/etc/apt/sources.list]
		...
		163
	
		[aliyun 源]
		deb http://mirrors.aliyun.com/ubuntu/ precise main restricted universe multiverse
		deb http://mirrors.aliyun.com/ubuntu/ precise-security main restricted universe multiverse
		deb http://mirrors.aliyun.com/ubuntu/ precise-updates main restricted universe multiverse
		deb http://mirrors.aliyun.com/ubuntu/ precise-proposed main restricted universe multiverse
		deb http://mirrors.aliyun.com/ubuntu/ precise-backports main restricted universe multiverse
		deb-src http://mirrors.aliyun.com/ubuntu/ precise main restricted universe multiverse
		deb-src http://mirrors.aliyun.com/ubuntu/ precise-security main restricted universe multiverse
		deb-src http://mirrors.aliyun.com/ubuntu/ precise-updates main restricted universe multiverse
		deb-src http://mirrors.aliyun.com/ubuntu/ precise-proposed main restricted universe multiverse
		deb-src http://mirrors.aliyun.com/ubuntu/ precise-backports main restricted universe multiverse
	
	5.安装ssh
		$>sudo apt-get install ssh
	
	6.查看进程,是否启动了sshd服务
		$>ps -Af | grep ssh
	
	7.生成秘钥对
		$>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
	
	8.导入公钥到授权keys文件
		$>cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
	
	9.登录localhost
		$>ssh localhost
		$>输入yes
	10.退出之后，再登录
		$>ssh localhost

## 4.2 生成秘钥

使用下面的命令即可生成秘钥

```
ssh-keygen -t rsa
```

## 4.3 免密码登录

使用下面的命令即可免密码登录localhost

```
ssh-copy-id localhost
```

但是到了这里执行ssh localhost的时候依然提示我输入密码。我仔细分析了原因，一般来讲失败的原因有两个。

### 4.3.1 失败原因之一

.ssh及其下属子文件的权限问题，首选.ssh目录权限是700， 两个dsa 和 rsa的 私钥权限是600，其余文件权限是644。下面列出.ssh目录及子文件的权限表：

```
drwx------ 2 root root 4096  1月 28 23:03 ./
drwxr-xr-x 9 root root 4096  1月 28 23:03 ../
-rw------- 1 root root  394  1月 28 23:03 authorized_keys
-rw------- 1 root root 1679  1月 28 23:03 id_rsa
-rw-r--r-- 1 root root  394  1月 28 23:03 id_rsa.pub
-rw-r--r-- 1 root root 1776  1月 28 23:03 known_hosts
```

### 4.3.2 失败原因之二

.ssh父目录的权限问题，我的.ssh文件夹绝对路径是`/root/.ssh`，所以`/root`目录的权限应该是755。这次我的问题就出现在这里。

# 5 完全分布式

5.1 模块构成
-----------
	1.common
	2.hdfs	
		hadoop distributed file system,分布式文件系统
		NameNode				// 名称节点		NN（最重要的，只有目录）
		DataNode				// 数据节点		DN（数据）
		SecondaryNamenode			// 辅助名称节点	        2ndNN
	3.yarn
		资源调度框架。
		ResourceManager				// 资源管理器 RM
		NodeManager				// 节点管理器 NM
	4.mapred
		 编程模型,map + reduce

5.2 配置Hadoop
---------------
	1.standalone/local(默认)
		本地模式.
		所有的程序运行在一个JVM中。不需要启动hadoop的进程。应用的文件系统就是本地文件系统。
	
	2.pseudo
		伪分布式，完全类似于完全分布式,但是只有一个节点。
		// hdfs默认端口8020
		[配置文件xml文件格式]
		[core-site.xml]
		fs.defaultFS=hdfs://localhost/
	
		// 配置副本数
		[hdfs-site.xml]
		dfs.replication=1
	
		// yarn资源调度框架
		[mapred-site.xml]
		mapreduce.framework.name=yarn
	
		// 
		[yarn-site.xml]
		yarn.resourcemanager.hostname=localhost
		yarn.nodemanager.aux-services=shuffle


	3.full distributed
		完全分布式.


5.3 SSH
--------------
	配置安全登录.
	1.安装ssh
		$>sudo apt-get install ssh
	2.查看进程
		$>ps -Af | grep ssh
		$>....sshd				//守护进程
	3.生成公司秘钥
		$>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
	
	4.生成公钥和私钥
		$>~/.ssh/id_rsa					// 私钥
		$>~/.ssh/id_rsa.pub				// 公钥
	
	5.添加公钥到授权key文件
		$>cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
	
	6.登录localhost
		$>ssh localhost
		$>需要确认 yes
	
	7.退出
		$>exit
	
	8.重新登录
		$>ssh localhost		//不需要yes，直接登录成功。

5.4 UDP
-------------
	user datagram protocal,用户（用户数据报协议）.
	0.0.0.0
	
	ServeSocket ss = new ServerSocket("0.0.0.0",8888);
	
	Socket s = new Socket("192.168.13.12"",8888);

5.5 首次启动
-------------------
	1.格式化文件系统
		$>hadoop namenode -format
	2.启动所有进程
		$>start-all.sh
	3.查询进程
		$>jps
	4.停止所有进程
		$stop-all.sh
	
	5.文件操作
		$hadoop fs -ls (-R) /
		$hadoop fs -mkdir -p /user/guodd/guodd	
		$hadoop fs -put hello.txt /usr/
		$hadoop fs -put hello.txt /usr/guodd/data

5.6 Web UI
----------------------------

访问hadoop hdfs

```properties
1.hdfs webui（名称节点）
	http://localhost:50070/
	http://localhost:9870/（V3.2.1）
2.data node（数据节点）
	http://localhost:50075/
	http://localhost:9864/（V3.2.1）
3.2nn（辅助节点）
	http://localhost:50090/
	http://localhost:9868/（V3.2.1）
4.Yarn ResourceManage（资源管理器）
	http://localhost:8088/
	http://localhost:8088/（V3.2.1）
	http://localhost:19888/（V3.2.1）
```

文件夹在名称节点还是数据节点
folder / path
/user/ubuntu/data/...是一个路径


5.7 完全分布式
-----------------
	1.准备5台客户机
	2.安装jdk
		略
	3.配置环境变量
		export JAVA_HOME=/opt/sotf/java/jdk
		export PATH=$PATH:$JAVA_HOME/bin
	4.安装hadoop
		下载安装包进行解压
	5.配置环境变量
		exprot HADOOP_HOME=/opt/soft/bigdata/hadoop
		export PATH=$PAHT:$HADOOP_HOME/bin
	6.安装ssh
	7.配置文件（名称节点和资源管理器节点一般在一个服务上）
		[/soft/hadoop/etc/hadoop/core-site.xml]
		fs.defaultFS=hdfs://s100/
	
		[/soft/hadoop/etc/hadoop/hdfs-site.xml]
		replication=3
	
		[/soft/hadoop/etc/hadoop/yarn-site.xml]
		yarn.resourcemanager.hostname=s100
	
		[/soft/hadoop/etc/hadoop/slaves]  // 注意高版本的配置文件给我了workers
		s10
		s11
		s12
	8.在集群上分发以上三个文件
		$>cd /soft/hadoop/etc/hadoop
		$>xsync core-site.xml 
		$>xsync yarn-site.xml 
		$>xsync slaves


5.8 Scp
---------------------
	安全远程文件复制程序,基于ssh。
	不支持符号连接.
	支持两个远程主机之间的复制.
	
	$>scp -r ~/xxx.txt ubuntu@s101:/home/ubuntu/		//递归复制

5.9 Rsync
---------------------
	远程同步工具，主要用于备份和镜像。
	支持链接、设备等等。
	速度快，避免复制相同内容的文件数据。
	不支持两个远程主机之间的复制。
	rsync -rvl ~/hello.txt root@s101:/home/ubuntu/

5.10 辅助名称节点配置
----------------------

	1.在hdfs-site.xml中增加如下属性.
		[hdfs-site.xml]
		<?xml version="1.0"?>
		<configuration>
			<property>
				<name>dfs.replication</name>
				<value>3</value>
			</property>
			<property>
				<name>dfs.namenode.secondary.http-address</name>
				<value>s104:50090</value>
			</property>
		</configuration>
	
	2.分发文件
		$>xsync hdfs-site.xml
	3.停止所有进程
		$>stop-all.sh
	
	4.格式化文件系统
		$>hadoop namenode -format
	
	5.启动所有进程
		$>start-all.sh

5.11 考查本地文件的内容
---------------------

	1.s100:/tmp/hadoop-ubuntu/dfs/name/current/VERSION
		#Thu Aug 25 00:56:49 PDT 2016
		namespaceID=393070549
		clusterID=CID-355cc87a-870f-415c-9bad-f0b549116d79
		cTime=0
		storageType=NAME_NODE
		blockpoolID=BP-2070789203-192.168.231.100-1472111809289
		layoutVersion=-63
	
	2.s101:/tmp/hadoop-ubuntu/dfs/data/current/VERSION
		#Thu Aug 25 00:57:18 PDT 2016
		storageID=DS-69589dc3-a9d3-44f4-a3b5-db3dfd49eb30
		clusterID=CID-355cc87a-870f-415c-9bad-f0b549116d79
		cTime=0
		datanodeUuid=b5c7c184-83e9-4b48-8019-6c9f6b6e8194
		storageType=DATA_NODE
		layoutVersion=-56
	
	3.s102:/tmp/hadoop-ubuntu/dfs/data/current/VERSION
		#Thu Aug 25 00:57:18 PDT 2016
		storageID=DS-fa3fb086-c94f-4ebd-adc7-1de26c859502
		clusterID=CID-355cc87a-870f-415c-9bad-f0b549116d79
		cTime=0
		datanodeUuid=5c16340e-9277-473e-bd30-74ace56f0fad
		storageType=DATA_NODE
	
		layoutVersion=-56
	
	4.s103
		略
	
	5.s104:/tmp/hadoop-ubuntu/dfs/secondary/current/VERSION
		#Thu Aug 25 01:58:16 PDT 2016
		namespaceID=393070549
		clusterID=CID-355cc87a-870f-415c-9bad-f0b549116d79
		cTime=0
		storageType=NAME_NODE
		blockpoolID=BP-2070789203-192.168.231.100-1472111809289
		layoutVersion=-63


5.12 修改本地的临时目录
-----------------------	

	1.修改hadoop.tmp.dir
		[core-site.xml]
		<property>
		  <name>hadoop.tmp.dir</name>
		  <value>/home/ubuntu/hadoop/</value>
		</property>
	
	2.分发core-site.xml
		$>xsync core-site.xml
	
	3.停止进程
		$>stop-all.sh
		
	4.格式化
		$>hadoop namenode -format
	5.启动所有进程
		$>start-all.sh
	6.重启系统 
		$>start-all.sh			//无需再格式化，是否ok.

# 6 自定义脚本

## 6.1 文件分发脚本

自定义脚本xsync,在集群上分发文件。

```shell
循环复制文件到所有节点的相同目录下。
rsync -rvl /home/ubuntu ubuntu@s101:
xsync hello.txt

[/usr/local/bin/xsync]
#!/bin/bash
pcount=$#
if (( pcount<1 )) ; then
	echo no args;
	exit ;
fi

p1=$1;
fname=`basename $p1`
#echo fname=$fname;

# 父路径，-P以物理路径进入
pdir=`cd -P $(dirname $p1) ; pwd`
#echo pdir=$pdir

cuser=`whoami`
for (( host=100 ; host<105 ; host=host+1 )) ; do 
  echo ------------ s$host ---------------
  rsync -rvl $pdir/$fname $cuser@s$host:$pdir
done
```

## 6.2 统一命令脚本

编写/usr/local/bin/xcall脚本,在所有主机上执行相同的命令

```shell
[/usr/local/bin/xcall]
#!/bin/bash
pcount=$#
if (( pcount<1 )) ; then
	echo no args;
	exit ;
fi

echo   -------- localhost --------
$@
for (( host=101 ; host<105 ; host=host+1 )) ; do 
  echo -------- s$host --------
  ssh s$host $@
done
```

# 7 类库&配置文件

## 7.1 类库

整理hadoop的所有类库和配置文件。解压缩hadoop-2.7.2.tar.gz到目录下，整理jar包，抽取所有配置文件。

## 7.2 配置文件

	[core-default.xml]
	hadoop-common-2.7.2.jar/core-default.xml
	
	[hdfs-default.xml]
	hadoop-hdfs-2.7.2.jar/hdfs-default.xml
	
	[yarn-default.xml]
	hadoop-yarn-2.7.2.jar/hdfs-default.xml
	
	[mapred-default.xml]
	hadoop-mapreduce-client-core-2.7.2.jar/mapred-default.xml

# 8 入坑

## 8.1 SLF4J

mv /opt/soft/bigdata/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar /opt/soft/bigdata/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar.bak

