# 1 基础复习

1.1 kafka
--------------

	流计算框架。
	集群。
	中间件技术。
	JMS:java message service.
	耦合低。
	queue : p2p
	subscribe : pub-sub
	消费者组.
	
	producer
	consumer//
	consumer group
	broker	//代理,kafka server
	topic	//副本 + 分区。offset(zk)
	
	zk		//zookeeper 协同 HA
	SPOF	//single point of failure,
	
	flume + kafka
	--------------
		1.source
			kafka : consumer
		2.sink
			producer
		3.channel

1.2 hive
-----------------
	数据仓库。
	分析。
	延迟高。
	OLAP
	MR:		//hive ql(HQL === SQL)
	UDF		//user define function.

1.3 DB
------------
	OLTP
	延迟低
	事务性处理。

2 安装配置
===============
	RDBMS和hdfs之间进行数据的export/import，工具。


2.1 V-1.4.7使用
---------------------
	1.安装
		 略
	2.配置
		a)环境变量
			export SQOOP_HOME=/opt/soft/bigdata/sqoop
			export PATH=$PATH:$SQOOP_HOME/bin
		b)sqoop-env.sh
			和hadoop协同
			[sqoop/conf/sqoop-env.sh]
			export HADOOP_COMMON_HOME=/usr/local/hadoop
			export HADOOP_MAPRED_HOME=/usr/local/hadoop
	3.配置mysql驱动程序
		$>cp mysql-xx-xx.jar /opt/soft/bigdata/sqoop/lib
		$>cp commons-lang-2.6.jar /opt/soft/bigdata/sqoop/lib
	4.验证sqoop安装
		$>sqoop/bin/sqoop-version
	5.查看sqoop所有帮助
		$>sqoop help
	6.查看指定命令帮助
		$>sqoop help import

2.2 sqoop import
----------------

配置mapred-site.xml

```properties
<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
```

com.mysql.jdbc.Driver

com.mysql.cj.jdbc.Driver

	1.import
	sqoop import \
		--connect jdbc:mysql://192.168.10.1:3306/test \
		--driver com.mysql.cj.jdbc.Driver --username root --password passw0rd \
		--table aa \
		--target-dir /user/guodd/table \
		--incremental append --check-column id;
	
	2.import-all-tables		
		sqoop import-all-tables --connect jdbc:mysql://192.168.231.1:3306/test \
					 --driver com.mysql.jdbc.Driver \
					 --username root \
					 --password root \
					 --target-dir	//没有该参数,默认使用表名作为目录.
	3.export
		注意:从HDFS文件系统中导出数据到MySQL
		sqoop			:是1.4.2-hadoop-xx
		mysql-connector	:5.1.17版本.
	
	sqoop export \
		--connect jdbc:mysql://192.168.10.1:3306/test \
		--username root --password passw0rd \
		--table bb --export-dir /user/guodd/table/part-m-00002 \
		--colum ns id,user_name,age;

# 3 导入到Hive

3.1 基础导入
------------------------

创建表结构

```properties
create table aa (
	id int,
	user_name varchar(50),
	age int,
	date data,
	time time,
);
```



	1.[import]
	sqoop import \
		--connect jdbc:mysql://192.168.10.1:3306/test \
		--driver com.mysql.cj.jdbc.Driver --username root --password passw0rd  \
		--table aa \
		-m 1 \
		--hive-import --hive-overwrite --hive-table aa;
		
		#可以手动指定存储位置--target-dir /user/hive/warehouse/test1 --hive-table dblab.test1
	
	2.[export]
	
	3.查看
	hdfs dfs -cat /user/hive/warehouse/aa/*

3.2 增量导入
-------------------

sqoop help import

	1.参数
		--check-column		//指定的检查字段,该字段不能使varchar。。类型
		--incremental		//增量类型,append | lastmodified
		--last-value		//指定之前导入时候的最大值。
		
	sqoop import \
		--connect jdbc:mysql://192.168.10.1:3306/test \
		--driver com.mysql.cj.jdbc.Driver --username root --password passw0rd  \
		--table aa \
		-m 1 \
		--hive-import --hive-overwrite --hive-table aa \
		--check-column id --incremental append --last-value 3;

## 3.3 Job作业

```properties
sqoop job --create myjob -- import \
	--connect jdbc:mysql://192.168.10.1:3306/test \
	--driver com.mysql.cj.jdbc.Driver --username root --password passw0rd  \
	--table bb \
	-m 1 \
	--hive-import --hive-overwrite --hive-table bb;
```



# 4 导入到Hbase

4.1 基础导入
-----------------

	不要使用它的创建表。
	[import]
	sqoop import --connect jdbc:mysql://192.168.231.1:3306/test 
		--driver com.mysql.cj.jdbc.Driver
		--username root --password root  
		--table customers --hbase-table t1 
		--column-family cf1 --hbase-row-key id --check-column id 
		--incremental append --last-value 3


2.6 Job作业
------------------
	1.在/opt/soft/bigdata/sqoop/lib文件下添加缺失的jar
		java-json.jar
	1.create创建作业
		 sqoop job --create myjob -- import 
		 --connect jdbc:mysql://192.168.231.1:3306/test 
		 --driver com.mysql.jdbc.Driver --username root --password root  
		 --table customers --hbase-table t1 
		 --column-family cf1 --hbase-row-key id
	2.查看job列表
		sqoop job --list
	3.查看指定job
		$>sqoop job --show myjob
	4.启动job
		$>sqoop job --exec myjob
	
	5.删除job
		$>sqooop job --delete myjob

5 入坑
=============================

## 5.1 版本文件

坑(hadoop - mysql)，根据自己的MySQL版本

	[一下两个软件的版本是ok的]
	sqoop-1.4.2.bin__hadoop-2.0.0-alpha(OK-mysq5.1.17-jar).tar.gz
	mysql-connector-java-5.1.17.jar
## 5.2 配置缺失

1.org.apache.hadoop.yarn.exceptions.InvalidAuxServiceException: The auxService:mapreduce_shuffle does not exist

问题分析：The auxService:mapreduce_shuffle does not exist；**yarn-site.xml**配置文件有问题

解决方法：

1）vim /usr/local/hadoop/etc/hadoop/**yarn-site.xml**

> **
> **
>
>   <name>**yarn.nodemanager.aux-services**</name>
>
>   <value>**mapreduce_shuffle**</value>
>
>  </property>
>
>  <property>
>
>   <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
>
>   <value>org.apache.hadoop.mapred.ShuffleHandler</value>
>
>  </property>

2）重新启动yarn：

/usr/local/hadoop/sbin/stop-yarn.sh 

/usr/local/hadoop/sbin/start-yarn.sh 

## 5.3 重复创建问题

2.ERROR tool.ImportTool: Import failed: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://192.168.15.115:9000/hive/root already exists

解决方法： hadoop fs -rm -r /hive/root

## 5.4 包冲突

3.ERROR exec.DDLTask: java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.readerFor(Ljava/lang/Class;)Lcom/fasterxml/jackson/databind/ObjectReader;

造成的原因：sqoop与hive使用的jackson包的版本冲突导致

解决方法：将sqoop的lib目录下jackson*.jar包备份，hive的lib目录下的jackson相关jar包拷贝到sqoop的lib目录下

## 5.5 环境变量缺失

ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.

解决方法：

1)/etc/profile最后加入 export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/lib/*

然后刷新配置，source /etc/profile

2)复制hive库文件中hive-exec*到sqoop的库文件中

## 5.6 JDK配置

ERROR Could not register mbeans java.security.AccessControlException: access denied ("javax.management.MBeanTrustPermission" "register")

解决方法：

vim /opt/soft/jdk/jdk1.8/jre/lib/security/java.policy

新增语句：

grant {

// JMX Java Management eXtensions

**permission javax.management.MBeanTrustPermission "register";**

};

## 5.7 创建job的时候报错：

ERROR sqoop.Sqoop: Got exception running Sqoop: java.lang.NullPointerException

Exception in thread "main" java.lang.NoClassDefFoundError: org/json/JSONObject

造成的原因：这是因为sqoop缺少java-json.jar包.

解决方法：

java-json.jar下载地址：http://www.java2s.com/Code/Jar/j/Downloadjavajsonjar.htm
将java-json.jar放到/usr/local/sqoop/lib