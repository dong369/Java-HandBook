org.apache.hadoop.hbase.filter.RegexStringComparator
where ((name like 't%') and (age > 20)) 
						or 
	  ((name like 't%') and (age <= 20))

FilterList()

自定义过滤器
-----------------
	1.定义过滤器类
		package com.it18zhang.myhbase123.filter;
	
		import java.io.IOException;
	
		import org.apache.hadoop.hbase.Cell;
		import org.apache.hadoop.hbase.filter.FilterBase;
		import org.apache.hadoop.hbase.util.Bytes;
	
		/**
		 * 自定义过滤器
		 */
		public class MyFilter extends FilterBase {
			private byte[] value = null ;
			private boolean rowFilter = true ;
	
			public MyFilter(byte[] value, boolean rowFilter) {
				super();
			}
			
			public MyFilter(byte[] value) {
				this.value = value ;
			}


​			
			public void reset() throws IOException {
				this.rowFilter = true ;
			}
			
			public boolean filterRow() throws IOException {
				return rowFilter;
			}
			
			public ReturnCode filterKeyValue(Cell v) throws IOException {
				int res = Bytes.compareTo(v.getValue(),value);
				if(res <= 0){
					return ReturnCode.INCLUDE ;
				}
				else{
					return ReturnCode.NEXT_ROW;
				}
			}
		}
		
	2.导出jar
		mvn package -DskipTests
	3.部署jar到hbase环境变量classpath
		cp ... hbase/lib
		xsync ...
	3'重启hbase集群
		$>stop-hbase.sh
		$>start-hbase.sh
	
	4.使用filter
		
	5.

使用counter
-----------------
	1.频繁对col值进行修改。
		
	2.创建表
		$hbase>create 'ns1:t3','cf1'
	3.直接通过incr命令增加列，并递增值
		$>incr 'ns1:t3','row1','cf1:no' , 1		//创建long型col 0,1,-1
	4.通过API进行col自增
		Table t = conn.getTable(TableName.valueOf("ns1:t3"));
		long c = t.incrementColumnValue(Bytes.toBytes("row1"), Bytes.toBytes("cf1"),Bytes.toBytes("no"), 20);
		System.out.println(c);

多计数器
-----------------
	Table t = conn.getTable(TableName.valueOf("ns1:t3"));
	Increment incr = new Increment(Bytes.toBytes("row1"));
	incr.addColumn(Bytes.toBytes("cf1"), Bytes.toBytes("no"), 1);
	incr.addColumn(Bytes.toBytes("cf1"), Bytes.toBytes("no1"), 2);
	incr.addColumn(Bytes.toBytes("cf1"), Bytes.toBytes("no2"), 3);
	incr.addColumn(Bytes.toBytes("cf1"), Bytes.toBytes("no3"), 4);
	t.increment(incr);


Coprocessor
-------------------
	协处理器.在rs上运行自己的代码(自定义功能),协处理器可以
	进行动态加载。
	类型:
		1.observer
		  观察者，类似于trigger，回调程序。特定事件发生时执行。
		  框架提供的抽象
		  RegionObserver			//表修改事件
		  MasterObserver			//DDL事件，集群级事件
		  WALObserver				//相应WAL事件。
		2.endpoint		终端
		  类似于RDBMS的存储过程。


​		
		3.Coprocessor
			//在相同优先级的处理器中，需要使用序号控制执行的先后顺序。
			public interface Coprocessor {
			  int VERSION = 1;
	
			  //优先级决定了处理器执行的先后顺序.
			  /** Highest installation priority,最高优先级 */
			  int PRIORITY_HIGHEST = 0;
			  /** High (system) installation priority */
			  int PRIORITY_SYSTEM = Integer.MAX_VALUE / 4;
			  /** Default installation priority for user coprocessors */
			  int PRIORITY_USER = Integer.MAX_VALUE / 2;
			  /** Lowest installation priority */
			  int PRIORITY_LOWEST = Integer.MAX_VALUE;
			  enum State {
				UNINSTALLED,		//未安装
				INSTALLED,			//已安装
				STARTING,			//正在启动
				ACTIVE,				//激活
				STOPPING,			//正在停止
				STOPPED				//已停止
			  }
			  void start(CoprocessorEnvironment env) throws IOException;
			  void stop(CoprocessorEnvironment env) throws IOException;
			}
		4.CoprocessorHost
			对协处理器提供了公共的运行时服务。

协处理器的加载过程
-----------------------
	1.配置文件加载
		[hbase-site.xml]
		<property>
			<name>hbase.coprocessor.region.classes</name>
			<value>coprocessor.RegionObserverExample, coprocessor.AnotherCoprocessor</value>
		</property>
		<property>
			<name>hbase.coprocessor.master.classes</name>
			<value>coprocessor.MasterObserverExample</value>
		</property>
		<property>
			<name>hbase.coprocessor.wal.classes</name>
			<value>coprocessor.WALObserverExample, bar.foo.MyWALObserver</value>
		</property>
	2.通过HTableDescriptor加载


自定义协处理器
----------------------
	1.创建类
		package com.it18zhang.myhbase123.filter;
	
		import java.io.FileOutputStream;
		import java.io.IOException;
	
		import org.apache.hadoop.hbase.CoprocessorEnvironment;
		import org.apache.hadoop.hbase.client.Durability;
		import org.apache.hadoop.hbase.client.Put;
		import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;
		import org.apache.hadoop.hbase.coprocessor.ObserverContext;
		import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
		import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
	
		/**
		 * 自定义协处理器
		 */
		public class MyRegionProcessor extends BaseRegionObserver {
			
			public MyRegionProcessor(){
				log("new MyRegionProcessor()");
			}
	
			public void start(CoprocessorEnvironment e) throws IOException {
				log("start : " + this.toString());
			}
	
			public void stop(CoprocessorEnvironment e) throws IOException {
				log("stop : " + this.toString());
			}
	
			public void prePut(ObserverContext<RegionCoprocessorEnvironment> e, Put put, WALEdit edit, Durability durability)
					throws IOException {
				log("prePut : " + put + " ," + edit + "," + durability);
			}
	
			public void postPut(ObserverContext<RegionCoprocessorEnvironment> e, Put put, WALEdit edit, Durability durability)
					throws IOException {
				log("postPut : " + put + " ," + edit + "," + durability);
			}
			
			private void log(String log){
				try {
					FileOutputStream fos = new FileOutputStream("/home/ubuntu/co.log",true);
					fos.write((log + "\r\n").getBytes());
					fos.close();
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
		}
	
	2.导出jar,分发到hbase
	3.配置hbase-site.xml
		<property>
			<name>hbase.coprocessor.region.classes</name>
			<value>coprocessor.RegionObserverExample, coprocessor.AnotherCoprocessor</value>
		</property>

JPA:规范
-------------
	Jva Persistence API
	Get Scan Put Delete


使用kundera以JPA方式操纵Hbase
------------------------------
	1.创建mvn项目
	2.引入依赖
		[pom.xml]
		<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
			xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
			<modelVersion>4.0.0</modelVersion>
	
			<groupId>com.it18zhang</groupId>
			<artifactId>mykundera281</artifactId>
			<version>0.0.1-SNAPSHOT</version>
			<packaging>jar</packaging>
	
			<name>mykundera281</name>
			<url>http://maven.apache.org</url>
	
			<properties>
				<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
			</properties>
	
			<dependencies>
				<dependency>
					<groupId>junit</groupId>
					<artifactId>junit</artifactId>
					<version>4.11</version>
					<scope>test</scope>
				</dependency>
				<!-- https://mvnrepository.com/artifact/com.impetus.client/kundera-hbase -->
				<!-- https://mvnrepository.com/artifact/com.impetus.kundera.client/kundera-hbase -->
				<dependency>
					<groupId>com.impetus.kundera.client</groupId>
					<artifactId>kundera-hbase</artifactId>
					<version>3.6</version>
				</dependency>
			<!--
				<dependency>
					<groupId>javax.persistence</groupId>
					<artifactId>persistence-api</artifactId>
					<version>1.0.2</version>
				</dependency>
				-->
			</dependencies>
		</project>


	3.创建src/main/java/META-INF/persistence.xml
		<?xml version="1.0"?>
		<persistence xmlns="http://java.sun.com/xml/ns/persistence"
			xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
			xsi:schemaLocation="http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd"
			version="2.0">
			<persistence-unit name="hbase_pu">
				<provider>com.impetus.kundera.KunderaPersistence</provider>
				<properties>
					<property name="kundera.nodes" value="s101" />
					<property name="kundera.port" value="2181" />
					<!-- ns1:t1 ==> table -->
					<property name="kundera.keyspace" value="ns1:t1" />
					<property name="kundera.dialect" value="hbase" />
					<property name="kundera.client.lookup.class" value="com.impetus.client.hbase.HBaseClientFactory" />
				</properties>
			</persistence-unit>
		</persistence>
	
	4.创建实体类
		package com.it18zhang.mykundera281;
	
		import javax.persistence.Column;
		import javax.persistence.Entity;
		import javax.persistence.Id;
		import javax.persistence.Table;
	
		/**
		 * 实体类
		 */
		@Entity
		//name ===> family
		@Table(name="cf1")
		public class T1 {
			//id ==> rowod
			@Id
			private String no ;
			
			@Column
			private String name ;
			
			@Column
			private int age ;
			
			public String getNo() {
				return no;
			}
			public void setNo(String no) {
				this.no = no;
			}
			public String getName() {
				return name;
			}
			public void setName(String name) {
				this.name = name;
			}
			public int getAge() {
				return age;
			}
			public void setAge(int age) {
				this.age = age;
			}
		}
	
	5.App
		public static void main(String[] args) {
			EntityManagerFactory emf = Persistence.createEntityManagerFactory("hbase_pu");
			EntityManager em = emf.createEntityManager();
			T1 t1 = new T1();
			t1.setNo("row1");
			t1.setName("tomas");
			t1.setAge(23);
			em.persist(t1);
		}
	
	6.单元测试
		package com.it18zhang.mykundera281;
	
		import java.util.List;
	
		import javax.persistence.EntityManager;
		import javax.persistence.EntityManagerFactory;
		import javax.persistence.Persistence;
		import javax.persistence.Query;
	
		import org.junit.Before;
		import org.junit.Test;
	
		/**
		 * Unit test for simple App.
		 */
		public class TestCRUD {
			private	EntityManagerFactory emf ;
			@Before
			public void iniEMF(){
				emf = Persistence.createEntityManagerFactory("hbase_pu");
			}
			
			/**
			 * 查询
			 */
			@Test
			public void get(){
				EntityManager em = emf.createEntityManager();
				T1 e = em.find(T1.class, "row1");
				System.out.println(e.getNo() + "," + e.getName());
			}
			
			/**
			 * 删除
			 */
			@Test
			public void delete(){
				EntityManager em = emf.createEntityManager();
				T1 t1 = new T1();
				t1.setNo("row1");
				em.remove(t1);
			}
			
			/**
			 * 删除
			 */
			@Test
			public void batchInsert(){
				EntityManager em = emf.createEntityManager();
				em.getTransaction().begin();
				for(int i = 1 ; i < 1000 ;  i++){
					T1 t1 = new T1();
					t1.setNo("row"+ i);
					t1.setName("tom" + i);
					t1.setAge(i % 100);
					em.persist(t1);
				}
				em.getTransaction().commit();
				System.out.println("over");
			}
			
			/**
			 * find
			 */
			@Test
			public void findAll(){
				EntityManager em = emf.createEntityManager();
				String jpaql = "select t from T1 t" ;
				Query q = em.createQuery(jpaql);
				List<T1> list = q.getResultList();
				for(T1 t : list){
					System.out.println(t.getName());
				}
			}
		}