# 1 基础复习

数据库访问技术，NoSql。随机访问，实时读写，延迟低，建立在Hadoop之上的。

1.1 hive
-----------------
	1.内部表
		数据生命周期。
	2.外部表
		删除外部表，比没有删除数据，删掉了schema(rdbms).
	3.分区表
		表目录的子目录。
		create table xx(...) partitioned by ()
		alter table xxx add partitions() ..
		load data local inpath ... into table xxx partition(,,,);
	4.bucket表
		create table xxx(...) ... clustered by (fieldName) into n buckets ;
		数据文件.hash


1.2 调优
------------
	1.explain
		解释执行计划
		explain select sum(*) from test2 ;
	2.启用limit调优,避免全表扫描，使用抽样机制。
		select * from xxx limit 1,2;
		hive.limit.optimize.enable=true
	3.JOIN
		使用map端连接(/*+ streamtable(table)*/)
		连接查询表的大小是从左至右一次增长.
	
	4.设置本地模式,在单台机器上处理所有任务。
		使用与小数据情况.
		$hive>set hive.exec.mode.local.auto=true	//默认false
		$hive>set mapreduce.framework.name=local	//默认false
	
	5.并行执行job
		如果job之间没有依赖关系，可以并发执行，缩短执行时间。
		$hive>set hive.exec.parallel=true			//默认false
	
	6.严格模式
		$hive>set hive.mapred.mode=strict				//不推荐,早期版本使用
		$hive>
		$hive>set hive.strict.checks.large.query=true	//默认false,该设置会禁用以下操作
														//1.不指定limit的orderby
														//2.对分区表不指定分区进行查询
														//3.和数据量无关，只是一个查询模式
		$hive>set hive.strict.checks.type.safety=true;	//严格类型的安全检查,不允许以下操作
														//1.bigint 和 string之间比较
														//2.bigint 和 double之间比较
	
		$hive>set hive.strict.checks.cartesian.product=true;		//不允许笛卡尔积连接。
	
	7.调整map,reduce个数
		$hive>set hive.exec.reducers.bytes.per.reducer=256000000	//每个reducer task的字节数
		$hive>set hive.exec.reducers.max=1009						//每个reducer task的最大值，属性为负值时，会
																	//使用该属性
	
	8.*****JVM重用*****
		是的同一个JVM在一个Job(map*, reduce *)中执行多次,避免启动jvm的开销.
		$hive>set mapreduce.job.ubertask.enable=true;					//是否启用uber,jvm重用
		$hive>set mapreduce.job.ubertask.maxmaps=9;						//uber,降低
		$hive>set mapreduce.job.ubertask.maxreduces=1;					//uber
	
		$hive>set hive.exec.reducers.bytes.per.reducer=100000000;		//
		$hive>set hive.exec.reducers.max=5;
		$hive>set mapreduce.job.reduces=3;
	
	9.*****索引*****
		使用index.
	
	10.*****动态分区调整***** + *****bucket表*****
		$hive>set hive.exec.dynamic.partition.mode=strict				//动态分区严格模式
		$hive>set hive.max.dynamic.partitions=300000					//设置最大分区数
		$hive>set hive.max.dynamic.partitions.pernode=1000				//设置每个节点最大分区数
	
	11.推测执行,让map|reduce多个实例并发执行
		$hive>set mapreduce.map.speculative=true						//map推测
		$hive>set mapreduce.reduce.speculative=true						//reduce推测
	
	12.多个分组优化
		若多个groupby操作使用的是一个公共的字段，则这些groupby可以生成一个mr.
		$hive>hive.multigroupby.singlereducer=true;						//默认true
	
	13.虚拟列
		$hive>set hive.exec.rowoffset=true;									//是否启用虚拟列
		select INPUT__FILE__NAME, BLOCK__OFFSET__INSIDE__FILE from test2;	//__

1.3 压缩
-------------------
	1.查看压缩的编解码器
		$hive>set io.compression.codecs
	2.启用中间结果压缩
		$hive>set hive.exec.compress.intermediate=true;		//
		$hive>set mapred.compress.map.output=				//控制map的输出压缩设置		
		
	3.修改map输出结构的压缩,默认值DefaultCodec
		<property>
			<name>mapred.map.output.compression.codec</name>
			<value>org.apache.hadoop.io.compress.SnappyCodec</value>
		</property>
	4.设置最终的job输出结果为压缩
		a.设置hive的查询结果是否压缩
			$>hive set hive.exec.compress.output=true;		//默认false，不压缩
		b.具体配置要依靠hadoop配置 
			[mapred-site.xml]
			<property>
				<name>mapred.output.compression.codec</name>
				<value>org.apache.hadoop.io.compress.GzipCodec</value>
			</property>
	5.使用sequencefile作为存储格式
		$hive>create table seqfile(...) stored as sequencefile;	//使用序列文件存储
		$hive>insert into t_seq select * from test1 ;			//复制test1数据到t_seq中。
	
	6.控制sequencefile中map端输出时文件的压缩类型,使用block压缩.
		<property>
			<name>mapred.output.compression.type</name>
			<value>BLOCK</value>
		</phive> 
		
		$hive>set mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;	//map输出codec
		$hive>set hive.exec.compress.intermediate=true;		//中间结果是否压缩
	
		$hive>set hive.exec.compress.output=true;			//job的输出是否压缩，阀门
		$hive>set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;			//job输出的codec
		$hive>set mapred.output.compression.type=BLOCK;		//job的输出block压缩
	
	7.对分区进行归档(只能对内部分区表进行归档，外部表不可以)
		a.启用hive的归档
			$hive>set hive.archive.enabled=true;			//是否允许归档操作
		b.对表的指定分区进行归档
			$hive>alter table test3 archive partition(province='hebei',city='baoding');
		
		c.操作
			$hive>create table test3 as select id,name,age from test2 ;
		
		注意:运行时缺少hadoopArchive.class类，查看日志/tmp/ubuntu/hive/hive.log可见。
		     复制${hadoop_home}/shared/hadoop/tools/hadoop-archive-xxx.jar hive/auxlib/
		     复制${hadoop_home}/shared/hadoop/tools/hadoop-archive-xxx.jar hive/lib/

1.4 函数
-------------------------------

UDF：user define function

	1.procedure + function
		
	2.函数操作
		$hive>show functions ;				//显示所有函数
		$hive>desc[ribe] function case ;	//查看函数的帮助
		$hive>desc[ribe] function |	;		//查看函数的帮助
		$hive>desc function extended |;		//查看函数的扩展帮助
	3.UDF
		输入时一行或多行，输出单行。
	4.UDAF:
		User Define aggregate function,用户自定义聚合函数
		一行或多行的n个列作为输入，输出一个值。
	5.UDTF
		User Define table function,表生成函数.
		n个输入,多行或者多列作为输入。
		select explode(array(1,2,3))  ;		//
	
	6.自定义函数
		a.创建类继承UDF
			package com.it18zhang.myhive210.func;
	
			import java.text.ParseException;
			import java.text.SimpleDateFormat;
			import java.util.Date;
	
			import org.apache.hadoop.hive.ql.exec.Description;
			import org.apache.hadoop.hive.ql.exec.UDF;
	
			@Description(name="to_date",
						value="this is my first udf!",
						extended="For example : select to_date('2016/09/17 12:12:12') ;")
				
			/**
			 * 将字符串转换成日期
			 */
			public class ToDate extends UDF {
				
				/**
				 * 计算
				 */
				public Date evaluate(String date){
					try {
						SimpleDateFormat sdf = new SimpleDateFormat();
						sdf.applyPattern("yyyy/MM/dd HH:mm:ss");
						return sdf.parse(date);
					} catch (ParseException e) {
						e.printStackTrace();
					}
					return new Date() ;
				}
			}
		b.将函数导出jar包
			eclipse -> export -> jar 
	
		c.通过hive命令将jar添加到hive的类路径
			$hive>add jar /home/ubuntu/Desktop/ToDate.jar
	
		d.注册函数
			$hive>create temporary function to_date as 'com.it18zhang.myhive210.func.ToDate' ;
	
		e.调用函数
			$hive>desc function to_date;

1.5 自定以表生成函数
---------------------
	1.创建类UDTF
		package com.it18zhang.myhive210.func;
	
		import java.util.ArrayList;
	
		import org.apache.hadoop.hive.ql.exec.Description;
		import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
		import org.apache.hadoop.hive.ql.metadata.HiveException;
		import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
		import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
		import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
		import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
		import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
		import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
		import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantIntObjectInspector;
		import org.apache.hadoop.io.IntWritable;
	
		/**
		 * 简单for循环函数
		 */
		@Description(name="forx",
		value="this is my first UDTF!",
		extended="For example : select forx(1,5,1)) ;")
		public class ForUDTF extends GenericUDTF {
			// 起始
			IntWritable start;
			// 结束
			IntWritable end;
			// 增量
			IntWritable inc;
			//
			private Object[] forwardObj;
	
			/**
			 * 初始化函数
			 */
			public StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException {
				start = ((WritableConstantIntObjectInspector) args[0]).getWritableConstantValue();
				end = ((WritableConstantIntObjectInspector) args[1]).getWritableConstantValue();
				if (args.length == 3) {
					inc = ((WritableConstantIntObjectInspector) args[2]).getWritableConstantValue();
				} else {
					inc = new IntWritable(1);
				}
	
				this.forwardObj = new Object[1];
	
				ArrayList<String> fieldNames = new ArrayList<String>();
	
				ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
				fieldNames.add("col0");
	
				fieldOIs.add(PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(PrimitiveCategory.INT));
				return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
			}
	
			public void process(Object[] args) throws HiveException {
				for (int i = start.get(); i < end.get(); i = i + inc.get()) {
					this.forwardObj[0] = new Integer(i);
					forward(forwardObj);
				}
			}
	
			public void close() throws HiveException {
			}
		}
	2.编译、打包、导出
		mvn package -DskipTests
	3.复制jar到ubuntu
		
	4.使用hive的add jar命令，添加jar到classpath
		$hive>add jar /home/ubuntu/Desktop/forx.jar;
	5.创建临时函数
		$hive>create temporary function forx as 'x.x.x.xxx.ForUDTF' ;
	6.调用函数
		$hive>select forx(1,5) ;

1.6 OLAP
---------------
	仓库软件，类SQL语言,HiveQL,HQL.结构化数据.
	schema : rdbms(derby -- mysql),hdfs .
	延迟高.

2 HBase
===============
	1、介绍
		是分布式面向列的数据库，构建在hadoop之上，
		类似于google的big table,对海量结构化数据的快速随机访问.
		是hadoop生态系统的一部分。提供随机实时读写。
	
	2、hbase和hadoop的区别
			hdfs						  hbase
		1.	分布式文件系统,存储大量数据		 数据库,构建与hdfs之上
		2.	不支持快速单个记录查找			  支持大表的快速查找
		3.	提供了高延迟批处理				提供了单行记录低延迟随机访问(10亿级别)
		4.	只提供数据按序访问				内部使用hash表提供的随访访问。在hdfs上存放索引文件，用于快速查找。

2.1 存储机制
----------------
	1.面向列族的数据库，适用于海量数据的随机读写。
	2.table中只定义了列族，表按照row存储。
	3.术语
		table			:表，是row的集合。
		row				:行，是列族的集合.
		column family	:列族，是列的集合。
		colum			:列，是kv对集合。
	
	4.面向行和面向列的区别
			row				column
		a.	适合于OLTP		适合于OLAP
		b.	针对行列较少的	大表。
	
	5.hbase和RDBMS区别
	
			hbase							RDBMS
		a.	是无模式的，没有列的定义.			有模式的，描述整个table结构。
			只定义列族,列是key				
		b.	适合于宽表,水平可伸缩				 适合于小表，难于扩展。
		c.	没有事务支持					    事务性的。
		d.	不是规范化的					    规范化的
		e.	结构化和半结构化				   结构化的。

2.2 架构
-------------------
1. master-slave主从结构
2.table从竖直方向进行切割，分成若干个区域。由每个region server（区域服务器）进行处理。
3.master server(ms)
	a.负责指派region给rs，通过zk获得task的帮助
	b.处理跨rs的region的负载均衡问题。
	c.从繁忙服务到空闲服务器之间的数据转载。
	d.通过裁定负载均衡判断集群的状态。

4.region
	 被切割的表，跨rs。
4.region server(rs)
	和client通信
	处理数据操作
	处理它下面的所有region的读写请求。
	通过阀值决定region size.


2.3 安装
-------------------	
	1.下载hbase-1.2.3.tar.gz
		
	2.tar开
		...
	3.配置环境变量
		..
	4.配置hbase的本地模式
		[hbase/conf/hbase-site.xml]
		<configuration>
			<property>
					<name>hbase.rootdir</name>
					<value>file:///home/ubuntu/hbase/root</value>
				</property>
				<property>
					<name>hbase.zookeeper.property.dataDir</name>
					<value>/home/ubuntu/hbase/zk</value>
				</property>
		</configuration>
	5.启动hbase。
		$>hbase/bin/start-hbase.sh

# 3 ZK

3.1 背景
-------------------

## 3.2 概述

	保证高可靠、高可用的[协同服务]。
	集中式服务，用于配置信息、名称服务、分布式同步处理。

目前zookeeper是没有取代方案的  目前解决分布式一致性问题的最完美的解决方案

3.3 组件
---------------

	1.client
		向server周期性发送信息，表名自己还活着。server向client回应确认消息，
		client没有收响应，自动重定向消息到其他server.
	2.server
		一个zk节点，向client提供所有服务。通知client，server是alive的。
	3.ensemble
		全体，一组zk节点，需要最小值3。
	4.leader
		领袖，特殊的zk节点。zk集群启动时，推选leader，leader在follower故障进行处理。
	5.follower
		随从，听命于leader的指令。

## 3.4 等级结构

zk的namespace的等级结构(zk data model)

	1.驻留在内存的
	2.树上的每个节点都是znonde
	2.每个znode都有name，而且/分割
	3.每个znode存放的数据不能超过1M=1024K
	4.每个节点都有stat对象
		a.version：与之关联的数据发生改变，版本增加。
		b.acl：action control list（协同服务）
		c.timestamp：时间戳
		d.datalength：数据长度

## 3.5 节点类型

zk znode节点类型

	1.持久节点: 始终驻留
	2.临时节点: 会话结束，节点删除。
	3.顺序节点: 节点名称后面附加数字(递增)

3.6 安装
------------------
	1.下载zk
	2.安装jdk
	3.tar
	4.配置环境变量
		export ZK_HOME=/opt/soft/bigdata/zk
		export PATH=$PATH:$ZK_HOME/bin

3.7 部署
--------------------

单机版部署和伪分布式部署

	1.单机版
		a.创建配置conf/zoo.cfg配置
			// 每次心跳的毫秒数
			tickTime=2000
			// 开始同步阶段的心跳数
	        initLimit=10
	        // 发送请求和得到确认直接可以传递的心跳个数
	        syncLimit=5
	        // 存储ZK的快照目录
	        dataDir=/home/guodd/zk/data
	        // 日志目录
	        dataLogDir=/home/guodd/zk/datalog
	        // 客户端连接的端口，默认端口
	        clientPort=2181
	
		b.启动zkServer
			$>bin/zkServer.sh start
			
		c.启动zkClient
			$>zkCli.sh
			$zlCli>ls /
			$zlCli>ls /zookeeper
			$zlCli>ls /zookeeper/quota
	
		e.可使用四字命令远程访问zk服务器（不需要安装zk软件包.）
			$>echo ruok | nc localhost 2181
			$>echo conf | nc localhost 2181
			$>echo envi | nc localhost 2181
			$>echo stat | nc localhost 2181
	2.在一个节点上配置zk的集群(主要用于集群)，伪分布式
		a.给每个zk server指定一个id(唯一性),存放在myid文件中,值介于1-255之间.
			[${datadir/myid]]:/home/ubuntu/zk/data/myid
			n
		b.在zoo.cfg配置文件中指名集群中所有的zk节点
			[conf/zoo.cfg]
			语法:server.n=ip:port1:port
	
			tickTime=2000
			dataDir=/home/ubuntu/zk/data_1
			clientPort=2181
			initLimit=5
			syncLimit=2
	
			server.1=s10:2887:3887
			server.2=s10:2888:3888
			server.3=s10:2889:3889
			
			echo "1" > /tmp/zookeeper/data_1/myid
			echo "2" > /tmp/zookeeper/data_2/myid
			echo "3" > /tmp/zookeeper/data_3/myid
	
		c.启动三个zk进程
			$>zkServer.sh start /soft/zk/conf/zoo1.cfg
			$>zkServer.sh start /soft/zk/conf/zoo2.cfg
			$>zkServer.sh start /soft/zk/conf/zoo3.cfg
	
		d.通过zkCli.sh连接zk服务器
			$>zkCli.sh					//localhost 2181		//zoo.cfg
			$>zkCli.sh -server s100:2182

3.8 配置zk
---------------
	1.考查配置文件
		[conf/zoo_sample.conf]
		# 每次心跳的毫秒数
		tickTime=2000
	
		# The number of ticks that the initial 
		# synchronization phase can take
		# 开始同步阶段的心跳数
		initLimit=10
	
		# The number of ticks that can pass between 
		# sending a request and getting an acknowledgement
		# 发送请求和得到确认之间可以传递的心跳个数.
		syncLimit=5
	
		# 存放zk的快照目录,不要存在/tmp下
		dataDir=/tmp/zookeeper
	
		# 客户端连接的端口
		clientPort=2181
	
		# the maximum number of client connections.
		# increase this if you need to handle more clients
		# 客户端最大连接数
		#maxClientCnxns=60
	
		#
		# Be sure to read the maintenance section of the 
		# administrator guide before turning on autopurge.
		#
		# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
		#
		# The number of snapshots to retain in dataDir
		# 在datadir保留的快照数量
		#autopurge.snapRetainCount=3
	
		# Purge task interval in hours
		# Set to "0" to disable auto purge feature
		# 丢弃任务的间隔时间(小时数)
		#autopurge.purgeInterval=1
# 4 分布式

## 4.1 配置

```properties
1.配置hbase的本地模式
	[hbase-site.xml]
	hbase.rootdir=hdfs://s10:8020/hbase
	hbase.zookeeper.property.dataDir=/home/guodd/hbase/zk
	// HBASE和zookeeper运行同一个JVM
2.伪分布式模式
    hbase.rootdir=hdfs://s10:8020/hbase
    dfs.replication=1
    hbase.zookeeper.property.dataDir=/home/guodd/hbase/zk
3.完全分布式模式
	[hbase-site.xml]
    hbase.rootdir=hdfs://s10:8020/hbase
    dfs.replication=3
    hbase.cluster.distributed=true
	#zookeeper配置
	hbase.zookeeper.property.clientPort=2181
	hbase.zookeeper.quorum=s11,s12
	hbase.zookeeper.property.dataDir=/home/guodd/hbase/zk
    [regionservers]
    s11
    s12
```

## 4.2 分发

分发HBase

分发环境变量

## 4.3 启动Hadoop

```properties
start-dfs.sh
start-yarn.sh
```

## 4.4 查看进程

```properties
xcall jps
netstart -anltp | grop 10601
```

## 4.5 Web UI

s10:16010

s11:16030

## 4.6 启动命令

```properties
hbase-daemons.sh start zookeeper
hbase-daemons.sh start regionserver
hbase-daemon.sh start master
```

## 4.7 HBase shell

注意语句不需要添加结尾符号。

```properties
hbase shell // 进入
help    // 查看帮助
help "general"
create 't2', {NAME => 'f1'}  // 创建表
put 't2','row1','f1','Java'
put 't2','row1','f1:age','25'
get 't2','row1','f1'
scan 't2'
delete 't2','row1','f1'
disable 't2'
drop 't2'
```

# 5 入坑

## 5.1 HBase Master自动停止

HBase Master 启动 check the config value of 'hbase.procedure.store.wal.use.hsync' 解决方法

```properties
<property>
    <name>hbase.unsafe.stream.capability.enforce</name>
    <value>false</value>
    <description>
        Controls whether HBase will check for stream capabilities (hflush/hsync).
        Disable this if you intend to run on LocalFileSystem, denoted by a rootdir
        with the 'file://' scheme, but be mindful of the NOTE below.
        WARNING: Setting this to false blinds you to potential data loss and
        inconsistent system state in the event of process and/or node failures. If
        HBase is complaining of an inability to use hsync or hflush it's most
        likely not a false positive.
    </description>
</property>
```

hbase.unsafe.stream.capability.enforce：使用本地文件系统设置为false，使用hdfs设置为true。但根据HBase 官方手册的说明：HBase 从2.0.0 开始默认使用的是asyncfs。

## 5.2 Master启动顺序

hbase-daemons.sh start zookeeper

hbase-daemon.sh start master

hbase-daemons.sh start regionserver